{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import PIL\n",
    "\n",
    "#from utils import *\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('./train.csv', delimiter=',')\n",
    "data = np.delete(data, (0), axis=0)\n",
    "data_np_y = data[:,0]\n",
    "data = np.delete(data, (0), axis=1)\n",
    "data_np_x = data / 255\n",
    "\n",
    "\n",
    "pos = int(data_np_x.shape[0] * 0.9)\n",
    "\n",
    "train_np_y = data_np_y[:pos]\n",
    "train_np_x = data_np_x[:pos]\n",
    "test_np_y = data_np_y[pos:]\n",
    "test_np_x = data_np_x[pos:]\n",
    "\n",
    "print (\"Y: {}\".format(train_np_y[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data_np_x.shape, data_np_y.shape)\n",
    "print (train_np_x.shape, train_np_y.shape)\n",
    "print (test_np_x.shape, test_np_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 784) (4200,)\n"
     ]
    }
   ],
   "source": [
    "print (test_np_x.shape, test_np_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e7c3ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 4. 0. 0. 7. 3. 5. 3.]\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,10), dpi=80)\n",
    "shape = (1,10)\n",
    "for j in range(10):\n",
    "    ax = fig.add_subplot(shape[0], shape[1], j+1)\n",
    "    image = train_np_x[j].reshape(28,28)\n",
    "    ax.matshow(image, cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "plt.show()\n",
    "\n",
    "print (train_np_y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x = (x - 0.1307) / 0.3081    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.Tensor(np.expand_dims(normalize(train_np_x), axis=1))\n",
    "train_y = torch.LongTensor(train_np_y)\n",
    "\n",
    "test_x = torch.Tensor(np.expand_dims(normalize(test_np_x), axis=1))\n",
    "test_y = torch.LongTensor(test_np_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(x, y, batch_size=32):\n",
    "    for i in range(0, x.size(0) - 1, batch_size):\n",
    "        data = x[i:i+batch_size]\n",
    "        if data.shape[0] == batch_size:\n",
    "            data = data.reshape(batch_size, 1, 28, 28)\n",
    "            targets = y[i:i+batch_size]\n",
    "\n",
    "            yield data, targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
    "                 bn=False, dropout=False, activation_fn=nn.ReLU()):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        layers = []        \n",
    "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding))\n",
    "        if pool_layer is not None:\n",
    "            layers.append(pool_layer)\n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm2d(size[1]))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout2d())\n",
    "        layers.append(activation_fn)\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout())\n",
    "            layers.append(activation_fn())\n",
    "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
    "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, batchnorm=False, dropout=False, lr=5e-5, l2=0.):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "        self._conv1 = ConvLayer([1, 32, 3], bn=batchnorm)\n",
    "        self._conv2 = ConvLayer([32, 64, 3], bn=batchnorm, activation_fn=nn.Sigmoid())\n",
    "        \n",
    "        self.fc = FullyConnected([64*7*7, 10], dropout=dropout)\n",
    "        \n",
    "        self._loss = None\n",
    "        self.optim = optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "    \n",
    "    def conv(self, x):\n",
    "        x = self._conv1(x)\n",
    "        x = self._conv2(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):        \n",
    "        self._loss = F.cross_entropy(output, target, **kwargs)\n",
    "        self._correct = output.data.max(1, keepdim=True)[1]\n",
    "        self._correct = self._correct.eq(target.data.view_as(self._correct)).to(torch.float).cpu().mean()\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, models, log=None):\n",
    "    train_size = len(train_x)\n",
    "    for batch_idx, (data, target) in enumerate(loader(train_x, train_y)):\n",
    "        for model in models.values():                             \n",
    "            model.optim.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = model.loss(output, target)\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "            \n",
    "        if batch_idx % 200 == 0:\n",
    "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / train_size)\n",
    "            losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "            print(line + losses)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / train_size)\n",
    "        losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "        if log is not None:\n",
    "            for k in models:\n",
    "                log[k].append((models[k]._loss, models[k]._correct))\n",
    "        print(line + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, log=None):\n",
    "    test_size = len(test_x)\n",
    "    avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
    "    acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, test_size, p)\n",
    "    line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
    "\n",
    "    test_loss = {k: 0. for k in models}\n",
    "    correct = {k: 0. for k in models}\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader(test_x, test_y):\n",
    "            output = {k: m(data) for k, m in models.items()}           \n",
    "            for k, m in models.items():     \n",
    "                #print (output[k].shape, target.shape)\n",
    "                test_loss[k] += m.loss(output[k], target, size_average=False).item() # sum up batch loss\n",
    "                pred = output[k].data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "                correct[k] += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "    \n",
    "    for k in models:\n",
    "        test_loss[k] /= test_size\n",
    "    correct_pct = {k: c / test_size for k, c in correct.items()}\n",
    "    lines = '\\n'.join([line(k, test_loss[k], correct[k], 100*correct_pct[k]) for k in models]) + '\\n'\n",
    "    report = 'Test set:\\n' + lines\n",
    "    if log is not None:\n",
    "        for k in models:\n",
    "            log[k].append((test_loss[k], correct_pct[k]))\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(log, tpe='loss'):\n",
    "    keys = log.keys()\n",
    "    logs = {k:[z for z in zip(*log[k])] for k in keys}\n",
    "    epochs = {k:range(len(log[k])) for k in keys}\n",
    " \n",
    "    \n",
    "    if tpe == 'loss':\n",
    "        handlers, = zip(*[plt.plot(epochs[k], logs[k][0], label=k) for k in keys])\n",
    "        plt.title('errors')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('error')\n",
    "        plt.legend(handles=handlers)\n",
    "        plt.show()\n",
    "    elif tpe == 'accuracy':\n",
    "        handlers, = zip(*[plt.plot(epochs[k], logs[k][1], label=k) for k in log.keys()])\n",
    "        plt.title('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.legend(handles=handlers)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'bn': Net(True), 'drop': Net(False, True), 'plain': Net()}\n",
    "train_log = {k: [] for k in models}\n",
    "test_log = {k: [] for k in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/37800 (0%)]\tLosses bn: 2.392093 drop: 2.336140 plain: 2.323187\n",
      "Train Epoch: 1 [6400/37800 (1%)]\tLosses bn: 1.591996 drop: 2.081258 plain: 2.056485\n",
      "Train Epoch: 1 [12800/37800 (1%)]\tLosses bn: 1.307419 drop: 1.819666 plain: 1.755378\n",
      "Train Epoch: 1 [19200/37800 (2%)]\tLosses bn: 0.823513 drop: 1.185665 plain: 1.124634\n",
      "Train Epoch: 1 [25600/37800 (2%)]\tLosses bn: 0.895865 drop: 1.210897 plain: 1.138555\n",
      "Train Epoch: 1 [32000/37800 (3%)]\tLosses bn: 0.711713 drop: 0.818733 plain: 0.823962\n",
      "Train Epoch: 1 [37792/37800 (3%)]\tLosses bn: 0.563990 drop: 0.761150 plain: 0.734218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksey/anaconda3/envs/learning/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "bn: Loss: 0.5222\tAccuracy: 3875.0/4200 (92%)\n",
      "drop: Loss: 0.6520\tAccuracy: 3580.0/4200 (85%)\n",
      "plain: Loss: 0.6270\tAccuracy: 3631.0/4200 (86%)\n",
      "\n",
      "Train Epoch: 2 [0/37800 (0%)]\tLosses bn: 0.466278 drop: 0.588315 plain: 0.536954\n",
      "Train Epoch: 2 [6400/37800 (1%)]\tLosses bn: 0.340808 drop: 0.369763 plain: 0.358715\n",
      "Train Epoch: 2 [12800/37800 (1%)]\tLosses bn: 0.507216 drop: 0.676040 plain: 0.656819\n",
      "Train Epoch: 2 [19200/37800 (2%)]\tLosses bn: 0.256668 drop: 0.263278 plain: 0.267215\n",
      "Train Epoch: 2 [25600/37800 (2%)]\tLosses bn: 0.390311 drop: 0.646524 plain: 0.607104\n",
      "Train Epoch: 2 [32000/37800 (3%)]\tLosses bn: 0.316425 drop: 0.389902 plain: 0.402457\n",
      "Train Epoch: 2 [37792/37800 (3%)]\tLosses bn: 0.275606 drop: 0.428960 plain: 0.419263\n",
      "Test set:\n",
      "bn: Loss: 0.2657\tAccuracy: 3973.0/4200 (95%)\n",
      "drop: Loss: 0.3757\tAccuracy: 3807.0/4200 (91%)\n",
      "plain: Loss: 0.3651\tAccuracy: 3819.0/4200 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/37800 (0%)]\tLosses bn: 0.192105 drop: 0.310417 plain: 0.280580\n",
      "Train Epoch: 3 [6400/37800 (1%)]\tLosses bn: 0.154519 drop: 0.156553 plain: 0.152109\n",
      "Train Epoch: 3 [12800/37800 (1%)]\tLosses bn: 0.324554 drop: 0.542838 plain: 0.523099\n",
      "Train Epoch: 3 [19200/37800 (2%)]\tLosses bn: 0.153143 drop: 0.155781 plain: 0.166705\n",
      "Train Epoch: 3 [25600/37800 (2%)]\tLosses bn: 0.246923 drop: 0.480044 plain: 0.453074\n",
      "Train Epoch: 3 [32000/37800 (3%)]\tLosses bn: 0.190236 drop: 0.257201 plain: 0.270649\n",
      "Train Epoch: 3 [37792/37800 (3%)]\tLosses bn: 0.170484 drop: 0.293516 plain: 0.292805\n",
      "Test set:\n",
      "bn: Loss: 0.1800\tAccuracy: 4032.0/4200 (96%)\n",
      "drop: Loss: 0.2808\tAccuracy: 3883.0/4200 (92%)\n",
      "plain: Loss: 0.2755\tAccuracy: 3893.0/4200 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/37800 (0%)]\tLosses bn: 0.108552 drop: 0.225772 plain: 0.194812\n",
      "Train Epoch: 4 [6400/37800 (1%)]\tLosses bn: 0.088549 drop: 0.085293 plain: 0.083383\n",
      "Train Epoch: 4 [12800/37800 (1%)]\tLosses bn: 0.235966 drop: 0.473975 plain: 0.454857\n",
      "Train Epoch: 4 [19200/37800 (2%)]\tLosses bn: 0.112979 drop: 0.107558 plain: 0.122836\n",
      "Train Epoch: 4 [25600/37800 (2%)]\tLosses bn: 0.185141 drop: 0.374112 plain: 0.361483\n",
      "Train Epoch: 4 [32000/37800 (3%)]\tLosses bn: 0.134602 drop: 0.189374 plain: 0.205002\n",
      "Train Epoch: 4 [37792/37800 (3%)]\tLosses bn: 0.116704 drop: 0.207566 plain: 0.213729\n",
      "Test set:\n",
      "bn: Loss: 0.1374\tAccuracy: 4062.0/4200 (97%)\n",
      "drop: Loss: 0.2256\tAccuracy: 3945.0/4200 (94%)\n",
      "plain: Loss: 0.2235\tAccuracy: 3946.0/4200 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/37800 (0%)]\tLosses bn: 0.070595 drop: 0.172936 plain: 0.143616\n",
      "Train Epoch: 5 [6400/37800 (1%)]\tLosses bn: 0.057769 drop: 0.054679 plain: 0.053993\n",
      "Train Epoch: 5 [12800/37800 (1%)]\tLosses bn: 0.185388 drop: 0.413964 plain: 0.400702\n",
      "Train Epoch: 5 [19200/37800 (2%)]\tLosses bn: 0.090651 drop: 0.081192 plain: 0.097071\n",
      "Train Epoch: 5 [25600/37800 (2%)]\tLosses bn: 0.155656 drop: 0.298118 plain: 0.299704\n",
      "Train Epoch: 5 [32000/37800 (3%)]\tLosses bn: 0.104779 drop: 0.149917 plain: 0.165434\n",
      "Train Epoch: 5 [37792/37800 (3%)]\tLosses bn: 0.086158 drop: 0.150353 plain: 0.159124\n",
      "Test set:\n",
      "bn: Loss: 0.1123\tAccuracy: 4078.0/4200 (97%)\n",
      "drop: Loss: 0.1876\tAccuracy: 3978.0/4200 (95%)\n",
      "plain: Loss: 0.1876\tAccuracy: 3981.0/4200 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/37800 (0%)]\tLosses bn: 0.049944 drop: 0.131664 plain: 0.107928\n",
      "Train Epoch: 6 [6400/37800 (1%)]\tLosses bn: 0.041739 drop: 0.040236 plain: 0.040099\n",
      "Train Epoch: 6 [12800/37800 (1%)]\tLosses bn: 0.152840 drop: 0.359102 plain: 0.352710\n",
      "Train Epoch: 6 [19200/37800 (2%)]\tLosses bn: 0.076033 drop: 0.066031 plain: 0.080415\n",
      "Train Epoch: 6 [25600/37800 (2%)]\tLosses bn: 0.143987 drop: 0.244011 plain: 0.255777\n",
      "Train Epoch: 6 [32000/37800 (3%)]\tLosses bn: 0.085682 drop: 0.125163 plain: 0.140054\n",
      "Train Epoch: 6 [37792/37800 (3%)]\tLosses bn: 0.067834 drop: 0.112969 plain: 0.122315\n",
      "Test set:\n",
      "bn: Loss: 0.0961\tAccuracy: 4089.0/4200 (97%)\n",
      "drop: Loss: 0.1597\tAccuracy: 4013.0/4200 (96%)\n",
      "plain: Loss: 0.1611\tAccuracy: 4006.0/4200 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/37800 (0%)]\tLosses bn: 0.037526 drop: 0.099309 plain: 0.081564\n",
      "Train Epoch: 7 [6400/37800 (1%)]\tLosses bn: 0.032850 drop: 0.032720 plain: 0.033032\n",
      "Train Epoch: 7 [12800/37800 (1%)]\tLosses bn: 0.131396 drop: 0.309873 plain: 0.310064\n",
      "Train Epoch: 7 [19200/37800 (2%)]\tLosses bn: 0.064969 drop: 0.057409 plain: 0.069941\n",
      "Train Epoch: 7 [25600/37800 (2%)]\tLosses bn: 0.140953 drop: 0.205283 plain: 0.223453\n",
      "Train Epoch: 7 [32000/37800 (3%)]\tLosses bn: 0.072712 drop: 0.108707 plain: 0.121899\n",
      "Train Epoch: 7 [37792/37800 (3%)]\tLosses bn: 0.056314 drop: 0.089225 plain: 0.097916\n",
      "Test set:\n",
      "bn: Loss: 0.0850\tAccuracy: 4100.0/4200 (98%)\n",
      "drop: Loss: 0.1387\tAccuracy: 4031.0/4200 (96%)\n",
      "plain: Loss: 0.1410\tAccuracy: 4029.0/4200 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/37800 (0%)]\tLosses bn: 0.029301 drop: 0.074124 plain: 0.061702\n",
      "Train Epoch: 8 [6400/37800 (1%)]\tLosses bn: 0.027580 drop: 0.028553 plain: 0.029049\n",
      "Train Epoch: 8 [12800/37800 (1%)]\tLosses bn: 0.116500 drop: 0.268447 plain: 0.274136\n",
      "Train Epoch: 8 [19200/37800 (2%)]\tLosses bn: 0.056146 drop: 0.052086 plain: 0.063467\n",
      "Train Epoch: 8 [25600/37800 (2%)]\tLosses bn: 0.142717 drop: 0.177407 plain: 0.199158\n",
      "Train Epoch: 8 [32000/37800 (3%)]\tLosses bn: 0.063425 drop: 0.096730 plain: 0.107787\n",
      "Train Epoch: 8 [37792/37800 (3%)]\tLosses bn: 0.048822 drop: 0.074187 plain: 0.081055\n",
      "Test set:\n",
      "bn: Loss: 0.0769\tAccuracy: 4105.0/4200 (98%)\n",
      "drop: Loss: 0.1226\tAccuracy: 4055.0/4200 (97%)\n",
      "plain: Loss: 0.1255\tAccuracy: 4043.0/4200 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/37800 (0%)]\tLosses bn: 0.023615 drop: 0.054959 plain: 0.047325\n",
      "Train Epoch: 9 [6400/37800 (1%)]\tLosses bn: 0.024231 drop: 0.026186 plain: 0.026458\n",
      "Train Epoch: 9 [12800/37800 (1%)]\tLosses bn: 0.105671 drop: 0.235621 plain: 0.244986\n",
      "Train Epoch: 9 [19200/37800 (2%)]\tLosses bn: 0.049160 drop: 0.048385 plain: 0.058943\n",
      "Train Epoch: 9 [25600/37800 (2%)]\tLosses bn: 0.146730 drop: 0.156728 plain: 0.179289\n",
      "Train Epoch: 9 [32000/37800 (3%)]\tLosses bn: 0.056775 drop: 0.087381 plain: 0.097185\n",
      "Train Epoch: 9 [37792/37800 (3%)]\tLosses bn: 0.043440 drop: 0.064517 plain: 0.069126\n",
      "Test set:\n",
      "bn: Loss: 0.0709\tAccuracy: 4113.0/4200 (98%)\n",
      "drop: Loss: 0.1101\tAccuracy: 4067.0/4200 (97%)\n",
      "plain: Loss: 0.1132\tAccuracy: 4058.0/4200 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/37800 (0%)]\tLosses bn: 0.019459 drop: 0.040957 plain: 0.037251\n",
      "Train Epoch: 10 [6400/37800 (1%)]\tLosses bn: 0.022163 drop: 0.024811 plain: 0.024641\n",
      "Train Epoch: 10 [12800/37800 (1%)]\tLosses bn: 0.097598 drop: 0.209983 plain: 0.221867\n",
      "Train Epoch: 10 [19200/37800 (2%)]\tLosses bn: 0.043060 drop: 0.045446 plain: 0.055620\n",
      "Train Epoch: 10 [25600/37800 (2%)]\tLosses bn: 0.151622 drop: 0.140763 plain: 0.162984\n",
      "Train Epoch: 10 [32000/37800 (3%)]\tLosses bn: 0.051684 drop: 0.079836 plain: 0.088724\n",
      "Train Epoch: 10 [37792/37800 (3%)]\tLosses bn: 0.039483 drop: 0.058144 plain: 0.060272\n",
      "Test set:\n",
      "bn: Loss: 0.0661\tAccuracy: 4119.0/4200 (98%)\n",
      "drop: Loss: 0.1002\tAccuracy: 4074.0/4200 (97%)\n",
      "plain: Loss: 0.1034\tAccuracy: 4069.0/4200 (97%)\n",
      "\n",
      "Train Epoch: 11 [0/37800 (0%)]\tLosses bn: 0.016384 drop: 0.030894 plain: 0.030124\n",
      "Train Epoch: 11 [6400/37800 (1%)]\tLosses bn: 0.020582 drop: 0.023882 plain: 0.023226\n",
      "Train Epoch: 11 [12800/37800 (1%)]\tLosses bn: 0.090972 drop: 0.190253 plain: 0.203452\n",
      "Train Epoch: 11 [19200/37800 (2%)]\tLosses bn: 0.038468 drop: 0.042900 plain: 0.052772\n",
      "Train Epoch: 11 [25600/37800 (2%)]\tLosses bn: 0.157322 drop: 0.128561 plain: 0.149278\n",
      "Train Epoch: 11 [32000/37800 (3%)]\tLosses bn: 0.047233 drop: 0.073712 plain: 0.081928\n",
      "Train Epoch: 11 [37792/37800 (3%)]\tLosses bn: 0.036276 drop: 0.053796 plain: 0.053404\n",
      "Test set:\n",
      "bn: Loss: 0.0623\tAccuracy: 4121.0/4200 (98%)\n",
      "drop: Loss: 0.0922\tAccuracy: 4081.0/4200 (97%)\n",
      "plain: Loss: 0.0954\tAccuracy: 4081.0/4200 (97%)\n",
      "\n",
      "Train Epoch: 12 [0/37800 (0%)]\tLosses bn: 0.013962 drop: 0.023751 plain: 0.024921\n",
      "Train Epoch: 12 [6400/37800 (1%)]\tLosses bn: 0.019424 drop: 0.023194 plain: 0.022061\n",
      "Train Epoch: 12 [12800/37800 (1%)]\tLosses bn: 0.086035 drop: 0.174671 plain: 0.188692\n",
      "Train Epoch: 12 [19200/37800 (2%)]\tLosses bn: 0.034849 drop: 0.040665 plain: 0.050322\n",
      "Train Epoch: 12 [25600/37800 (2%)]\tLosses bn: 0.161342 drop: 0.119058 plain: 0.137602\n",
      "Train Epoch: 12 [32000/37800 (3%)]\tLosses bn: 0.044052 drop: 0.068442 plain: 0.076311\n",
      "Train Epoch: 12 [37792/37800 (3%)]\tLosses bn: 0.033540 drop: 0.050650 plain: 0.047783\n",
      "Test set:\n",
      "bn: Loss: 0.0591\tAccuracy: 4123.0/4200 (98%)\n",
      "drop: Loss: 0.0857\tAccuracy: 4085.0/4200 (97%)\n",
      "plain: Loss: 0.0888\tAccuracy: 4089.0/4200 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [0/37800 (0%)]\tLosses bn: 0.012134 drop: 0.018646 plain: 0.021041\n",
      "Train Epoch: 13 [6400/37800 (1%)]\tLosses bn: 0.018601 drop: 0.022601 plain: 0.020965\n",
      "Train Epoch: 13 [12800/37800 (1%)]\tLosses bn: 0.081125 drop: 0.162211 plain: 0.176807\n",
      "Train Epoch: 13 [19200/37800 (2%)]\tLosses bn: 0.031624 drop: 0.038767 plain: 0.048138\n",
      "Train Epoch: 13 [25600/37800 (2%)]\tLosses bn: 0.164956 drop: 0.111663 plain: 0.127758\n",
      "Train Epoch: 13 [32000/37800 (3%)]\tLosses bn: 0.041262 drop: 0.063882 plain: 0.071435\n",
      "Train Epoch: 13 [37792/37800 (3%)]\tLosses bn: 0.031276 drop: 0.048241 plain: 0.043119\n",
      "Test set:\n",
      "bn: Loss: 0.0565\tAccuracy: 4123.0/4200 (98%)\n",
      "drop: Loss: 0.0804\tAccuracy: 4091.0/4200 (97%)\n",
      "plain: Loss: 0.0832\tAccuracy: 4096.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 14 [0/37800 (0%)]\tLosses bn: 0.010711 drop: 0.014999 plain: 0.018088\n",
      "Train Epoch: 14 [6400/37800 (1%)]\tLosses bn: 0.017897 drop: 0.022012 plain: 0.020016\n",
      "Train Epoch: 14 [12800/37800 (1%)]\tLosses bn: 0.077106 drop: 0.152072 plain: 0.166745\n",
      "Train Epoch: 14 [19200/37800 (2%)]\tLosses bn: 0.029118 drop: 0.037135 plain: 0.046163\n",
      "Train Epoch: 14 [25600/37800 (2%)]\tLosses bn: 0.167516 drop: 0.105888 plain: 0.119141\n",
      "Train Epoch: 14 [32000/37800 (3%)]\tLosses bn: 0.039034 drop: 0.059782 plain: 0.067390\n",
      "Train Epoch: 14 [37792/37800 (3%)]\tLosses bn: 0.029241 drop: 0.046415 plain: 0.039208\n",
      "Test set:\n",
      "bn: Loss: 0.0543\tAccuracy: 4126.0/4200 (98%)\n",
      "drop: Loss: 0.0759\tAccuracy: 4098.0/4200 (98%)\n",
      "plain: Loss: 0.0786\tAccuracy: 4100.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 15 [0/37800 (0%)]\tLosses bn: 0.009624 drop: 0.012326 plain: 0.015748\n",
      "Train Epoch: 15 [6400/37800 (1%)]\tLosses bn: 0.017223 drop: 0.021346 plain: 0.019144\n",
      "Train Epoch: 15 [12800/37800 (1%)]\tLosses bn: 0.073099 drop: 0.143617 plain: 0.158246\n",
      "Train Epoch: 15 [19200/37800 (2%)]\tLosses bn: 0.026932 drop: 0.035665 plain: 0.044461\n",
      "Train Epoch: 15 [25600/37800 (2%)]\tLosses bn: 0.170347 drop: 0.101189 plain: 0.111842\n",
      "Train Epoch: 15 [32000/37800 (3%)]\tLosses bn: 0.036722 drop: 0.056118 plain: 0.063915\n",
      "Train Epoch: 15 [37792/37800 (3%)]\tLosses bn: 0.027695 drop: 0.045060 plain: 0.035975\n",
      "Test set:\n",
      "bn: Loss: 0.0524\tAccuracy: 4127.0/4200 (98%)\n",
      "drop: Loss: 0.0720\tAccuracy: 4100.0/4200 (98%)\n",
      "plain: Loss: 0.0745\tAccuracy: 4102.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 16 [0/37800 (0%)]\tLosses bn: 0.008752 drop: 0.010357 plain: 0.013894\n",
      "Train Epoch: 16 [6400/37800 (1%)]\tLosses bn: 0.016501 drop: 0.020625 plain: 0.018370\n",
      "Train Epoch: 16 [12800/37800 (1%)]\tLosses bn: 0.069449 drop: 0.136429 plain: 0.150887\n",
      "Train Epoch: 16 [19200/37800 (2%)]\tLosses bn: 0.025036 drop: 0.034345 plain: 0.043019\n",
      "Train Epoch: 16 [25600/37800 (2%)]\tLosses bn: 0.172723 drop: 0.097152 plain: 0.105755\n",
      "Train Epoch: 16 [32000/37800 (3%)]\tLosses bn: 0.034640 drop: 0.052746 plain: 0.060814\n",
      "Train Epoch: 16 [37792/37800 (3%)]\tLosses bn: 0.026460 drop: 0.044042 plain: 0.033357\n",
      "Test set:\n",
      "bn: Loss: 0.0507\tAccuracy: 4128.0/4200 (98%)\n",
      "drop: Loss: 0.0687\tAccuracy: 4104.0/4200 (98%)\n",
      "plain: Loss: 0.0711\tAccuracy: 4105.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 17 [0/37800 (0%)]\tLosses bn: 0.008061 drop: 0.008880 plain: 0.012387\n",
      "Train Epoch: 17 [6400/37800 (1%)]\tLosses bn: 0.016063 drop: 0.019869 plain: 0.017646\n",
      "Train Epoch: 17 [12800/37800 (1%)]\tLosses bn: 0.066267 drop: 0.130365 plain: 0.144295\n",
      "Train Epoch: 17 [19200/37800 (2%)]\tLosses bn: 0.023309 drop: 0.033114 plain: 0.041626\n",
      "Train Epoch: 17 [25600/37800 (2%)]\tLosses bn: 0.173170 drop: 0.093833 plain: 0.100491\n",
      "Train Epoch: 17 [32000/37800 (3%)]\tLosses bn: 0.032665 drop: 0.049667 plain: 0.057902\n",
      "Train Epoch: 17 [37792/37800 (3%)]\tLosses bn: 0.025162 drop: 0.043259 plain: 0.031135\n",
      "Test set:\n",
      "bn: Loss: 0.0493\tAccuracy: 4129.0/4200 (98%)\n",
      "drop: Loss: 0.0658\tAccuracy: 4108.0/4200 (98%)\n",
      "plain: Loss: 0.0680\tAccuracy: 4110.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 18 [0/37800 (0%)]\tLosses bn: 0.007511 drop: 0.007725 plain: 0.011157\n",
      "Train Epoch: 18 [6400/37800 (1%)]\tLosses bn: 0.015626 drop: 0.019174 plain: 0.016949\n",
      "Train Epoch: 18 [12800/37800 (1%)]\tLosses bn: 0.063201 drop: 0.125087 plain: 0.138201\n",
      "Train Epoch: 18 [19200/37800 (2%)]\tLosses bn: 0.021846 drop: 0.031962 plain: 0.040181\n",
      "Train Epoch: 18 [25600/37800 (2%)]\tLosses bn: 0.173890 drop: 0.090846 plain: 0.095790\n",
      "Train Epoch: 18 [32000/37800 (3%)]\tLosses bn: 0.031253 drop: 0.046806 plain: 0.055201\n",
      "Train Epoch: 18 [37792/37800 (3%)]\tLosses bn: 0.023849 drop: 0.042679 plain: 0.029277\n",
      "Test set:\n",
      "bn: Loss: 0.0481\tAccuracy: 4129.0/4200 (98%)\n",
      "drop: Loss: 0.0632\tAccuracy: 4114.0/4200 (98%)\n",
      "plain: Loss: 0.0653\tAccuracy: 4115.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 19 [0/37800 (0%)]\tLosses bn: 0.007095 drop: 0.006814 plain: 0.010160\n",
      "Train Epoch: 19 [6400/37800 (1%)]\tLosses bn: 0.015179 drop: 0.018510 plain: 0.016325\n",
      "Train Epoch: 19 [12800/37800 (1%)]\tLosses bn: 0.060182 drop: 0.120439 plain: 0.132718\n",
      "Train Epoch: 19 [19200/37800 (2%)]\tLosses bn: 0.020464 drop: 0.030941 plain: 0.038771\n",
      "Train Epoch: 19 [25600/37800 (2%)]\tLosses bn: 0.173417 drop: 0.088362 plain: 0.091727\n",
      "Train Epoch: 19 [32000/37800 (3%)]\tLosses bn: 0.029666 drop: 0.044207 plain: 0.052845\n",
      "Train Epoch: 19 [37792/37800 (3%)]\tLosses bn: 0.022693 drop: 0.042125 plain: 0.027655\n",
      "Test set:\n",
      "bn: Loss: 0.0470\tAccuracy: 4129.0/4200 (98%)\n",
      "drop: Loss: 0.0609\tAccuracy: 4115.0/4200 (98%)\n",
      "plain: Loss: 0.0629\tAccuracy: 4116.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 20 [0/37800 (0%)]\tLosses bn: 0.006724 drop: 0.006093 plain: 0.009349\n",
      "Train Epoch: 20 [6400/37800 (1%)]\tLosses bn: 0.014837 drop: 0.017850 plain: 0.015711\n",
      "Train Epoch: 20 [12800/37800 (1%)]\tLosses bn: 0.056531 drop: 0.116310 plain: 0.127651\n",
      "Train Epoch: 20 [19200/37800 (2%)]\tLosses bn: 0.019124 drop: 0.030099 plain: 0.037478\n",
      "Train Epoch: 20 [25600/37800 (2%)]\tLosses bn: 0.173105 drop: 0.086145 plain: 0.088237\n",
      "Train Epoch: 20 [32000/37800 (3%)]\tLosses bn: 0.028201 drop: 0.041748 plain: 0.050609\n",
      "Train Epoch: 20 [37792/37800 (3%)]\tLosses bn: 0.021723 drop: 0.041718 plain: 0.026274\n",
      "Test set:\n",
      "bn: Loss: 0.0459\tAccuracy: 4129.0/4200 (98%)\n",
      "drop: Loss: 0.0589\tAccuracy: 4118.0/4200 (98%)\n",
      "plain: Loss: 0.0607\tAccuracy: 4117.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 21 [0/37800 (0%)]\tLosses bn: 0.006378 drop: 0.005501 plain: 0.008677\n",
      "Train Epoch: 21 [6400/37800 (1%)]\tLosses bn: 0.014308 drop: 0.017171 plain: 0.015158\n",
      "Train Epoch: 21 [12800/37800 (1%)]\tLosses bn: 0.054098 drop: 0.112650 plain: 0.122723\n",
      "Train Epoch: 21 [19200/37800 (2%)]\tLosses bn: 0.017905 drop: 0.029325 plain: 0.036224\n",
      "Train Epoch: 21 [25600/37800 (2%)]\tLosses bn: 0.172613 drop: 0.084168 plain: 0.085063\n",
      "Train Epoch: 21 [32000/37800 (3%)]\tLosses bn: 0.026787 drop: 0.039522 plain: 0.048502\n",
      "Train Epoch: 21 [37792/37800 (3%)]\tLosses bn: 0.020605 drop: 0.041310 plain: 0.025100\n",
      "Test set:\n",
      "bn: Loss: 0.0450\tAccuracy: 4130.0/4200 (98%)\n",
      "drop: Loss: 0.0571\tAccuracy: 4117.0/4200 (98%)\n",
      "plain: Loss: 0.0587\tAccuracy: 4119.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 22 [0/37800 (0%)]\tLosses bn: 0.006049 drop: 0.005012 plain: 0.008130\n",
      "Train Epoch: 22 [6400/37800 (1%)]\tLosses bn: 0.013911 drop: 0.016538 plain: 0.014630\n",
      "Train Epoch: 22 [12800/37800 (1%)]\tLosses bn: 0.050970 drop: 0.109322 plain: 0.117990\n",
      "Train Epoch: 22 [19200/37800 (2%)]\tLosses bn: 0.016836 drop: 0.028711 plain: 0.034998\n",
      "Train Epoch: 22 [25600/37800 (2%)]\tLosses bn: 0.170783 drop: 0.082234 plain: 0.082129\n",
      "Train Epoch: 22 [32000/37800 (3%)]\tLosses bn: 0.025577 drop: 0.037438 plain: 0.046485\n",
      "Train Epoch: 22 [37792/37800 (3%)]\tLosses bn: 0.019750 drop: 0.040857 plain: 0.024109\n",
      "Test set:\n",
      "bn: Loss: 0.0443\tAccuracy: 4130.0/4200 (98%)\n",
      "drop: Loss: 0.0554\tAccuracy: 4120.0/4200 (98%)\n",
      "plain: Loss: 0.0570\tAccuracy: 4120.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 23 [0/37800 (0%)]\tLosses bn: 0.005752 drop: 0.004602 plain: 0.007671\n",
      "Train Epoch: 23 [6400/37800 (1%)]\tLosses bn: 0.013562 drop: 0.015912 plain: 0.014170\n",
      "Train Epoch: 23 [12800/37800 (1%)]\tLosses bn: 0.047798 drop: 0.106238 plain: 0.113471\n",
      "Train Epoch: 23 [19200/37800 (2%)]\tLosses bn: 0.015734 drop: 0.028244 plain: 0.033847\n",
      "Train Epoch: 23 [25600/37800 (2%)]\tLosses bn: 0.168553 drop: 0.080306 plain: 0.079282\n",
      "Train Epoch: 23 [32000/37800 (3%)]\tLosses bn: 0.024268 drop: 0.035525 plain: 0.044517\n",
      "Train Epoch: 23 [37792/37800 (3%)]\tLosses bn: 0.018980 drop: 0.040465 plain: 0.023226\n",
      "Test set:\n",
      "bn: Loss: 0.0436\tAccuracy: 4130.0/4200 (98%)\n",
      "drop: Loss: 0.0539\tAccuracy: 4123.0/4200 (98%)\n",
      "plain: Loss: 0.0553\tAccuracy: 4120.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 24 [0/37800 (0%)]\tLosses bn: 0.005451 drop: 0.004251 plain: 0.007289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 [6400/37800 (1%)]\tLosses bn: 0.013345 drop: 0.015310 plain: 0.013727\n",
      "Train Epoch: 24 [12800/37800 (1%)]\tLosses bn: 0.045483 drop: 0.103308 plain: 0.109192\n",
      "Train Epoch: 24 [19200/37800 (2%)]\tLosses bn: 0.014850 drop: 0.027794 plain: 0.032815\n",
      "Train Epoch: 24 [25600/37800 (2%)]\tLosses bn: 0.165725 drop: 0.078482 plain: 0.076689\n",
      "Train Epoch: 24 [32000/37800 (3%)]\tLosses bn: 0.023260 drop: 0.033783 plain: 0.042648\n",
      "Train Epoch: 24 [37792/37800 (3%)]\tLosses bn: 0.018360 drop: 0.040090 plain: 0.022434\n",
      "Test set:\n",
      "bn: Loss: 0.0429\tAccuracy: 4131.0/4200 (98%)\n",
      "drop: Loss: 0.0525\tAccuracy: 4125.0/4200 (98%)\n",
      "plain: Loss: 0.0539\tAccuracy: 4126.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 25 [0/37800 (0%)]\tLosses bn: 0.005198 drop: 0.003959 plain: 0.006967\n",
      "Train Epoch: 25 [6400/37800 (1%)]\tLosses bn: 0.012994 drop: 0.014721 plain: 0.013332\n",
      "Train Epoch: 25 [12800/37800 (1%)]\tLosses bn: 0.042790 drop: 0.100478 plain: 0.105012\n",
      "Train Epoch: 25 [19200/37800 (2%)]\tLosses bn: 0.013904 drop: 0.027269 plain: 0.031803\n",
      "Train Epoch: 25 [25600/37800 (2%)]\tLosses bn: 0.162555 drop: 0.076748 plain: 0.074306\n",
      "Train Epoch: 25 [32000/37800 (3%)]\tLosses bn: 0.022079 drop: 0.032223 plain: 0.040834\n",
      "Train Epoch: 25 [37792/37800 (3%)]\tLosses bn: 0.017910 drop: 0.039800 plain: 0.021698\n",
      "Test set:\n",
      "bn: Loss: 0.0423\tAccuracy: 4130.0/4200 (98%)\n",
      "drop: Loss: 0.0512\tAccuracy: 4126.0/4200 (98%)\n",
      "plain: Loss: 0.0525\tAccuracy: 4128.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 26 [0/37800 (0%)]\tLosses bn: 0.005022 drop: 0.003705 plain: 0.006683\n",
      "Train Epoch: 26 [6400/37800 (1%)]\tLosses bn: 0.012707 drop: 0.014122 plain: 0.012995\n",
      "Train Epoch: 26 [12800/37800 (1%)]\tLosses bn: 0.040514 drop: 0.097728 plain: 0.101001\n",
      "Train Epoch: 26 [19200/37800 (2%)]\tLosses bn: 0.013171 drop: 0.026913 plain: 0.030840\n",
      "Train Epoch: 26 [25600/37800 (2%)]\tLosses bn: 0.158997 drop: 0.075075 plain: 0.072100\n",
      "Train Epoch: 26 [32000/37800 (3%)]\tLosses bn: 0.021266 drop: 0.030810 plain: 0.039104\n",
      "Train Epoch: 26 [37792/37800 (3%)]\tLosses bn: 0.017428 drop: 0.039379 plain: 0.021022\n",
      "Test set:\n",
      "bn: Loss: 0.0418\tAccuracy: 4133.0/4200 (98%)\n",
      "drop: Loss: 0.0500\tAccuracy: 4126.0/4200 (98%)\n",
      "plain: Loss: 0.0513\tAccuracy: 4129.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 27 [0/37800 (0%)]\tLosses bn: 0.004842 drop: 0.003490 plain: 0.006429\n",
      "Train Epoch: 27 [6400/37800 (1%)]\tLosses bn: 0.012377 drop: 0.013544 plain: 0.012694\n",
      "Train Epoch: 27 [12800/37800 (1%)]\tLosses bn: 0.038458 drop: 0.095119 plain: 0.097127\n",
      "Train Epoch: 27 [19200/37800 (2%)]\tLosses bn: 0.012502 drop: 0.026546 plain: 0.029894\n",
      "Train Epoch: 27 [25600/37800 (2%)]\tLosses bn: 0.154986 drop: 0.073524 plain: 0.070078\n",
      "Train Epoch: 27 [32000/37800 (3%)]\tLosses bn: 0.020137 drop: 0.029453 plain: 0.037396\n",
      "Train Epoch: 27 [37792/37800 (3%)]\tLosses bn: 0.017114 drop: 0.039015 plain: 0.020418\n",
      "Test set:\n",
      "bn: Loss: 0.0413\tAccuracy: 4132.0/4200 (98%)\n",
      "drop: Loss: 0.0489\tAccuracy: 4128.0/4200 (98%)\n",
      "plain: Loss: 0.0501\tAccuracy: 4128.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 28 [0/37800 (0%)]\tLosses bn: 0.004713 drop: 0.003305 plain: 0.006167\n",
      "Train Epoch: 28 [6400/37800 (1%)]\tLosses bn: 0.012035 drop: 0.012970 plain: 0.012405\n",
      "Train Epoch: 28 [12800/37800 (1%)]\tLosses bn: 0.036489 drop: 0.092490 plain: 0.093383\n",
      "Train Epoch: 28 [19200/37800 (2%)]\tLosses bn: 0.011935 drop: 0.026199 plain: 0.029039\n",
      "Train Epoch: 28 [25600/37800 (2%)]\tLosses bn: 0.150673 drop: 0.071963 plain: 0.068173\n",
      "Train Epoch: 28 [32000/37800 (3%)]\tLosses bn: 0.019029 drop: 0.028229 plain: 0.035747\n",
      "Train Epoch: 28 [37792/37800 (3%)]\tLosses bn: 0.016433 drop: 0.038657 plain: 0.019850\n",
      "Test set:\n",
      "bn: Loss: 0.0408\tAccuracy: 4133.0/4200 (98%)\n",
      "drop: Loss: 0.0479\tAccuracy: 4129.0/4200 (98%)\n",
      "plain: Loss: 0.0491\tAccuracy: 4130.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 29 [0/37800 (0%)]\tLosses bn: 0.004560 drop: 0.003140 plain: 0.005935\n",
      "Train Epoch: 29 [6400/37800 (1%)]\tLosses bn: 0.011850 drop: 0.012423 plain: 0.012144\n",
      "Train Epoch: 29 [12800/37800 (1%)]\tLosses bn: 0.033659 drop: 0.089942 plain: 0.089836\n",
      "Train Epoch: 29 [19200/37800 (2%)]\tLosses bn: 0.011309 drop: 0.025811 plain: 0.028162\n",
      "Train Epoch: 29 [25600/37800 (2%)]\tLosses bn: 0.146221 drop: 0.070394 plain: 0.066394\n",
      "Train Epoch: 29 [32000/37800 (3%)]\tLosses bn: 0.018046 drop: 0.027117 plain: 0.034216\n",
      "Train Epoch: 29 [37792/37800 (3%)]\tLosses bn: 0.015878 drop: 0.038300 plain: 0.019308\n",
      "Test set:\n",
      "bn: Loss: 0.0403\tAccuracy: 4134.0/4200 (98%)\n",
      "drop: Loss: 0.0470\tAccuracy: 4130.0/4200 (98%)\n",
      "plain: Loss: 0.0481\tAccuracy: 4131.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 30 [0/37800 (0%)]\tLosses bn: 0.004425 drop: 0.002996 plain: 0.005712\n",
      "Train Epoch: 30 [6400/37800 (1%)]\tLosses bn: 0.011610 drop: 0.011948 plain: 0.011881\n",
      "Train Epoch: 30 [12800/37800 (1%)]\tLosses bn: 0.031455 drop: 0.087519 plain: 0.086431\n",
      "Train Epoch: 30 [19200/37800 (2%)]\tLosses bn: 0.010728 drop: 0.025495 plain: 0.027407\n",
      "Train Epoch: 30 [25600/37800 (2%)]\tLosses bn: 0.139864 drop: 0.068839 plain: 0.064728\n",
      "Train Epoch: 30 [32000/37800 (3%)]\tLosses bn: 0.017280 drop: 0.026051 plain: 0.032763\n",
      "Train Epoch: 30 [37792/37800 (3%)]\tLosses bn: 0.015459 drop: 0.037916 plain: 0.018806\n",
      "Test set:\n",
      "bn: Loss: 0.0399\tAccuracy: 4133.0/4200 (98%)\n",
      "drop: Loss: 0.0461\tAccuracy: 4130.0/4200 (98%)\n",
      "plain: Loss: 0.0472\tAccuracy: 4133.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 31 [0/37800 (0%)]\tLosses bn: 0.004323 drop: 0.002865 plain: 0.005503\n",
      "Train Epoch: 31 [6400/37800 (1%)]\tLosses bn: 0.011273 drop: 0.011480 plain: 0.011614\n",
      "Train Epoch: 31 [12800/37800 (1%)]\tLosses bn: 0.029208 drop: 0.085088 plain: 0.083150\n",
      "Train Epoch: 31 [19200/37800 (2%)]\tLosses bn: 0.010293 drop: 0.025073 plain: 0.026743\n",
      "Train Epoch: 31 [25600/37800 (2%)]\tLosses bn: 0.135149 drop: 0.067238 plain: 0.063201\n",
      "Train Epoch: 31 [32000/37800 (3%)]\tLosses bn: 0.016655 drop: 0.025062 plain: 0.031375\n",
      "Train Epoch: 31 [37792/37800 (3%)]\tLosses bn: 0.014971 drop: 0.037586 plain: 0.018365\n",
      "Test set:\n",
      "bn: Loss: 0.0395\tAccuracy: 4134.0/4200 (98%)\n",
      "drop: Loss: 0.0453\tAccuracy: 4128.0/4200 (98%)\n",
      "plain: Loss: 0.0464\tAccuracy: 4134.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 32 [0/37800 (0%)]\tLosses bn: 0.004149 drop: 0.002744 plain: 0.005321\n",
      "Train Epoch: 32 [6400/37800 (1%)]\tLosses bn: 0.011028 drop: 0.011050 plain: 0.011354\n",
      "Train Epoch: 32 [12800/37800 (1%)]\tLosses bn: 0.026842 drop: 0.082714 plain: 0.079978\n",
      "Train Epoch: 32 [19200/37800 (2%)]\tLosses bn: 0.009786 drop: 0.024709 plain: 0.026139\n",
      "Train Epoch: 32 [25600/37800 (2%)]\tLosses bn: 0.130040 drop: 0.065627 plain: 0.061729\n",
      "Train Epoch: 32 [32000/37800 (3%)]\tLosses bn: 0.015617 drop: 0.024090 plain: 0.030073\n",
      "Train Epoch: 32 [37792/37800 (3%)]\tLosses bn: 0.014303 drop: 0.037240 plain: 0.017945\n",
      "Test set:\n",
      "bn: Loss: 0.0391\tAccuracy: 4132.0/4200 (98%)\n",
      "drop: Loss: 0.0445\tAccuracy: 4129.0/4200 (98%)\n",
      "plain: Loss: 0.0456\tAccuracy: 4135.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 33 [0/37800 (0%)]\tLosses bn: 0.004014 drop: 0.002637 plain: 0.005145\n",
      "Train Epoch: 33 [6400/37800 (1%)]\tLosses bn: 0.010649 drop: 0.010614 plain: 0.011095\n",
      "Train Epoch: 33 [12800/37800 (1%)]\tLosses bn: 0.024919 drop: 0.080352 plain: 0.076982\n",
      "Train Epoch: 33 [19200/37800 (2%)]\tLosses bn: 0.009344 drop: 0.024339 plain: 0.025551\n",
      "Train Epoch: 33 [25600/37800 (2%)]\tLosses bn: 0.123570 drop: 0.064045 plain: 0.060446\n",
      "Train Epoch: 33 [32000/37800 (3%)]\tLosses bn: 0.014832 drop: 0.023182 plain: 0.028855\n",
      "Train Epoch: 33 [37792/37800 (3%)]\tLosses bn: 0.014090 drop: 0.036893 plain: 0.017531\n",
      "Test set:\n",
      "bn: Loss: 0.0388\tAccuracy: 4132.0/4200 (98%)\n",
      "drop: Loss: 0.0438\tAccuracy: 4130.0/4200 (98%)\n",
      "plain: Loss: 0.0448\tAccuracy: 4137.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 34 [0/37800 (0%)]\tLosses bn: 0.003881 drop: 0.002542 plain: 0.004986\n",
      "Train Epoch: 34 [6400/37800 (1%)]\tLosses bn: 0.010242 drop: 0.010185 plain: 0.010839\n",
      "Train Epoch: 34 [12800/37800 (1%)]\tLosses bn: 0.023128 drop: 0.077953 plain: 0.074144\n",
      "Train Epoch: 34 [19200/37800 (2%)]\tLosses bn: 0.008933 drop: 0.023974 plain: 0.025041\n",
      "Train Epoch: 34 [25600/37800 (2%)]\tLosses bn: 0.116531 drop: 0.062519 plain: 0.059202\n",
      "Train Epoch: 34 [32000/37800 (3%)]\tLosses bn: 0.013938 drop: 0.022334 plain: 0.027696\n",
      "Train Epoch: 34 [37792/37800 (3%)]\tLosses bn: 0.013892 drop: 0.036524 plain: 0.017173\n",
      "Test set:\n",
      "bn: Loss: 0.0385\tAccuracy: 4132.0/4200 (98%)\n",
      "drop: Loss: 0.0431\tAccuracy: 4130.0/4200 (98%)\n",
      "plain: Loss: 0.0441\tAccuracy: 4137.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 35 [0/37800 (0%)]\tLosses bn: 0.003775 drop: 0.002450 plain: 0.004854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 35 [6400/37800 (1%)]\tLosses bn: 0.009835 drop: 0.009801 plain: 0.010579\n",
      "Train Epoch: 35 [12800/37800 (1%)]\tLosses bn: 0.021223 drop: 0.075612 plain: 0.071387\n",
      "Train Epoch: 35 [19200/37800 (2%)]\tLosses bn: 0.008532 drop: 0.023591 plain: 0.024497\n",
      "Train Epoch: 35 [25600/37800 (2%)]\tLosses bn: 0.110460 drop: 0.061016 plain: 0.057947\n",
      "Train Epoch: 35 [32000/37800 (3%)]\tLosses bn: 0.013336 drop: 0.021519 plain: 0.026586\n",
      "Train Epoch: 35 [37792/37800 (3%)]\tLosses bn: 0.013214 drop: 0.036160 plain: 0.016824\n",
      "Test set:\n",
      "bn: Loss: 0.0382\tAccuracy: 4132.0/4200 (98%)\n",
      "drop: Loss: 0.0425\tAccuracy: 4133.0/4200 (98%)\n",
      "plain: Loss: 0.0435\tAccuracy: 4137.0/4200 (98%)\n",
      "\n",
      "Train Epoch: 36 [0/37800 (0%)]\tLosses bn: 0.003608 drop: 0.002370 plain: 0.004728\n",
      "Train Epoch: 36 [6400/37800 (1%)]\tLosses bn: 0.009620 drop: 0.009422 plain: 0.010309\n",
      "Train Epoch: 36 [12800/37800 (1%)]\tLosses bn: 0.019783 drop: 0.073209 plain: 0.068738\n",
      "Train Epoch: 36 [19200/37800 (2%)]\tLosses bn: 0.008179 drop: 0.023202 plain: 0.024013\n",
      "Train Epoch: 36 [25600/37800 (2%)]\tLosses bn: 0.103594 drop: 0.059559 plain: 0.056868\n",
      "Train Epoch: 36 [32000/37800 (3%)]\tLosses bn: 0.012758 drop: 0.020764 plain: 0.025570\n",
      "Train Epoch: 36 [37792/37800 (3%)]\tLosses bn: 0.012944 drop: 0.035718 plain: 0.016459\n",
      "Test set:\n",
      "bn: Loss: 0.0380\tAccuracy: 4133.0/4200 (98%)\n",
      "drop: Loss: 0.0418\tAccuracy: 4135.0/4200 (98%)\n",
      "plain: Loss: 0.0429\tAccuracy: 4138.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 37 [0/37800 (0%)]\tLosses bn: 0.003550 drop: 0.002302 plain: 0.004622\n",
      "Train Epoch: 37 [6400/37800 (1%)]\tLosses bn: 0.009242 drop: 0.009061 plain: 0.010033\n",
      "Train Epoch: 37 [12800/37800 (1%)]\tLosses bn: 0.018145 drop: 0.070831 plain: 0.066163\n",
      "Train Epoch: 37 [19200/37800 (2%)]\tLosses bn: 0.007945 drop: 0.022875 plain: 0.023555\n",
      "Train Epoch: 37 [25600/37800 (2%)]\tLosses bn: 0.098114 drop: 0.058111 plain: 0.055705\n",
      "Train Epoch: 37 [32000/37800 (3%)]\tLosses bn: 0.012425 drop: 0.020076 plain: 0.024624\n",
      "Train Epoch: 37 [37792/37800 (3%)]\tLosses bn: 0.012253 drop: 0.035175 plain: 0.016121\n",
      "Test set:\n",
      "bn: Loss: 0.0378\tAccuracy: 4132.0/4200 (98%)\n",
      "drop: Loss: 0.0413\tAccuracy: 4138.0/4200 (99%)\n",
      "plain: Loss: 0.0423\tAccuracy: 4140.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 38 [0/37800 (0%)]\tLosses bn: 0.003436 drop: 0.002239 plain: 0.004509\n",
      "Train Epoch: 38 [6400/37800 (1%)]\tLosses bn: 0.008826 drop: 0.008710 plain: 0.009808\n",
      "Train Epoch: 38 [12800/37800 (1%)]\tLosses bn: 0.016835 drop: 0.068488 plain: 0.063567\n",
      "Train Epoch: 38 [19200/37800 (2%)]\tLosses bn: 0.007476 drop: 0.022480 plain: 0.023130\n",
      "Train Epoch: 38 [25600/37800 (2%)]\tLosses bn: 0.091167 drop: 0.056731 plain: 0.054580\n",
      "Train Epoch: 38 [32000/37800 (3%)]\tLosses bn: 0.012190 drop: 0.019411 plain: 0.023675\n",
      "Train Epoch: 38 [37792/37800 (3%)]\tLosses bn: 0.011946 drop: 0.034760 plain: 0.015793\n",
      "Test set:\n",
      "bn: Loss: 0.0375\tAccuracy: 4133.0/4200 (98%)\n",
      "drop: Loss: 0.0407\tAccuracy: 4138.0/4200 (99%)\n",
      "plain: Loss: 0.0417\tAccuracy: 4140.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 39 [0/37800 (0%)]\tLosses bn: 0.003357 drop: 0.002183 plain: 0.004418\n",
      "Train Epoch: 39 [6400/37800 (1%)]\tLosses bn: 0.008318 drop: 0.008372 plain: 0.009585\n",
      "Train Epoch: 39 [12800/37800 (1%)]\tLosses bn: 0.015516 drop: 0.066155 plain: 0.061167\n",
      "Train Epoch: 39 [19200/37800 (2%)]\tLosses bn: 0.007121 drop: 0.022087 plain: 0.022708\n",
      "Train Epoch: 39 [25600/37800 (2%)]\tLosses bn: 0.084235 drop: 0.055310 plain: 0.053557\n",
      "Train Epoch: 39 [32000/37800 (3%)]\tLosses bn: 0.011640 drop: 0.018721 plain: 0.022798\n",
      "Train Epoch: 39 [37792/37800 (3%)]\tLosses bn: 0.011331 drop: 0.034283 plain: 0.015481\n",
      "Test set:\n",
      "bn: Loss: 0.0372\tAccuracy: 4132.0/4200 (98%)\n",
      "drop: Loss: 0.0402\tAccuracy: 4139.0/4200 (99%)\n",
      "plain: Loss: 0.0412\tAccuracy: 4139.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 40 [0/37800 (0%)]\tLosses bn: 0.003314 drop: 0.002134 plain: 0.004336\n",
      "Train Epoch: 40 [6400/37800 (1%)]\tLosses bn: 0.008100 drop: 0.008063 plain: 0.009366\n",
      "Train Epoch: 40 [12800/37800 (1%)]\tLosses bn: 0.014342 drop: 0.063817 plain: 0.058817\n",
      "Train Epoch: 40 [19200/37800 (2%)]\tLosses bn: 0.006909 drop: 0.021696 plain: 0.022285\n",
      "Train Epoch: 40 [25600/37800 (2%)]\tLosses bn: 0.077177 drop: 0.053872 plain: 0.052469\n",
      "Train Epoch: 40 [32000/37800 (3%)]\tLosses bn: 0.011227 drop: 0.018141 plain: 0.021970\n",
      "Train Epoch: 40 [37792/37800 (3%)]\tLosses bn: 0.010858 drop: 0.033861 plain: 0.015179\n",
      "Test set:\n",
      "bn: Loss: 0.0370\tAccuracy: 4133.0/4200 (98%)\n",
      "drop: Loss: 0.0398\tAccuracy: 4140.0/4200 (99%)\n",
      "plain: Loss: 0.0408\tAccuracy: 4140.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 41 [0/37800 (0%)]\tLosses bn: 0.003170 drop: 0.002088 plain: 0.004258\n",
      "Train Epoch: 41 [6400/37800 (1%)]\tLosses bn: 0.007572 drop: 0.007780 plain: 0.009154\n",
      "Train Epoch: 41 [12800/37800 (1%)]\tLosses bn: 0.013190 drop: 0.061520 plain: 0.056556\n",
      "Train Epoch: 41 [19200/37800 (2%)]\tLosses bn: 0.006542 drop: 0.021388 plain: 0.021883\n",
      "Train Epoch: 41 [25600/37800 (2%)]\tLosses bn: 0.070045 drop: 0.052402 plain: 0.051460\n",
      "Train Epoch: 41 [32000/37800 (3%)]\tLosses bn: 0.010861 drop: 0.017530 plain: 0.021147\n",
      "Train Epoch: 41 [37792/37800 (3%)]\tLosses bn: 0.010158 drop: 0.033440 plain: 0.014899\n",
      "Test set:\n",
      "bn: Loss: 0.0368\tAccuracy: 4133.0/4200 (98%)\n",
      "drop: Loss: 0.0393\tAccuracy: 4140.0/4200 (99%)\n",
      "plain: Loss: 0.0403\tAccuracy: 4141.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 42 [0/37800 (0%)]\tLosses bn: 0.003068 drop: 0.002046 plain: 0.004194\n",
      "Train Epoch: 42 [6400/37800 (1%)]\tLosses bn: 0.007233 drop: 0.007507 plain: 0.008937\n",
      "Train Epoch: 42 [12800/37800 (1%)]\tLosses bn: 0.012410 drop: 0.059225 plain: 0.054338\n",
      "Train Epoch: 42 [19200/37800 (2%)]\tLosses bn: 0.006303 drop: 0.021017 plain: 0.021515\n",
      "Train Epoch: 42 [25600/37800 (2%)]\tLosses bn: 0.062941 drop: 0.050988 plain: 0.050508\n",
      "Train Epoch: 42 [32000/37800 (3%)]\tLosses bn: 0.010724 drop: 0.016980 plain: 0.020363\n",
      "Train Epoch: 42 [37792/37800 (3%)]\tLosses bn: 0.009834 drop: 0.032986 plain: 0.014636\n",
      "Test set:\n",
      "bn: Loss: 0.0366\tAccuracy: 4133.0/4200 (98%)\n",
      "drop: Loss: 0.0389\tAccuracy: 4140.0/4200 (99%)\n",
      "plain: Loss: 0.0399\tAccuracy: 4144.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 43 [0/37800 (0%)]\tLosses bn: 0.002964 drop: 0.002008 plain: 0.004140\n",
      "Train Epoch: 43 [6400/37800 (1%)]\tLosses bn: 0.006664 drop: 0.007251 plain: 0.008743\n",
      "Train Epoch: 43 [12800/37800 (1%)]\tLosses bn: 0.011632 drop: 0.057023 plain: 0.052184\n",
      "Train Epoch: 43 [19200/37800 (2%)]\tLosses bn: 0.006018 drop: 0.020692 plain: 0.021107\n",
      "Train Epoch: 43 [25600/37800 (2%)]\tLosses bn: 0.056003 drop: 0.049593 plain: 0.049563\n",
      "Train Epoch: 43 [32000/37800 (3%)]\tLosses bn: 0.010267 drop: 0.016423 plain: 0.019598\n",
      "Train Epoch: 43 [37792/37800 (3%)]\tLosses bn: 0.009357 drop: 0.032554 plain: 0.014388\n",
      "Test set:\n",
      "bn: Loss: 0.0363\tAccuracy: 4134.0/4200 (98%)\n",
      "drop: Loss: 0.0385\tAccuracy: 4141.0/4200 (99%)\n",
      "plain: Loss: 0.0395\tAccuracy: 4144.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 44 [0/37800 (0%)]\tLosses bn: 0.002794 drop: 0.001974 plain: 0.004095\n",
      "Train Epoch: 44 [6400/37800 (1%)]\tLosses bn: 0.006226 drop: 0.007019 plain: 0.008536\n",
      "Train Epoch: 44 [12800/37800 (1%)]\tLosses bn: 0.010778 drop: 0.054806 plain: 0.050143\n",
      "Train Epoch: 44 [19200/37800 (2%)]\tLosses bn: 0.005949 drop: 0.020365 plain: 0.020701\n",
      "Train Epoch: 44 [25600/37800 (2%)]\tLosses bn: 0.049926 drop: 0.048206 plain: 0.048648\n",
      "Train Epoch: 44 [32000/37800 (3%)]\tLosses bn: 0.010116 drop: 0.015910 plain: 0.018888\n",
      "Train Epoch: 44 [37792/37800 (3%)]\tLosses bn: 0.008923 drop: 0.032094 plain: 0.014146\n",
      "Test set:\n",
      "bn: Loss: 0.0361\tAccuracy: 4134.0/4200 (98%)\n",
      "drop: Loss: 0.0381\tAccuracy: 4141.0/4200 (99%)\n",
      "plain: Loss: 0.0391\tAccuracy: 4144.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 45 [0/37800 (0%)]\tLosses bn: 0.002725 drop: 0.001942 plain: 0.004048\n",
      "Train Epoch: 45 [6400/37800 (1%)]\tLosses bn: 0.005745 drop: 0.006777 plain: 0.008345\n",
      "Train Epoch: 45 [12800/37800 (1%)]\tLosses bn: 0.009969 drop: 0.052669 plain: 0.048126\n",
      "Train Epoch: 45 [19200/37800 (2%)]\tLosses bn: 0.005542 drop: 0.020033 plain: 0.020318\n",
      "Train Epoch: 45 [25600/37800 (2%)]\tLosses bn: 0.045033 drop: 0.046734 plain: 0.047736\n",
      "Train Epoch: 45 [32000/37800 (3%)]\tLosses bn: 0.009712 drop: 0.015389 plain: 0.018223\n",
      "Train Epoch: 45 [37792/37800 (3%)]\tLosses bn: 0.008703 drop: 0.031620 plain: 0.013902\n",
      "Test set:\n",
      "bn: Loss: 0.0359\tAccuracy: 4136.0/4200 (98%)\n",
      "drop: Loss: 0.0378\tAccuracy: 4141.0/4200 (99%)\n",
      "plain: Loss: 0.0387\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 46 [0/37800 (0%)]\tLosses bn: 0.002660 drop: 0.001915 plain: 0.004009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 [6400/37800 (1%)]\tLosses bn: 0.005377 drop: 0.006552 plain: 0.008134\n",
      "Train Epoch: 46 [12800/37800 (1%)]\tLosses bn: 0.009265 drop: 0.050554 plain: 0.046176\n",
      "Train Epoch: 46 [19200/37800 (2%)]\tLosses bn: 0.005354 drop: 0.019671 plain: 0.019979\n",
      "Train Epoch: 46 [25600/37800 (2%)]\tLosses bn: 0.039422 drop: 0.045342 plain: 0.046776\n",
      "Train Epoch: 46 [32000/37800 (3%)]\tLosses bn: 0.009256 drop: 0.014903 plain: 0.017525\n",
      "Train Epoch: 46 [37792/37800 (3%)]\tLosses bn: 0.008134 drop: 0.031224 plain: 0.013662\n",
      "Test set:\n",
      "bn: Loss: 0.0357\tAccuracy: 4138.0/4200 (99%)\n",
      "drop: Loss: 0.0374\tAccuracy: 4141.0/4200 (99%)\n",
      "plain: Loss: 0.0384\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 47 [0/37800 (0%)]\tLosses bn: 0.002524 drop: 0.001890 plain: 0.003977\n",
      "Train Epoch: 47 [6400/37800 (1%)]\tLosses bn: 0.004987 drop: 0.006332 plain: 0.007927\n",
      "Train Epoch: 47 [12800/37800 (1%)]\tLosses bn: 0.008398 drop: 0.048502 plain: 0.044283\n",
      "Train Epoch: 47 [19200/37800 (2%)]\tLosses bn: 0.005053 drop: 0.019385 plain: 0.019592\n",
      "Train Epoch: 47 [25600/37800 (2%)]\tLosses bn: 0.034939 drop: 0.043946 plain: 0.045967\n",
      "Train Epoch: 47 [32000/37800 (3%)]\tLosses bn: 0.008808 drop: 0.014411 plain: 0.016880\n",
      "Train Epoch: 47 [37792/37800 (3%)]\tLosses bn: 0.007515 drop: 0.030776 plain: 0.013418\n",
      "Test set:\n",
      "bn: Loss: 0.0355\tAccuracy: 4139.0/4200 (99%)\n",
      "drop: Loss: 0.0371\tAccuracy: 4141.0/4200 (99%)\n",
      "plain: Loss: 0.0381\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 48 [0/37800 (0%)]\tLosses bn: 0.002414 drop: 0.001866 plain: 0.003940\n",
      "Train Epoch: 48 [6400/37800 (1%)]\tLosses bn: 0.004599 drop: 0.006124 plain: 0.007721\n",
      "Train Epoch: 48 [12800/37800 (1%)]\tLosses bn: 0.007901 drop: 0.046517 plain: 0.042448\n",
      "Train Epoch: 48 [19200/37800 (2%)]\tLosses bn: 0.004796 drop: 0.019046 plain: 0.019161\n",
      "Train Epoch: 48 [25600/37800 (2%)]\tLosses bn: 0.030663 drop: 0.042706 plain: 0.045102\n",
      "Train Epoch: 48 [32000/37800 (3%)]\tLosses bn: 0.008420 drop: 0.013925 plain: 0.016278\n",
      "Train Epoch: 48 [37792/37800 (3%)]\tLosses bn: 0.007170 drop: 0.030335 plain: 0.013173\n",
      "Test set:\n",
      "bn: Loss: 0.0353\tAccuracy: 4140.0/4200 (99%)\n",
      "drop: Loss: 0.0368\tAccuracy: 4141.0/4200 (99%)\n",
      "plain: Loss: 0.0378\tAccuracy: 4146.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 49 [0/37800 (0%)]\tLosses bn: 0.002328 drop: 0.001841 plain: 0.003909\n",
      "Train Epoch: 49 [6400/37800 (1%)]\tLosses bn: 0.004266 drop: 0.005918 plain: 0.007531\n",
      "Train Epoch: 49 [12800/37800 (1%)]\tLosses bn: 0.007186 drop: 0.044611 plain: 0.040641\n",
      "Train Epoch: 49 [19200/37800 (2%)]\tLosses bn: 0.004577 drop: 0.018778 plain: 0.018804\n",
      "Train Epoch: 49 [25600/37800 (2%)]\tLosses bn: 0.027198 drop: 0.041459 plain: 0.044250\n",
      "Train Epoch: 49 [32000/37800 (3%)]\tLosses bn: 0.008089 drop: 0.013468 plain: 0.015698\n",
      "Train Epoch: 49 [37792/37800 (3%)]\tLosses bn: 0.006740 drop: 0.029887 plain: 0.012944\n",
      "Test set:\n",
      "bn: Loss: 0.0351\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0365\tAccuracy: 4141.0/4200 (99%)\n",
      "plain: Loss: 0.0375\tAccuracy: 4146.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 50 [0/37800 (0%)]\tLosses bn: 0.002249 drop: 0.001821 plain: 0.003889\n",
      "Train Epoch: 50 [6400/37800 (1%)]\tLosses bn: 0.003914 drop: 0.005737 plain: 0.007335\n",
      "Train Epoch: 50 [12800/37800 (1%)]\tLosses bn: 0.006702 drop: 0.042708 plain: 0.038923\n",
      "Train Epoch: 50 [19200/37800 (2%)]\tLosses bn: 0.004393 drop: 0.018501 plain: 0.018404\n",
      "Train Epoch: 50 [25600/37800 (2%)]\tLosses bn: 0.023649 drop: 0.040198 plain: 0.043124\n",
      "Train Epoch: 50 [32000/37800 (3%)]\tLosses bn: 0.007677 drop: 0.013034 plain: 0.015143\n",
      "Train Epoch: 50 [37792/37800 (3%)]\tLosses bn: 0.006482 drop: 0.029492 plain: 0.012699\n",
      "Test set:\n",
      "bn: Loss: 0.0349\tAccuracy: 4140.0/4200 (99%)\n",
      "drop: Loss: 0.0362\tAccuracy: 4140.0/4200 (99%)\n",
      "plain: Loss: 0.0372\tAccuracy: 4146.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 51 [0/37800 (0%)]\tLosses bn: 0.002164 drop: 0.001800 plain: 0.003848\n",
      "Train Epoch: 51 [6400/37800 (1%)]\tLosses bn: 0.003622 drop: 0.005556 plain: 0.007153\n",
      "Train Epoch: 51 [12800/37800 (1%)]\tLosses bn: 0.006124 drop: 0.040967 plain: 0.037272\n",
      "Train Epoch: 51 [19200/37800 (2%)]\tLosses bn: 0.004154 drop: 0.018252 plain: 0.018072\n",
      "Train Epoch: 51 [25600/37800 (2%)]\tLosses bn: 0.020916 drop: 0.039005 plain: 0.042133\n",
      "Train Epoch: 51 [32000/37800 (3%)]\tLosses bn: 0.007396 drop: 0.012600 plain: 0.014630\n",
      "Train Epoch: 51 [37792/37800 (3%)]\tLosses bn: 0.005998 drop: 0.029005 plain: 0.012463\n",
      "Test set:\n",
      "bn: Loss: 0.0347\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0360\tAccuracy: 4140.0/4200 (99%)\n",
      "plain: Loss: 0.0369\tAccuracy: 4146.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 52 [0/37800 (0%)]\tLosses bn: 0.002057 drop: 0.001782 plain: 0.003802\n",
      "Train Epoch: 52 [6400/37800 (1%)]\tLosses bn: 0.003370 drop: 0.005384 plain: 0.006971\n",
      "Train Epoch: 52 [12800/37800 (1%)]\tLosses bn: 0.005615 drop: 0.039249 plain: 0.035678\n",
      "Train Epoch: 52 [19200/37800 (2%)]\tLosses bn: 0.003937 drop: 0.017964 plain: 0.017735\n",
      "Train Epoch: 52 [25600/37800 (2%)]\tLosses bn: 0.018315 drop: 0.037834 plain: 0.041157\n",
      "Train Epoch: 52 [32000/37800 (3%)]\tLosses bn: 0.006879 drop: 0.012180 plain: 0.014120\n",
      "Train Epoch: 52 [37792/37800 (3%)]\tLosses bn: 0.005661 drop: 0.028480 plain: 0.012244\n",
      "Test set:\n",
      "bn: Loss: 0.0347\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0357\tAccuracy: 4141.0/4200 (99%)\n",
      "plain: Loss: 0.0367\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 53 [0/37800 (0%)]\tLosses bn: 0.002069 drop: 0.001765 plain: 0.003766\n",
      "Train Epoch: 53 [6400/37800 (1%)]\tLosses bn: 0.003110 drop: 0.005217 plain: 0.006766\n",
      "Train Epoch: 53 [12800/37800 (1%)]\tLosses bn: 0.005137 drop: 0.037558 plain: 0.034168\n",
      "Train Epoch: 53 [19200/37800 (2%)]\tLosses bn: 0.003640 drop: 0.017715 plain: 0.017386\n",
      "Train Epoch: 53 [25600/37800 (2%)]\tLosses bn: 0.016883 drop: 0.036692 plain: 0.040174\n",
      "Train Epoch: 53 [32000/37800 (3%)]\tLosses bn: 0.006445 drop: 0.011790 plain: 0.013611\n",
      "Train Epoch: 53 [37792/37800 (3%)]\tLosses bn: 0.005283 drop: 0.027965 plain: 0.012046\n",
      "Test set:\n",
      "bn: Loss: 0.0344\tAccuracy: 4145.0/4200 (99%)\n",
      "drop: Loss: 0.0355\tAccuracy: 4142.0/4200 (99%)\n",
      "plain: Loss: 0.0365\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 54 [0/37800 (0%)]\tLosses bn: 0.001968 drop: 0.001747 plain: 0.003731\n",
      "Train Epoch: 54 [6400/37800 (1%)]\tLosses bn: 0.002913 drop: 0.005056 plain: 0.006583\n",
      "Train Epoch: 54 [12800/37800 (1%)]\tLosses bn: 0.004766 drop: 0.035942 plain: 0.032756\n",
      "Train Epoch: 54 [19200/37800 (2%)]\tLosses bn: 0.003509 drop: 0.017456 plain: 0.017039\n",
      "Train Epoch: 54 [25600/37800 (2%)]\tLosses bn: 0.014509 drop: 0.035595 plain: 0.039187\n",
      "Train Epoch: 54 [32000/37800 (3%)]\tLosses bn: 0.005999 drop: 0.011424 plain: 0.013134\n",
      "Train Epoch: 54 [37792/37800 (3%)]\tLosses bn: 0.004964 drop: 0.027458 plain: 0.011849\n",
      "Test set:\n",
      "bn: Loss: 0.0343\tAccuracy: 4144.0/4200 (99%)\n",
      "drop: Loss: 0.0353\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0362\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 55 [0/37800 (0%)]\tLosses bn: 0.001866 drop: 0.001729 plain: 0.003695\n",
      "Train Epoch: 55 [6400/37800 (1%)]\tLosses bn: 0.002712 drop: 0.004913 plain: 0.006402\n",
      "Train Epoch: 55 [12800/37800 (1%)]\tLosses bn: 0.004386 drop: 0.034427 plain: 0.031432\n",
      "Train Epoch: 55 [19200/37800 (2%)]\tLosses bn: 0.003262 drop: 0.017151 plain: 0.016706\n",
      "Train Epoch: 55 [25600/37800 (2%)]\tLosses bn: 0.013159 drop: 0.034504 plain: 0.038188\n",
      "Train Epoch: 55 [32000/37800 (3%)]\tLosses bn: 0.005791 drop: 0.011046 plain: 0.012675\n",
      "Train Epoch: 55 [37792/37800 (3%)]\tLosses bn: 0.004568 drop: 0.026972 plain: 0.011660\n",
      "Test set:\n",
      "bn: Loss: 0.0341\tAccuracy: 4145.0/4200 (99%)\n",
      "drop: Loss: 0.0350\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0360\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 56 [0/37800 (0%)]\tLosses bn: 0.001775 drop: 0.001711 plain: 0.003668\n",
      "Train Epoch: 56 [6400/37800 (1%)]\tLosses bn: 0.002495 drop: 0.004765 plain: 0.006225\n",
      "Train Epoch: 56 [12800/37800 (1%)]\tLosses bn: 0.004084 drop: 0.033022 plain: 0.030125\n",
      "Train Epoch: 56 [19200/37800 (2%)]\tLosses bn: 0.003113 drop: 0.016918 plain: 0.016340\n",
      "Train Epoch: 56 [25600/37800 (2%)]\tLosses bn: 0.011610 drop: 0.033417 plain: 0.037220\n",
      "Train Epoch: 56 [32000/37800 (3%)]\tLosses bn: 0.005332 drop: 0.010703 plain: 0.012234\n",
      "Train Epoch: 56 [37792/37800 (3%)]\tLosses bn: 0.004399 drop: 0.026496 plain: 0.011471\n",
      "Test set:\n",
      "bn: Loss: 0.0340\tAccuracy: 4145.0/4200 (99%)\n",
      "drop: Loss: 0.0348\tAccuracy: 4142.0/4200 (99%)\n",
      "plain: Loss: 0.0358\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 57 [0/37800 (0%)]\tLosses bn: 0.001679 drop: 0.001694 plain: 0.003631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 57 [6400/37800 (1%)]\tLosses bn: 0.002370 drop: 0.004622 plain: 0.006060\n",
      "Train Epoch: 57 [12800/37800 (1%)]\tLosses bn: 0.003812 drop: 0.031612 plain: 0.028888\n",
      "Train Epoch: 57 [19200/37800 (2%)]\tLosses bn: 0.002923 drop: 0.016694 plain: 0.016045\n",
      "Train Epoch: 57 [25600/37800 (2%)]\tLosses bn: 0.010306 drop: 0.032277 plain: 0.036315\n",
      "Train Epoch: 57 [32000/37800 (3%)]\tLosses bn: 0.005017 drop: 0.010383 plain: 0.011814\n",
      "Train Epoch: 57 [37792/37800 (3%)]\tLosses bn: 0.003997 drop: 0.026019 plain: 0.011286\n",
      "Test set:\n",
      "bn: Loss: 0.0339\tAccuracy: 4145.0/4200 (99%)\n",
      "drop: Loss: 0.0346\tAccuracy: 4142.0/4200 (99%)\n",
      "plain: Loss: 0.0356\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 58 [0/37800 (0%)]\tLosses bn: 0.001569 drop: 0.001678 plain: 0.003603\n",
      "Train Epoch: 58 [6400/37800 (1%)]\tLosses bn: 0.002208 drop: 0.004482 plain: 0.005900\n",
      "Train Epoch: 58 [12800/37800 (1%)]\tLosses bn: 0.003382 drop: 0.030347 plain: 0.027733\n",
      "Train Epoch: 58 [19200/37800 (2%)]\tLosses bn: 0.002729 drop: 0.016426 plain: 0.015747\n",
      "Train Epoch: 58 [25600/37800 (2%)]\tLosses bn: 0.009695 drop: 0.031247 plain: 0.035391\n",
      "Train Epoch: 58 [32000/37800 (3%)]\tLosses bn: 0.004692 drop: 0.010067 plain: 0.011422\n",
      "Train Epoch: 58 [37792/37800 (3%)]\tLosses bn: 0.003698 drop: 0.025502 plain: 0.011077\n",
      "Test set:\n",
      "bn: Loss: 0.0338\tAccuracy: 4145.0/4200 (99%)\n",
      "drop: Loss: 0.0345\tAccuracy: 4142.0/4200 (99%)\n",
      "plain: Loss: 0.0355\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 59 [0/37800 (0%)]\tLosses bn: 0.001488 drop: 0.001662 plain: 0.003563\n",
      "Train Epoch: 59 [6400/37800 (1%)]\tLosses bn: 0.002044 drop: 0.004342 plain: 0.005728\n",
      "Train Epoch: 59 [12800/37800 (1%)]\tLosses bn: 0.003118 drop: 0.029069 plain: 0.026651\n",
      "Train Epoch: 59 [19200/37800 (2%)]\tLosses bn: 0.002529 drop: 0.016212 plain: 0.015431\n",
      "Train Epoch: 59 [25600/37800 (2%)]\tLosses bn: 0.008330 drop: 0.030204 plain: 0.034439\n",
      "Train Epoch: 59 [32000/37800 (3%)]\tLosses bn: 0.004286 drop: 0.009756 plain: 0.011036\n",
      "Train Epoch: 59 [37792/37800 (3%)]\tLosses bn: 0.003473 drop: 0.025001 plain: 0.010888\n",
      "Test set:\n",
      "bn: Loss: 0.0337\tAccuracy: 4147.0/4200 (99%)\n",
      "drop: Loss: 0.0343\tAccuracy: 4142.0/4200 (99%)\n",
      "plain: Loss: 0.0353\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 60 [0/37800 (0%)]\tLosses bn: 0.001418 drop: 0.001646 plain: 0.003539\n",
      "Train Epoch: 60 [6400/37800 (1%)]\tLosses bn: 0.001924 drop: 0.004204 plain: 0.005565\n",
      "Train Epoch: 60 [12800/37800 (1%)]\tLosses bn: 0.002982 drop: 0.027928 plain: 0.025614\n",
      "Train Epoch: 60 [19200/37800 (2%)]\tLosses bn: 0.002421 drop: 0.015974 plain: 0.015153\n",
      "Train Epoch: 60 [25600/37800 (2%)]\tLosses bn: 0.007854 drop: 0.029246 plain: 0.033638\n",
      "Train Epoch: 60 [32000/37800 (3%)]\tLosses bn: 0.004020 drop: 0.009441 plain: 0.010696\n",
      "Train Epoch: 60 [37792/37800 (3%)]\tLosses bn: 0.003237 drop: 0.024532 plain: 0.010687\n",
      "Test set:\n",
      "bn: Loss: 0.0336\tAccuracy: 4146.0/4200 (99%)\n",
      "drop: Loss: 0.0341\tAccuracy: 4142.0/4200 (99%)\n",
      "plain: Loss: 0.0351\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 61 [0/37800 (0%)]\tLosses bn: 0.001296 drop: 0.001627 plain: 0.003506\n",
      "Train Epoch: 61 [6400/37800 (1%)]\tLosses bn: 0.001782 drop: 0.004072 plain: 0.005417\n",
      "Train Epoch: 61 [12800/37800 (1%)]\tLosses bn: 0.002665 drop: 0.026732 plain: 0.024619\n",
      "Train Epoch: 61 [19200/37800 (2%)]\tLosses bn: 0.002281 drop: 0.015726 plain: 0.014828\n",
      "Train Epoch: 61 [25600/37800 (2%)]\tLosses bn: 0.007159 drop: 0.028244 plain: 0.032730\n",
      "Train Epoch: 61 [32000/37800 (3%)]\tLosses bn: 0.003714 drop: 0.009159 plain: 0.010352\n",
      "Train Epoch: 61 [37792/37800 (3%)]\tLosses bn: 0.002956 drop: 0.024115 plain: 0.010499\n",
      "Test set:\n",
      "bn: Loss: 0.0335\tAccuracy: 4144.0/4200 (99%)\n",
      "drop: Loss: 0.0340\tAccuracy: 4142.0/4200 (99%)\n",
      "plain: Loss: 0.0350\tAccuracy: 4142.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 62 [0/37800 (0%)]\tLosses bn: 0.001208 drop: 0.001609 plain: 0.003469\n",
      "Train Epoch: 62 [6400/37800 (1%)]\tLosses bn: 0.001632 drop: 0.003956 plain: 0.005265\n",
      "Train Epoch: 62 [12800/37800 (1%)]\tLosses bn: 0.002450 drop: 0.025699 plain: 0.023686\n",
      "Train Epoch: 62 [19200/37800 (2%)]\tLosses bn: 0.001949 drop: 0.015504 plain: 0.014511\n",
      "Train Epoch: 62 [25600/37800 (2%)]\tLosses bn: 0.006266 drop: 0.027323 plain: 0.031906\n",
      "Train Epoch: 62 [32000/37800 (3%)]\tLosses bn: 0.003464 drop: 0.008863 plain: 0.010044\n",
      "Train Epoch: 62 [37792/37800 (3%)]\tLosses bn: 0.002821 drop: 0.023631 plain: 0.010285\n",
      "Test set:\n",
      "bn: Loss: 0.0335\tAccuracy: 4144.0/4200 (99%)\n",
      "drop: Loss: 0.0338\tAccuracy: 4142.0/4200 (99%)\n",
      "plain: Loss: 0.0348\tAccuracy: 4142.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 63 [0/37800 (0%)]\tLosses bn: 0.001155 drop: 0.001589 plain: 0.003452\n",
      "Train Epoch: 63 [6400/37800 (1%)]\tLosses bn: 0.001548 drop: 0.003842 plain: 0.005123\n",
      "Train Epoch: 63 [12800/37800 (1%)]\tLosses bn: 0.002236 drop: 0.024624 plain: 0.022818\n",
      "Train Epoch: 63 [19200/37800 (2%)]\tLosses bn: 0.001885 drop: 0.015283 plain: 0.014189\n",
      "Train Epoch: 63 [25600/37800 (2%)]\tLosses bn: 0.006004 drop: 0.026394 plain: 0.031086\n",
      "Train Epoch: 63 [32000/37800 (3%)]\tLosses bn: 0.003278 drop: 0.008609 plain: 0.009716\n",
      "Train Epoch: 63 [37792/37800 (3%)]\tLosses bn: 0.002567 drop: 0.023199 plain: 0.010115\n",
      "Test set:\n",
      "bn: Loss: 0.0334\tAccuracy: 4144.0/4200 (99%)\n",
      "drop: Loss: 0.0337\tAccuracy: 4142.0/4200 (99%)\n",
      "plain: Loss: 0.0347\tAccuracy: 4142.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 64 [0/37800 (0%)]\tLosses bn: 0.001053 drop: 0.001570 plain: 0.003413\n",
      "Train Epoch: 64 [6400/37800 (1%)]\tLosses bn: 0.001429 drop: 0.003739 plain: 0.004978\n",
      "Train Epoch: 64 [12800/37800 (1%)]\tLosses bn: 0.002059 drop: 0.023652 plain: 0.021974\n",
      "Train Epoch: 64 [19200/37800 (2%)]\tLosses bn: 0.001839 drop: 0.015059 plain: 0.013915\n",
      "Train Epoch: 64 [25600/37800 (2%)]\tLosses bn: 0.005263 drop: 0.025576 plain: 0.030355\n",
      "Train Epoch: 64 [32000/37800 (3%)]\tLosses bn: 0.003058 drop: 0.008353 plain: 0.009426\n",
      "Train Epoch: 64 [37792/37800 (3%)]\tLosses bn: 0.002423 drop: 0.022756 plain: 0.009897\n",
      "Test set:\n",
      "bn: Loss: 0.0333\tAccuracy: 4144.0/4200 (99%)\n",
      "drop: Loss: 0.0336\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0346\tAccuracy: 4142.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 65 [0/37800 (0%)]\tLosses bn: 0.000997 drop: 0.001554 plain: 0.003381\n",
      "Train Epoch: 65 [6400/37800 (1%)]\tLosses bn: 0.001333 drop: 0.003636 plain: 0.004846\n",
      "Train Epoch: 65 [12800/37800 (1%)]\tLosses bn: 0.001883 drop: 0.022715 plain: 0.021180\n",
      "Train Epoch: 65 [19200/37800 (2%)]\tLosses bn: 0.001551 drop: 0.014839 plain: 0.013665\n",
      "Train Epoch: 65 [25600/37800 (2%)]\tLosses bn: 0.004637 drop: 0.024705 plain: 0.029564\n",
      "Train Epoch: 65 [32000/37800 (3%)]\tLosses bn: 0.002757 drop: 0.008103 plain: 0.009137\n",
      "Train Epoch: 65 [37792/37800 (3%)]\tLosses bn: 0.002257 drop: 0.022282 plain: 0.009724\n",
      "Test set:\n",
      "bn: Loss: 0.0334\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0334\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0344\tAccuracy: 4142.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 66 [0/37800 (0%)]\tLosses bn: 0.000911 drop: 0.001536 plain: 0.003346\n",
      "Train Epoch: 66 [6400/37800 (1%)]\tLosses bn: 0.001222 drop: 0.003541 plain: 0.004715\n",
      "Train Epoch: 66 [12800/37800 (1%)]\tLosses bn: 0.001734 drop: 0.021851 plain: 0.020415\n",
      "Train Epoch: 66 [19200/37800 (2%)]\tLosses bn: 0.001582 drop: 0.014673 plain: 0.013451\n",
      "Train Epoch: 66 [25600/37800 (2%)]\tLosses bn: 0.004256 drop: 0.023927 plain: 0.028840\n",
      "Train Epoch: 66 [32000/37800 (3%)]\tLosses bn: 0.002666 drop: 0.007863 plain: 0.008855\n",
      "Train Epoch: 66 [37792/37800 (3%)]\tLosses bn: 0.002054 drop: 0.021852 plain: 0.009538\n",
      "Test set:\n",
      "bn: Loss: 0.0333\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0333\tAccuracy: 4142.0/4200 (99%)\n",
      "plain: Loss: 0.0343\tAccuracy: 4142.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 67 [0/37800 (0%)]\tLosses bn: 0.000853 drop: 0.001519 plain: 0.003313\n",
      "Train Epoch: 67 [6400/37800 (1%)]\tLosses bn: 0.001144 drop: 0.003444 plain: 0.004584\n",
      "Train Epoch: 67 [12800/37800 (1%)]\tLosses bn: 0.001597 drop: 0.021060 plain: 0.019671\n",
      "Train Epoch: 67 [19200/37800 (2%)]\tLosses bn: 0.001418 drop: 0.014464 plain: 0.013199\n",
      "Train Epoch: 67 [25600/37800 (2%)]\tLosses bn: 0.003818 drop: 0.023137 plain: 0.028029\n",
      "Train Epoch: 67 [32000/37800 (3%)]\tLosses bn: 0.002448 drop: 0.007628 plain: 0.008581\n",
      "Train Epoch: 67 [37792/37800 (3%)]\tLosses bn: 0.001913 drop: 0.021372 plain: 0.009341\n",
      "Test set:\n",
      "bn: Loss: 0.0333\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0332\tAccuracy: 4142.0/4200 (99%)\n",
      "plain: Loss: 0.0342\tAccuracy: 4142.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 68 [0/37800 (0%)]\tLosses bn: 0.000780 drop: 0.001496 plain: 0.003286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 68 [6400/37800 (1%)]\tLosses bn: 0.001053 drop: 0.003349 plain: 0.004460\n",
      "Train Epoch: 68 [12800/37800 (1%)]\tLosses bn: 0.001478 drop: 0.020303 plain: 0.019007\n",
      "Train Epoch: 68 [19200/37800 (2%)]\tLosses bn: 0.001243 drop: 0.014268 plain: 0.012953\n",
      "Train Epoch: 68 [25600/37800 (2%)]\tLosses bn: 0.003541 drop: 0.022440 plain: 0.027243\n",
      "Train Epoch: 68 [32000/37800 (3%)]\tLosses bn: 0.002213 drop: 0.007383 plain: 0.008318\n",
      "Train Epoch: 68 [37792/37800 (3%)]\tLosses bn: 0.001790 drop: 0.020928 plain: 0.009159\n",
      "Test set:\n",
      "bn: Loss: 0.0334\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0331\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0341\tAccuracy: 4142.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 69 [0/37800 (0%)]\tLosses bn: 0.000731 drop: 0.001476 plain: 0.003236\n",
      "Train Epoch: 69 [6400/37800 (1%)]\tLosses bn: 0.000955 drop: 0.003253 plain: 0.004326\n",
      "Train Epoch: 69 [12800/37800 (1%)]\tLosses bn: 0.001393 drop: 0.019522 plain: 0.018316\n",
      "Train Epoch: 69 [19200/37800 (2%)]\tLosses bn: 0.001207 drop: 0.014126 plain: 0.012674\n",
      "Train Epoch: 69 [25600/37800 (2%)]\tLosses bn: 0.003134 drop: 0.021681 plain: 0.026501\n",
      "Train Epoch: 69 [32000/37800 (3%)]\tLosses bn: 0.002065 drop: 0.007162 plain: 0.008058\n",
      "Train Epoch: 69 [37792/37800 (3%)]\tLosses bn: 0.001617 drop: 0.020494 plain: 0.008970\n",
      "Test set:\n",
      "bn: Loss: 0.0334\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0330\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0340\tAccuracy: 4142.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 70 [0/37800 (0%)]\tLosses bn: 0.000664 drop: 0.001455 plain: 0.003212\n",
      "Train Epoch: 70 [6400/37800 (1%)]\tLosses bn: 0.000914 drop: 0.003160 plain: 0.004198\n",
      "Train Epoch: 70 [12800/37800 (1%)]\tLosses bn: 0.001262 drop: 0.018808 plain: 0.017671\n",
      "Train Epoch: 70 [19200/37800 (2%)]\tLosses bn: 0.001118 drop: 0.013946 plain: 0.012446\n",
      "Train Epoch: 70 [25600/37800 (2%)]\tLosses bn: 0.002910 drop: 0.020979 plain: 0.025775\n",
      "Train Epoch: 70 [32000/37800 (3%)]\tLosses bn: 0.001945 drop: 0.006943 plain: 0.007818\n",
      "Train Epoch: 70 [37792/37800 (3%)]\tLosses bn: 0.001469 drop: 0.020016 plain: 0.008804\n",
      "Test set:\n",
      "bn: Loss: 0.0334\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0329\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0339\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 71 [0/37800 (0%)]\tLosses bn: 0.000619 drop: 0.001435 plain: 0.003165\n",
      "Train Epoch: 71 [6400/37800 (1%)]\tLosses bn: 0.000819 drop: 0.003074 plain: 0.004072\n",
      "Train Epoch: 71 [12800/37800 (1%)]\tLosses bn: 0.001178 drop: 0.018082 plain: 0.017085\n",
      "Train Epoch: 71 [19200/37800 (2%)]\tLosses bn: 0.001031 drop: 0.013762 plain: 0.012187\n",
      "Train Epoch: 71 [25600/37800 (2%)]\tLosses bn: 0.002567 drop: 0.020263 plain: 0.025198\n",
      "Train Epoch: 71 [32000/37800 (3%)]\tLosses bn: 0.001754 drop: 0.006744 plain: 0.007586\n",
      "Train Epoch: 71 [37792/37800 (3%)]\tLosses bn: 0.001330 drop: 0.019506 plain: 0.008624\n",
      "Test set:\n",
      "bn: Loss: 0.0335\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0328\tAccuracy: 4144.0/4200 (99%)\n",
      "plain: Loss: 0.0338\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 72 [0/37800 (0%)]\tLosses bn: 0.000582 drop: 0.001411 plain: 0.003129\n",
      "Train Epoch: 72 [6400/37800 (1%)]\tLosses bn: 0.000760 drop: 0.002985 plain: 0.003947\n",
      "Train Epoch: 72 [12800/37800 (1%)]\tLosses bn: 0.001067 drop: 0.017433 plain: 0.016470\n",
      "Train Epoch: 72 [19200/37800 (2%)]\tLosses bn: 0.000904 drop: 0.013553 plain: 0.011927\n",
      "Train Epoch: 72 [25600/37800 (2%)]\tLosses bn: 0.002309 drop: 0.019573 plain: 0.024583\n",
      "Train Epoch: 72 [32000/37800 (3%)]\tLosses bn: 0.001604 drop: 0.006549 plain: 0.007358\n",
      "Train Epoch: 72 [37792/37800 (3%)]\tLosses bn: 0.001207 drop: 0.019137 plain: 0.008444\n",
      "Test set:\n",
      "bn: Loss: 0.0335\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0327\tAccuracy: 4144.0/4200 (99%)\n",
      "plain: Loss: 0.0337\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 73 [0/37800 (0%)]\tLosses bn: 0.000529 drop: 0.001391 plain: 0.003099\n",
      "Train Epoch: 73 [6400/37800 (1%)]\tLosses bn: 0.000681 drop: 0.002906 plain: 0.003831\n",
      "Train Epoch: 73 [12800/37800 (1%)]\tLosses bn: 0.001007 drop: 0.016828 plain: 0.015885\n",
      "Train Epoch: 73 [19200/37800 (2%)]\tLosses bn: 0.000864 drop: 0.013349 plain: 0.011607\n",
      "Train Epoch: 73 [25600/37800 (2%)]\tLosses bn: 0.002129 drop: 0.018861 plain: 0.023955\n",
      "Train Epoch: 73 [32000/37800 (3%)]\tLosses bn: 0.001462 drop: 0.006354 plain: 0.007128\n",
      "Train Epoch: 73 [37792/37800 (3%)]\tLosses bn: 0.001125 drop: 0.018694 plain: 0.008292\n",
      "Test set:\n",
      "bn: Loss: 0.0336\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0326\tAccuracy: 4144.0/4200 (99%)\n",
      "plain: Loss: 0.0337\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 74 [0/37800 (0%)]\tLosses bn: 0.000492 drop: 0.001372 plain: 0.003060\n",
      "Train Epoch: 74 [6400/37800 (1%)]\tLosses bn: 0.000631 drop: 0.002828 plain: 0.003732\n",
      "Train Epoch: 74 [12800/37800 (1%)]\tLosses bn: 0.000923 drop: 0.016259 plain: 0.015319\n",
      "Train Epoch: 74 [19200/37800 (2%)]\tLosses bn: 0.000798 drop: 0.013178 plain: 0.011384\n",
      "Train Epoch: 74 [25600/37800 (2%)]\tLosses bn: 0.001932 drop: 0.018274 plain: 0.023293\n",
      "Train Epoch: 74 [32000/37800 (3%)]\tLosses bn: 0.001331 drop: 0.006169 plain: 0.006932\n",
      "Train Epoch: 74 [37792/37800 (3%)]\tLosses bn: 0.000999 drop: 0.018265 plain: 0.008097\n",
      "Test set:\n",
      "bn: Loss: 0.0337\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0325\tAccuracy: 4146.0/4200 (99%)\n",
      "plain: Loss: 0.0336\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 75 [0/37800 (0%)]\tLosses bn: 0.000452 drop: 0.001352 plain: 0.003010\n",
      "Train Epoch: 75 [6400/37800 (1%)]\tLosses bn: 0.000573 drop: 0.002752 plain: 0.003621\n",
      "Train Epoch: 75 [12800/37800 (1%)]\tLosses bn: 0.000837 drop: 0.015712 plain: 0.014811\n",
      "Train Epoch: 75 [19200/37800 (2%)]\tLosses bn: 0.000711 drop: 0.012943 plain: 0.011148\n",
      "Train Epoch: 75 [25600/37800 (2%)]\tLosses bn: 0.001741 drop: 0.017637 plain: 0.022712\n",
      "Train Epoch: 75 [32000/37800 (3%)]\tLosses bn: 0.001204 drop: 0.005998 plain: 0.006729\n",
      "Train Epoch: 75 [37792/37800 (3%)]\tLosses bn: 0.000913 drop: 0.017831 plain: 0.007926\n",
      "Test set:\n",
      "bn: Loss: 0.0338\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0324\tAccuracy: 4146.0/4200 (99%)\n",
      "plain: Loss: 0.0335\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 76 [0/37800 (0%)]\tLosses bn: 0.000422 drop: 0.001330 plain: 0.002985\n",
      "Train Epoch: 76 [6400/37800 (1%)]\tLosses bn: 0.000523 drop: 0.002680 plain: 0.003526\n",
      "Train Epoch: 76 [12800/37800 (1%)]\tLosses bn: 0.000750 drop: 0.015200 plain: 0.014322\n",
      "Train Epoch: 76 [19200/37800 (2%)]\tLosses bn: 0.000641 drop: 0.012787 plain: 0.010893\n",
      "Train Epoch: 76 [25600/37800 (2%)]\tLosses bn: 0.001574 drop: 0.017074 plain: 0.022101\n",
      "Train Epoch: 76 [32000/37800 (3%)]\tLosses bn: 0.001075 drop: 0.005834 plain: 0.006535\n",
      "Train Epoch: 76 [37792/37800 (3%)]\tLosses bn: 0.000837 drop: 0.017365 plain: 0.007752\n",
      "Test set:\n",
      "bn: Loss: 0.0339\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0324\tAccuracy: 4146.0/4200 (99%)\n",
      "plain: Loss: 0.0335\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 77 [0/37800 (0%)]\tLosses bn: 0.000383 drop: 0.001307 plain: 0.002934\n",
      "Train Epoch: 77 [6400/37800 (1%)]\tLosses bn: 0.000466 drop: 0.002608 plain: 0.003422\n",
      "Train Epoch: 77 [12800/37800 (1%)]\tLosses bn: 0.000714 drop: 0.014732 plain: 0.013791\n",
      "Train Epoch: 77 [19200/37800 (2%)]\tLosses bn: 0.000583 drop: 0.012623 plain: 0.010639\n",
      "Train Epoch: 77 [25600/37800 (2%)]\tLosses bn: 0.001416 drop: 0.016495 plain: 0.021529\n",
      "Train Epoch: 77 [32000/37800 (3%)]\tLosses bn: 0.001000 drop: 0.005676 plain: 0.006338\n",
      "Train Epoch: 77 [37792/37800 (3%)]\tLosses bn: 0.000769 drop: 0.016922 plain: 0.007587\n",
      "Test set:\n",
      "bn: Loss: 0.0340\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0323\tAccuracy: 4146.0/4200 (99%)\n",
      "plain: Loss: 0.0334\tAccuracy: 4144.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 78 [0/37800 (0%)]\tLosses bn: 0.000354 drop: 0.001284 plain: 0.002901\n",
      "Train Epoch: 78 [6400/37800 (1%)]\tLosses bn: 0.000425 drop: 0.002540 plain: 0.003329\n",
      "Train Epoch: 78 [12800/37800 (1%)]\tLosses bn: 0.000651 drop: 0.014259 plain: 0.013348\n",
      "Train Epoch: 78 [19200/37800 (2%)]\tLosses bn: 0.000550 drop: 0.012431 plain: 0.010387\n",
      "Train Epoch: 78 [25600/37800 (2%)]\tLosses bn: 0.001302 drop: 0.015966 plain: 0.020942\n",
      "Train Epoch: 78 [32000/37800 (3%)]\tLosses bn: 0.000909 drop: 0.005524 plain: 0.006149\n",
      "Train Epoch: 78 [37792/37800 (3%)]\tLosses bn: 0.000690 drop: 0.016523 plain: 0.007427\n",
      "Test set:\n",
      "bn: Loss: 0.0341\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0322\tAccuracy: 4146.0/4200 (99%)\n",
      "plain: Loss: 0.0333\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 79 [0/37800 (0%)]\tLosses bn: 0.000332 drop: 0.001262 plain: 0.002859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 79 [6400/37800 (1%)]\tLosses bn: 0.000379 drop: 0.002471 plain: 0.003237\n",
      "Train Epoch: 79 [12800/37800 (1%)]\tLosses bn: 0.000597 drop: 0.013859 plain: 0.012896\n",
      "Train Epoch: 79 [19200/37800 (2%)]\tLosses bn: 0.000491 drop: 0.012244 plain: 0.010165\n",
      "Train Epoch: 79 [25600/37800 (2%)]\tLosses bn: 0.001172 drop: 0.015484 plain: 0.020367\n",
      "Train Epoch: 79 [32000/37800 (3%)]\tLosses bn: 0.000791 drop: 0.005362 plain: 0.005971\n",
      "Train Epoch: 79 [37792/37800 (3%)]\tLosses bn: 0.000638 drop: 0.016127 plain: 0.007258\n",
      "Test set:\n",
      "bn: Loss: 0.0342\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0322\tAccuracy: 4146.0/4200 (99%)\n",
      "plain: Loss: 0.0333\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 80 [0/37800 (0%)]\tLosses bn: 0.000307 drop: 0.001240 plain: 0.002821\n",
      "Train Epoch: 80 [6400/37800 (1%)]\tLosses bn: 0.000340 drop: 0.002409 plain: 0.003141\n",
      "Train Epoch: 80 [12800/37800 (1%)]\tLosses bn: 0.000529 drop: 0.013434 plain: 0.012479\n",
      "Train Epoch: 80 [19200/37800 (2%)]\tLosses bn: 0.000440 drop: 0.012031 plain: 0.009956\n",
      "Train Epoch: 80 [25600/37800 (2%)]\tLosses bn: 0.001063 drop: 0.015023 plain: 0.019797\n",
      "Train Epoch: 80 [32000/37800 (3%)]\tLosses bn: 0.000750 drop: 0.005219 plain: 0.005793\n",
      "Train Epoch: 80 [37792/37800 (3%)]\tLosses bn: 0.000583 drop: 0.015704 plain: 0.007112\n",
      "Test set:\n",
      "bn: Loss: 0.0344\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0321\tAccuracy: 4146.0/4200 (99%)\n",
      "plain: Loss: 0.0332\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 81 [0/37800 (0%)]\tLosses bn: 0.000287 drop: 0.001216 plain: 0.002773\n",
      "Train Epoch: 81 [6400/37800 (1%)]\tLosses bn: 0.000295 drop: 0.002344 plain: 0.003060\n",
      "Train Epoch: 81 [12800/37800 (1%)]\tLosses bn: 0.000486 drop: 0.013035 plain: 0.012057\n",
      "Train Epoch: 81 [19200/37800 (2%)]\tLosses bn: 0.000406 drop: 0.011820 plain: 0.009756\n",
      "Train Epoch: 81 [25600/37800 (2%)]\tLosses bn: 0.000963 drop: 0.014550 plain: 0.019301\n",
      "Train Epoch: 81 [32000/37800 (3%)]\tLosses bn: 0.000669 drop: 0.005092 plain: 0.005622\n",
      "Train Epoch: 81 [37792/37800 (3%)]\tLosses bn: 0.000531 drop: 0.015317 plain: 0.006956\n",
      "Test set:\n",
      "bn: Loss: 0.0346\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0321\tAccuracy: 4146.0/4200 (99%)\n",
      "plain: Loss: 0.0332\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 82 [0/37800 (0%)]\tLosses bn: 0.000269 drop: 0.001191 plain: 0.002724\n",
      "Train Epoch: 82 [6400/37800 (1%)]\tLosses bn: 0.000261 drop: 0.002284 plain: 0.002975\n",
      "Train Epoch: 82 [12800/37800 (1%)]\tLosses bn: 0.000449 drop: 0.012667 plain: 0.011656\n",
      "Train Epoch: 82 [19200/37800 (2%)]\tLosses bn: 0.000378 drop: 0.011613 plain: 0.009449\n",
      "Train Epoch: 82 [25600/37800 (2%)]\tLosses bn: 0.000872 drop: 0.014103 plain: 0.018752\n",
      "Train Epoch: 82 [32000/37800 (3%)]\tLosses bn: 0.000618 drop: 0.004948 plain: 0.005462\n",
      "Train Epoch: 82 [37792/37800 (3%)]\tLosses bn: 0.000483 drop: 0.014920 plain: 0.006832\n",
      "Test set:\n",
      "bn: Loss: 0.0347\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0320\tAccuracy: 4146.0/4200 (99%)\n",
      "plain: Loss: 0.0331\tAccuracy: 4144.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 83 [0/37800 (0%)]\tLosses bn: 0.000251 drop: 0.001168 plain: 0.002671\n",
      "Train Epoch: 83 [6400/37800 (1%)]\tLosses bn: 0.000233 drop: 0.002225 plain: 0.002887\n",
      "Train Epoch: 83 [12800/37800 (1%)]\tLosses bn: 0.000398 drop: 0.012319 plain: 0.011291\n",
      "Train Epoch: 83 [19200/37800 (2%)]\tLosses bn: 0.000346 drop: 0.011361 plain: 0.009231\n",
      "Train Epoch: 83 [25600/37800 (2%)]\tLosses bn: 0.000761 drop: 0.013698 plain: 0.018203\n",
      "Train Epoch: 83 [32000/37800 (3%)]\tLosses bn: 0.000563 drop: 0.004806 plain: 0.005303\n",
      "Train Epoch: 83 [37792/37800 (3%)]\tLosses bn: 0.000439 drop: 0.014498 plain: 0.006684\n",
      "Test set:\n",
      "bn: Loss: 0.0348\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0320\tAccuracy: 4147.0/4200 (99%)\n",
      "plain: Loss: 0.0331\tAccuracy: 4144.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 84 [0/37800 (0%)]\tLosses bn: 0.000233 drop: 0.001142 plain: 0.002631\n",
      "Train Epoch: 84 [6400/37800 (1%)]\tLosses bn: 0.000209 drop: 0.002163 plain: 0.002811\n",
      "Train Epoch: 84 [12800/37800 (1%)]\tLosses bn: 0.000359 drop: 0.011968 plain: 0.010951\n",
      "Train Epoch: 84 [19200/37800 (2%)]\tLosses bn: 0.000315 drop: 0.011051 plain: 0.008970\n",
      "Train Epoch: 84 [25600/37800 (2%)]\tLosses bn: 0.000680 drop: 0.013291 plain: 0.017680\n",
      "Train Epoch: 84 [32000/37800 (3%)]\tLosses bn: 0.000515 drop: 0.004672 plain: 0.005144\n",
      "Train Epoch: 84 [37792/37800 (3%)]\tLosses bn: 0.000396 drop: 0.014096 plain: 0.006538\n",
      "Test set:\n",
      "bn: Loss: 0.0350\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0319\tAccuracy: 4147.0/4200 (99%)\n",
      "plain: Loss: 0.0330\tAccuracy: 4144.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 85 [0/37800 (0%)]\tLosses bn: 0.000212 drop: 0.001116 plain: 0.002583\n",
      "Train Epoch: 85 [6400/37800 (1%)]\tLosses bn: 0.000189 drop: 0.002105 plain: 0.002727\n",
      "Train Epoch: 85 [12800/37800 (1%)]\tLosses bn: 0.000325 drop: 0.011611 plain: 0.010590\n",
      "Train Epoch: 85 [19200/37800 (2%)]\tLosses bn: 0.000280 drop: 0.010779 plain: 0.008762\n",
      "Train Epoch: 85 [25600/37800 (2%)]\tLosses bn: 0.000625 drop: 0.012903 plain: 0.017105\n",
      "Train Epoch: 85 [32000/37800 (3%)]\tLosses bn: 0.000456 drop: 0.004553 plain: 0.005001\n",
      "Train Epoch: 85 [37792/37800 (3%)]\tLosses bn: 0.000360 drop: 0.013737 plain: 0.006388\n",
      "Test set:\n",
      "bn: Loss: 0.0352\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0319\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0330\tAccuracy: 4144.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 86 [0/37800 (0%)]\tLosses bn: 0.000197 drop: 0.001095 plain: 0.002526\n",
      "Train Epoch: 86 [6400/37800 (1%)]\tLosses bn: 0.000169 drop: 0.002055 plain: 0.002645\n",
      "Train Epoch: 86 [12800/37800 (1%)]\tLosses bn: 0.000301 drop: 0.011291 plain: 0.010243\n",
      "Train Epoch: 86 [19200/37800 (2%)]\tLosses bn: 0.000255 drop: 0.010555 plain: 0.008459\n",
      "Train Epoch: 86 [25600/37800 (2%)]\tLosses bn: 0.000558 drop: 0.012565 plain: 0.016693\n",
      "Train Epoch: 86 [32000/37800 (3%)]\tLosses bn: 0.000414 drop: 0.004436 plain: 0.004857\n",
      "Train Epoch: 86 [37792/37800 (3%)]\tLosses bn: 0.000332 drop: 0.013358 plain: 0.006237\n",
      "Test set:\n",
      "bn: Loss: 0.0353\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0319\tAccuracy: 4147.0/4200 (99%)\n",
      "plain: Loss: 0.0330\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 87 [0/37800 (0%)]\tLosses bn: 0.000184 drop: 0.001072 plain: 0.002487\n",
      "Train Epoch: 87 [6400/37800 (1%)]\tLosses bn: 0.000150 drop: 0.001998 plain: 0.002575\n",
      "Train Epoch: 87 [12800/37800 (1%)]\tLosses bn: 0.000270 drop: 0.010948 plain: 0.009936\n",
      "Train Epoch: 87 [19200/37800 (2%)]\tLosses bn: 0.000233 drop: 0.010270 plain: 0.008262\n",
      "Train Epoch: 87 [25600/37800 (2%)]\tLosses bn: 0.000485 drop: 0.012173 plain: 0.016177\n",
      "Train Epoch: 87 [32000/37800 (3%)]\tLosses bn: 0.000373 drop: 0.004320 plain: 0.004722\n",
      "Train Epoch: 87 [37792/37800 (3%)]\tLosses bn: 0.000300 drop: 0.013034 plain: 0.006095\n",
      "Test set:\n",
      "bn: Loss: 0.0355\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0318\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0329\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 88 [0/37800 (0%)]\tLosses bn: 0.000168 drop: 0.001047 plain: 0.002438\n",
      "Train Epoch: 88 [6400/37800 (1%)]\tLosses bn: 0.000133 drop: 0.001948 plain: 0.002501\n",
      "Train Epoch: 88 [12800/37800 (1%)]\tLosses bn: 0.000231 drop: 0.010639 plain: 0.009637\n",
      "Train Epoch: 88 [19200/37800 (2%)]\tLosses bn: 0.000206 drop: 0.010015 plain: 0.007977\n",
      "Train Epoch: 88 [25600/37800 (2%)]\tLosses bn: 0.000433 drop: 0.011817 plain: 0.015685\n",
      "Train Epoch: 88 [32000/37800 (3%)]\tLosses bn: 0.000331 drop: 0.004199 plain: 0.004582\n",
      "Train Epoch: 88 [37792/37800 (3%)]\tLosses bn: 0.000270 drop: 0.012628 plain: 0.005957\n",
      "Test set:\n",
      "bn: Loss: 0.0357\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0318\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0329\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 89 [0/37800 (0%)]\tLosses bn: 0.000151 drop: 0.001025 plain: 0.002390\n",
      "Train Epoch: 89 [6400/37800 (1%)]\tLosses bn: 0.000118 drop: 0.001895 plain: 0.002425\n",
      "Train Epoch: 89 [12800/37800 (1%)]\tLosses bn: 0.000217 drop: 0.010349 plain: 0.009359\n",
      "Train Epoch: 89 [19200/37800 (2%)]\tLosses bn: 0.000187 drop: 0.009744 plain: 0.007747\n",
      "Train Epoch: 89 [25600/37800 (2%)]\tLosses bn: 0.000387 drop: 0.011479 plain: 0.015254\n",
      "Train Epoch: 89 [32000/37800 (3%)]\tLosses bn: 0.000304 drop: 0.004087 plain: 0.004455\n",
      "Train Epoch: 89 [37792/37800 (3%)]\tLosses bn: 0.000247 drop: 0.012306 plain: 0.005827\n",
      "Test set:\n",
      "bn: Loss: 0.0359\tAccuracy: 4144.0/4200 (99%)\n",
      "drop: Loss: 0.0318\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0329\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 90 [0/37800 (0%)]\tLosses bn: 0.000141 drop: 0.001001 plain: 0.002340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 90 [6400/37800 (1%)]\tLosses bn: 0.000107 drop: 0.001845 plain: 0.002357\n",
      "Train Epoch: 90 [12800/37800 (1%)]\tLosses bn: 0.000201 drop: 0.010047 plain: 0.009069\n",
      "Train Epoch: 90 [19200/37800 (2%)]\tLosses bn: 0.000170 drop: 0.009484 plain: 0.007522\n",
      "Train Epoch: 90 [25600/37800 (2%)]\tLosses bn: 0.000350 drop: 0.011187 plain: 0.014804\n",
      "Train Epoch: 90 [32000/37800 (3%)]\tLosses bn: 0.000274 drop: 0.003978 plain: 0.004322\n",
      "Train Epoch: 90 [37792/37800 (3%)]\tLosses bn: 0.000229 drop: 0.011948 plain: 0.005711\n",
      "Test set:\n",
      "bn: Loss: 0.0360\tAccuracy: 4144.0/4200 (99%)\n",
      "drop: Loss: 0.0317\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0329\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 91 [0/37800 (0%)]\tLosses bn: 0.000128 drop: 0.000978 plain: 0.002293\n",
      "Train Epoch: 91 [6400/37800 (1%)]\tLosses bn: 0.000092 drop: 0.001793 plain: 0.002287\n",
      "Train Epoch: 91 [12800/37800 (1%)]\tLosses bn: 0.000183 drop: 0.009788 plain: 0.008831\n",
      "Train Epoch: 91 [19200/37800 (2%)]\tLosses bn: 0.000148 drop: 0.009156 plain: 0.007364\n",
      "Train Epoch: 91 [25600/37800 (2%)]\tLosses bn: 0.000325 drop: 0.010897 plain: 0.014390\n",
      "Train Epoch: 91 [32000/37800 (3%)]\tLosses bn: 0.000242 drop: 0.003871 plain: 0.004182\n",
      "Train Epoch: 91 [37792/37800 (3%)]\tLosses bn: 0.000213 drop: 0.011654 plain: 0.005562\n",
      "Test set:\n",
      "bn: Loss: 0.0362\tAccuracy: 4144.0/4200 (99%)\n",
      "drop: Loss: 0.0317\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0328\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 92 [0/37800 (0%)]\tLosses bn: 0.000118 drop: 0.000954 plain: 0.002241\n",
      "Train Epoch: 92 [6400/37800 (1%)]\tLosses bn: 0.000082 drop: 0.001741 plain: 0.002217\n",
      "Train Epoch: 92 [12800/37800 (1%)]\tLosses bn: 0.000165 drop: 0.009504 plain: 0.008580\n",
      "Train Epoch: 92 [19200/37800 (2%)]\tLosses bn: 0.000138 drop: 0.008947 plain: 0.007097\n",
      "Train Epoch: 92 [25600/37800 (2%)]\tLosses bn: 0.000285 drop: 0.010601 plain: 0.014007\n",
      "Train Epoch: 92 [32000/37800 (3%)]\tLosses bn: 0.000221 drop: 0.003761 plain: 0.004057\n",
      "Train Epoch: 92 [37792/37800 (3%)]\tLosses bn: 0.000196 drop: 0.011271 plain: 0.005440\n",
      "Test set:\n",
      "bn: Loss: 0.0364\tAccuracy: 4144.0/4200 (99%)\n",
      "drop: Loss: 0.0317\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0328\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 93 [0/37800 (0%)]\tLosses bn: 0.000108 drop: 0.000933 plain: 0.002191\n",
      "Train Epoch: 93 [6400/37800 (1%)]\tLosses bn: 0.000073 drop: 0.001696 plain: 0.002151\n",
      "Train Epoch: 93 [12800/37800 (1%)]\tLosses bn: 0.000151 drop: 0.009262 plain: 0.008327\n",
      "Train Epoch: 93 [19200/37800 (2%)]\tLosses bn: 0.000123 drop: 0.008650 plain: 0.006934\n",
      "Train Epoch: 93 [25600/37800 (2%)]\tLosses bn: 0.000265 drop: 0.010338 plain: 0.013600\n",
      "Train Epoch: 93 [32000/37800 (3%)]\tLosses bn: 0.000191 drop: 0.003664 plain: 0.003937\n",
      "Train Epoch: 93 [37792/37800 (3%)]\tLosses bn: 0.000172 drop: 0.010934 plain: 0.005311\n",
      "Test set:\n",
      "bn: Loss: 0.0367\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0317\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0328\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 94 [0/37800 (0%)]\tLosses bn: 0.000097 drop: 0.000906 plain: 0.002151\n",
      "Train Epoch: 94 [6400/37800 (1%)]\tLosses bn: 0.000066 drop: 0.001642 plain: 0.002089\n",
      "Train Epoch: 94 [12800/37800 (1%)]\tLosses bn: 0.000138 drop: 0.008991 plain: 0.008094\n",
      "Train Epoch: 94 [19200/37800 (2%)]\tLosses bn: 0.000108 drop: 0.008400 plain: 0.006694\n",
      "Train Epoch: 94 [25600/37800 (2%)]\tLosses bn: 0.000225 drop: 0.010050 plain: 0.013235\n",
      "Train Epoch: 94 [32000/37800 (3%)]\tLosses bn: 0.000177 drop: 0.003562 plain: 0.003828\n",
      "Train Epoch: 94 [37792/37800 (3%)]\tLosses bn: 0.000160 drop: 0.010615 plain: 0.005177\n",
      "Test set:\n",
      "bn: Loss: 0.0369\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0317\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0328\tAccuracy: 4146.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 95 [0/37800 (0%)]\tLosses bn: 0.000091 drop: 0.000883 plain: 0.002096\n",
      "Train Epoch: 95 [6400/37800 (1%)]\tLosses bn: 0.000059 drop: 0.001596 plain: 0.002024\n",
      "Train Epoch: 95 [12800/37800 (1%)]\tLosses bn: 0.000125 drop: 0.008763 plain: 0.007840\n",
      "Train Epoch: 95 [19200/37800 (2%)]\tLosses bn: 0.000098 drop: 0.008132 plain: 0.006506\n",
      "Train Epoch: 95 [25600/37800 (2%)]\tLosses bn: 0.000210 drop: 0.009822 plain: 0.012908\n",
      "Train Epoch: 95 [32000/37800 (3%)]\tLosses bn: 0.000155 drop: 0.003472 plain: 0.003707\n",
      "Train Epoch: 95 [37792/37800 (3%)]\tLosses bn: 0.000145 drop: 0.010288 plain: 0.005056\n",
      "Test set:\n",
      "bn: Loss: 0.0370\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0317\tAccuracy: 4147.0/4200 (99%)\n",
      "plain: Loss: 0.0327\tAccuracy: 4146.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 96 [0/37800 (0%)]\tLosses bn: 0.000082 drop: 0.000858 plain: 0.002055\n",
      "Train Epoch: 96 [6400/37800 (1%)]\tLosses bn: 0.000050 drop: 0.001550 plain: 0.001963\n",
      "Train Epoch: 96 [12800/37800 (1%)]\tLosses bn: 0.000113 drop: 0.008534 plain: 0.007617\n",
      "Train Epoch: 96 [19200/37800 (2%)]\tLosses bn: 0.000088 drop: 0.007927 plain: 0.006318\n",
      "Train Epoch: 96 [25600/37800 (2%)]\tLosses bn: 0.000180 drop: 0.009521 plain: 0.012507\n",
      "Train Epoch: 96 [32000/37800 (3%)]\tLosses bn: 0.000144 drop: 0.003377 plain: 0.003603\n",
      "Train Epoch: 96 [37792/37800 (3%)]\tLosses bn: 0.000130 drop: 0.009970 plain: 0.004932\n",
      "Test set:\n",
      "bn: Loss: 0.0373\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0316\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0327\tAccuracy: 4146.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 97 [0/37800 (0%)]\tLosses bn: 0.000071 drop: 0.000836 plain: 0.002005\n",
      "Train Epoch: 97 [6400/37800 (1%)]\tLosses bn: 0.000045 drop: 0.001503 plain: 0.001893\n",
      "Train Epoch: 97 [12800/37800 (1%)]\tLosses bn: 0.000101 drop: 0.008309 plain: 0.007400\n",
      "Train Epoch: 97 [19200/37800 (2%)]\tLosses bn: 0.000079 drop: 0.007665 plain: 0.006120\n",
      "Train Epoch: 97 [25600/37800 (2%)]\tLosses bn: 0.000168 drop: 0.009320 plain: 0.012143\n",
      "Train Epoch: 97 [32000/37800 (3%)]\tLosses bn: 0.000130 drop: 0.003287 plain: 0.003495\n",
      "Train Epoch: 97 [37792/37800 (3%)]\tLosses bn: 0.000118 drop: 0.009658 plain: 0.004796\n",
      "Test set:\n",
      "bn: Loss: 0.0376\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0316\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0327\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 98 [0/37800 (0%)]\tLosses bn: 0.000066 drop: 0.000815 plain: 0.001966\n",
      "Train Epoch: 98 [6400/37800 (1%)]\tLosses bn: 0.000040 drop: 0.001458 plain: 0.001836\n",
      "Train Epoch: 98 [12800/37800 (1%)]\tLosses bn: 0.000093 drop: 0.008078 plain: 0.007171\n",
      "Train Epoch: 98 [19200/37800 (2%)]\tLosses bn: 0.000069 drop: 0.007417 plain: 0.005950\n",
      "Train Epoch: 98 [25600/37800 (2%)]\tLosses bn: 0.000150 drop: 0.009082 plain: 0.011805\n",
      "Train Epoch: 98 [32000/37800 (3%)]\tLosses bn: 0.000114 drop: 0.003196 plain: 0.003391\n",
      "Train Epoch: 98 [37792/37800 (3%)]\tLosses bn: 0.000105 drop: 0.009392 plain: 0.004693\n",
      "Test set:\n",
      "bn: Loss: 0.0378\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0316\tAccuracy: 4147.0/4200 (99%)\n",
      "plain: Loss: 0.0327\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 99 [0/37800 (0%)]\tLosses bn: 0.000060 drop: 0.000791 plain: 0.001921\n",
      "Train Epoch: 99 [6400/37800 (1%)]\tLosses bn: 0.000035 drop: 0.001414 plain: 0.001776\n",
      "Train Epoch: 99 [12800/37800 (1%)]\tLosses bn: 0.000084 drop: 0.007875 plain: 0.006965\n",
      "Train Epoch: 99 [19200/37800 (2%)]\tLosses bn: 0.000062 drop: 0.007221 plain: 0.005741\n",
      "Train Epoch: 99 [25600/37800 (2%)]\tLosses bn: 0.000134 drop: 0.008807 plain: 0.011515\n",
      "Train Epoch: 99 [32000/37800 (3%)]\tLosses bn: 0.000106 drop: 0.003110 plain: 0.003284\n",
      "Train Epoch: 99 [37792/37800 (3%)]\tLosses bn: 0.000090 drop: 0.009075 plain: 0.004566\n",
      "Test set:\n",
      "bn: Loss: 0.0380\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0316\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0327\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "Train Epoch: 100 [0/37800 (0%)]\tLosses bn: 0.000056 drop: 0.000770 plain: 0.001875\n",
      "Train Epoch: 100 [6400/37800 (1%)]\tLosses bn: 0.000032 drop: 0.001372 plain: 0.001721\n",
      "Train Epoch: 100 [12800/37800 (1%)]\tLosses bn: 0.000079 drop: 0.007649 plain: 0.006736\n",
      "Train Epoch: 100 [19200/37800 (2%)]\tLosses bn: 0.000057 drop: 0.006952 plain: 0.005582\n",
      "Train Epoch: 100 [25600/37800 (2%)]\tLosses bn: 0.000119 drop: 0.008597 plain: 0.011131\n",
      "Train Epoch: 100 [32000/37800 (3%)]\tLosses bn: 0.000094 drop: 0.003029 plain: 0.003176\n",
      "Train Epoch: 100 [37792/37800 (3%)]\tLosses bn: 0.000082 drop: 0.008821 plain: 0.004445\n",
      "Test set:\n",
      "bn: Loss: 0.0381\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0316\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0327\tAccuracy: 4145.0/4200 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 101):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    train(epoch, models, train_log)\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    test(models, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16de13198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(test_log, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16de13cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(test_log, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('./test.csv', delimiter=',')\n",
    "data = np.delete(data, (0), axis=0)\n",
    "data_np_x = data / 255\n",
    "\n",
    "testing_x = torch.Tensor(np.expand_dims(normalize(data_np_x), axis=1))\n",
    "testing_y = torch.LongTensor(np.zeros((testing_x.shape[0], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28000, 1, 784])\n"
     ]
    }
   ],
   "source": [
    "print (testing_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (_conv1): ConvLayer(\n",
      "    (model): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (_conv2): ConvLayer(\n",
      "    (model): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (fc): FullyConnected(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=3136, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "(28000, 10)\n"
     ]
    }
   ],
   "source": [
    "answers = np.empty((0, 10))\n",
    "model = models['drop']\n",
    "print (model)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(loader(testing_x, testing_y)):\n",
    "    output = model(data)  \n",
    "    output_np = output.detach().numpy()        \n",
    "    answers = np.vstack((answers, output_np))\n",
    "        \n",
    "print (answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.argmax(answers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.16269875e+00 -1.60121822e+01  1.06884661e+01 -6.47136688e+00\n",
      "  -1.17631922e+01 -1.21434679e+01 -1.63155842e+01 -8.65103340e+00\n",
      "  -7.29650211e+00 -7.87598944e+00]\n",
      " [ 7.96404457e+00 -1.54184065e+01 -4.63093805e+00 -7.94373655e+00\n",
      "  -1.14978619e+01 -6.89394712e+00 -5.63781691e+00 -7.37076187e+00\n",
      "  -1.10100842e+01 -7.42037439e+00]\n",
      " [-2.00090389e+01 -1.17662468e+01 -3.89059043e+00 -6.97501516e+00\n",
      "  -1.54494703e+00 -9.46545982e+00 -1.91596947e+01 -9.47061825e+00\n",
      "  -2.20920134e+00  8.34610271e+00]\n",
      " [ 3.83594656e+00 -1.72275906e+01 -1.30564487e+00 -7.40027237e+00\n",
      "  -5.06602144e+00 -1.12251835e+01 -7.38695240e+00 -4.07968378e+00\n",
      "  -5.74912405e+00  1.89173996e+00]\n",
      " [-1.30997896e+01 -1.16446657e+01 -9.35777426e-01  6.50771093e+00\n",
      "  -1.47982273e+01 -6.08990860e+00 -1.18943367e+01 -1.00861454e+01\n",
      "  -3.96348596e+00 -6.14569759e+00]\n",
      " [-1.38568010e+01 -1.71890221e+01 -7.97658348e+00 -5.32540798e+00\n",
      "  -8.81492329e+00 -5.05572462e+00 -2.24025593e+01  3.83194304e+00\n",
      "  -7.94081116e+00 -3.24330986e-01]\n",
      " [ 1.47189388e+01 -1.46512756e+01 -1.13718402e+00 -9.47568417e+00\n",
      "  -1.08322392e+01 -1.33037167e+01  5.88893741e-02 -1.25346298e+01\n",
      "  -4.42448092e+00 -4.99127245e+00]\n",
      " [-1.38625336e+01 -1.21722603e+01 -5.65320206e+00  7.06730270e+00\n",
      "  -1.34995661e+01 -4.25352955e+00 -1.87994652e+01 -6.86221695e+00\n",
      "  -7.81651258e+00 -3.18401909e+00]\n",
      " [ 9.24506569e+00 -1.64358959e+01 -5.04839325e+00 -7.59463930e+00\n",
      "  -1.56671238e+01 -1.19187346e+01 -5.86005020e+00 -9.11125565e+00\n",
      "  -7.50330734e+00 -3.48468971e+00]\n",
      " [-2.06918373e+01 -1.61110077e+01 -1.00344915e+01  9.34218025e+00\n",
      "  -1.52139616e+01  1.20130014e+00 -1.66797466e+01 -9.22354126e+00\n",
      "  -5.86584663e+00 -4.35364819e+00]\n",
      " [-1.75132408e+01 -1.64069176e+01 -1.63600483e+01 -1.65863574e+00\n",
      "  -1.54530468e+01  1.38036947e+01 -8.98427963e+00 -1.11463709e+01\n",
      "  -3.10489964e+00  1.03085744e+00]\n",
      " [-8.79601479e+00 -9.76277733e+00 -7.51962185e-01 -4.05676174e+00\n",
      "  -1.19512138e+01 -1.34347639e+01 -2.44614563e+01  6.31511211e+00\n",
      "  -8.61325932e+00 -7.80752659e-01]\n",
      " [-1.72149582e+01 -1.13984375e+01 -1.31148796e+01 -6.77180290e+00\n",
      "   6.78067303e+00 -1.67145705e+00 -1.53380098e+01 -3.68253994e+00\n",
      "  -3.95762134e+00 -1.69313502e+00]\n",
      " [ 1.38221607e+01 -1.51134853e+01 -6.42460060e+00 -1.08789091e+01\n",
      "  -1.89396172e+01 -7.24973822e+00 -1.94066525e+00 -1.03504095e+01\n",
      "  -5.70705223e+00 -3.28794169e+00]\n",
      " [-1.39738913e+01 -1.23563833e+01 -1.33446341e+01 -9.78455257e+00\n",
      "   8.72121525e+00 -1.22909803e+01 -9.77924919e+00 -3.15148234e+00\n",
      "  -5.36598444e+00 -1.52559674e+00]\n",
      " [-8.99524117e+00 -1.44880657e+01 -8.14331532e+00  4.67074299e+00\n",
      "  -1.37834263e+01  1.08164454e+00 -1.50214577e+01 -1.28702402e+01\n",
      "  -3.64948416e+00 -6.01359463e+00]\n",
      " [-1.35756502e+01 -1.17859545e+01 -5.51177502e+00  7.36343908e+00\n",
      "  -1.63446007e+01 -1.41144037e+00 -1.22751360e+01 -9.65316105e+00\n",
      "  -1.08163090e+01 -6.93278646e+00]\n",
      " [-4.46682739e+00  1.11396284e+01 -4.83407164e+00 -8.46308422e+00\n",
      "  -7.02333212e-01 -4.25261259e+00 -4.39971638e+00 -3.12063336e+00\n",
      "  -2.39140725e+00 -8.79233932e+00]\n",
      " [-1.83611851e+01 -1.12134953e+01 -8.76964092e+00 -2.85981941e+00\n",
      "   1.72499299e+00 -6.60749388e+00 -2.09318600e+01 -3.48488474e+00\n",
      "   3.86041969e-01  9.74923134e+00]\n",
      " [ 1.46256599e+01 -1.57791615e+01 -2.78018594e+00 -1.64640293e+01\n",
      "  -1.38939095e+01 -6.45587492e+00  2.28773952e-02 -6.39506435e+00\n",
      "  -2.63760829e+00 -4.10058641e+00]\n",
      " [-1.36807642e+01 -1.69460640e+01 -1.15949001e+01 -3.66903830e+00\n",
      "  -2.72624469e+00 -8.99614906e+00 -1.99659233e+01 -4.21614587e-01\n",
      "  -1.07531524e+00  8.24060154e+00]\n",
      " [-4.67372274e+00  1.01772823e+01 -5.15756798e+00 -8.67490864e+00\n",
      "  -1.06684935e+00 -7.93191099e+00 -4.01582193e+00 -2.80556750e+00\n",
      "  -1.71980309e+00 -6.94333935e+00]\n",
      " [-8.12378311e+00  4.59175873e+00 -5.51041722e-01 -1.18849754e+01\n",
      "  -2.31279516e+00 -1.53642807e+01 -1.04530268e+01  3.45687002e-01\n",
      "  -3.12076449e+00 -8.37366295e+00]\n",
      " [-5.44896936e+00 -9.43832016e+00 -1.72218018e+01 -9.39654732e+00\n",
      "  -1.05991173e+01  1.14300146e+01 -9.97334385e+00 -7.33537579e+00\n",
      "  -5.36614418e+00 -4.73300552e+00]\n",
      " [-1.07134056e+01 -7.94060993e+00 -3.42225718e+00 -4.58897591e+00\n",
      "  -4.49724913e+00 -1.03658743e+01 -2.46021423e+01  1.04945221e+01\n",
      "  -7.18821287e+00 -1.93659794e+00]\n",
      " [-1.68347626e+01 -3.77202129e+00 -1.04382572e+01 -8.49251366e+00\n",
      "   1.21107588e+01 -9.37905312e+00 -1.31954689e+01 -3.40645146e+00\n",
      "  -4.84983110e+00  1.73861468e+00]\n",
      " [-1.94623699e+01 -5.88117933e+00  7.70068073e+00 -7.75720882e+00\n",
      "  -7.52497292e+00 -1.50395679e+01 -1.54836349e+01 -1.05803719e+01\n",
      "  -4.28264236e+00 -1.24098930e+01]\n",
      " [-8.30617809e+00 -1.34688072e+01 -1.13082228e+01 -8.29342175e+00\n",
      "  -3.81065249e+00 -1.02829685e+01 -2.72945080e+01  1.08285351e+01\n",
      "  -8.96093178e+00  2.08447838e+00]\n",
      " [-1.67016697e+01 -1.48746634e+00 -8.91101074e+00 -9.13868713e+00\n",
      "   3.80007124e+00 -1.18617945e+01 -1.83947468e+01 -1.37017226e+00\n",
      "  -1.69040406e+00  1.96779954e+00]\n",
      " [-7.19139624e+00 -7.56493282e+00 -1.19544172e+00 -5.07497644e+00\n",
      "  -6.90924692e+00 -1.12983456e+01 -2.25302429e+01  8.00815964e+00\n",
      "  -8.90962601e+00 -1.38136387e+00]\n",
      " [-1.20221577e+01 -1.16809425e+01 -4.10735607e+00 -4.46742535e+00\n",
      "  -9.85992050e+00 -1.60811710e+01 -2.34800282e+01  3.98414922e+00\n",
      "  -8.82573128e+00 -3.72155571e+00]\n",
      " [-1.31877651e+01 -1.98008671e+01 -1.27483397e+01 -2.30145621e+00\n",
      "  -7.73750734e+00  8.28397465e+00 -1.11569071e+01 -1.07244949e+01\n",
      "  -7.12630463e+00 -1.84731376e+00]\n",
      " [-1.77356415e+01 -8.59454727e+00 -1.09734793e+01 -1.44999342e+01\n",
      "   1.62264271e+01 -1.24212828e+01 -1.48835659e+01 -1.39485300e+00\n",
      "  -4.81808519e+00 -3.07376337e+00]\n",
      " [-8.48829556e+00 -5.14566183e+00  8.83678150e+00 -1.92873609e+00\n",
      "  -1.77066383e+01 -1.43205767e+01 -1.08592739e+01 -6.98958015e+00\n",
      "  -3.80488944e+00 -1.06849928e+01]\n",
      " [-3.06065941e+00 -1.01039286e+01 -4.38065672e+00 -1.01934624e+01\n",
      "  -2.44004536e+00 -5.71282864e+00  1.55111055e+01 -1.68685150e+01\n",
      "  -4.55260229e+00 -1.21383152e+01]\n",
      " [-1.24272585e+01  1.28350854e+00  8.00628185e+00 -4.33341312e+00\n",
      "  -1.07686462e+01 -1.53328276e+01 -1.83278599e+01 -1.89787149e-02\n",
      "  -5.13454580e+00 -1.08610067e+01]\n",
      " [-1.26607494e+01 -1.69991894e+01 -9.65196705e+00 -5.44427443e+00\n",
      "  -1.25395002e+01  7.43216276e+00 -6.85152006e+00 -5.90792942e+00\n",
      "  -3.33255529e+00 -3.24156952e+00]\n",
      " [-9.16812897e+00 -1.51015940e+01 -1.32185078e+01 -4.97871685e+00\n",
      "  -7.73132944e+00  9.44209194e+00 -7.26696396e+00 -1.20586071e+01\n",
      "  -3.71361041e+00 -8.84894371e+00]\n",
      " [-1.31311779e+01  6.49591780e+00 -5.02842379e+00 -4.42003489e+00\n",
      "  -2.31818438e-01 -3.60601926e+00 -9.27366352e+00 -6.28637171e+00\n",
      "  -5.96291780e-01 -3.77777910e+00]\n",
      " [-8.44260097e-01 -1.34655857e+01 -1.14919720e+01 -1.18070383e+01\n",
      "  -9.80139160e+00  3.34080887e+00  1.07860613e+01 -1.76500874e+01\n",
      "  -2.68187761e+00 -6.73166561e+00]\n",
      " [-6.50280762e+00 -9.51000595e+00 -1.86601925e+00 -6.94703102e+00\n",
      "  -8.80225182e+00 -1.01452112e+01 -2.17154083e+01  7.13831377e+00\n",
      "  -8.35336685e+00  6.14817202e-01]\n",
      " [-1.09014206e+01 -8.81375313e+00 -3.50768757e+00 -6.15820944e-01\n",
      "  -1.00292358e+01 -1.07475557e+01 -2.84553223e+01  1.09576340e+01\n",
      "  -1.02102966e+01 -2.85995531e+00]\n",
      " [-9.76395512e+00 -9.53192139e+00 -1.00286598e+01 -1.30796928e+01\n",
      "   1.20423555e+01 -1.22802372e+01 -8.60454845e+00 -4.74820375e+00\n",
      "  -5.89137888e+00 -5.27238989e+00]\n",
      " [-1.00053730e+01 -1.73087311e+01 -8.29509926e+00 -2.40721655e+00\n",
      "  -2.19523859e+00 -1.03722973e+01 -1.54994774e+01 -3.30086184e+00\n",
      "  -6.70185328e+00  9.57600594e+00]\n",
      " [-1.40070677e+01 -1.36280909e+01 -4.65852261e+00 -4.80845165e+00\n",
      "  -1.08894234e+01 -2.57992554e+00 -3.70007157e+00 -9.89249611e+00\n",
      "   5.15089655e+00 -3.99048018e+00]\n",
      " [-9.94795895e+00 -8.51136494e+00 -3.76363945e+00 -4.42250341e-01\n",
      "  -8.37395763e+00 -7.09219170e+00 -2.29147282e+01  1.05794401e+01\n",
      "  -1.03398733e+01 -4.89048910e+00]\n",
      " [-1.23908262e+01 -1.29856215e+01 -4.70727491e+00 -1.43712676e+00\n",
      "  -6.44912624e+00 -7.90182400e+00 -1.26922178e+01 -1.02751207e+01\n",
      "   1.33390551e+01 -1.69508219e+00]\n",
      " [-9.46702862e+00 -1.25034046e+01  6.65356064e+00  1.21086013e+00\n",
      "  -2.24099598e+01 -1.00164881e+01 -1.08440514e+01 -9.79460907e+00\n",
      "   2.35951591e+00 -7.37298250e+00]\n",
      " [-1.17092431e+00 -2.28407860e+00 -7.77180099e+00 -6.56944466e+00\n",
      "  -2.40255809e+00 -2.89426637e+00  8.45789433e+00 -1.31143208e+01\n",
      "   2.01054871e-01 -8.52334881e+00]\n",
      " [-1.52691860e+01 -7.72390652e+00 -8.47382641e+00 -4.03469563e+00\n",
      "  -4.27150583e+00 -8.33365536e+00 -2.05921898e+01  4.56437826e+00\n",
      "  -7.35894156e+00 -3.92351198e+00]\n",
      " [-1.54290330e+00 -8.84555149e+00 -7.40807247e+00 -8.39189816e+00\n",
      "  -2.10334882e-01 -6.50722885e+00  1.59943733e+01 -1.64904079e+01\n",
      "  -5.52801895e+00 -6.22402191e+00]\n",
      " [-8.64669895e+00 -2.20376244e+01 -4.45827532e+00 -2.85232687e+00\n",
      "  -1.40632296e+01 -7.91901636e+00 -6.29192877e+00 -1.45575991e+01\n",
      "   1.02138281e+01 -1.06706762e+00]\n",
      " [-1.03885689e+01 -3.94492507e+00 -5.09411383e+00 -6.22956324e+00\n",
      "  -5.39257956e+00 -9.77950764e+00 -8.11663532e+00 -8.84514046e+00\n",
      "   9.46833515e+00 -4.78590298e+00]\n",
      " [-1.31647549e+01 -1.49799547e+01 -7.90954733e+00  1.09025650e+01\n",
      "  -2.14077396e+01 -1.47612846e+00 -1.83096790e+01 -1.06665955e+01\n",
      "  -3.57201529e+00 -3.78677440e+00]\n",
      " [-1.05877066e+01 -1.79707642e+01 -5.77802038e+00 -2.37080359e+00\n",
      "  -1.03303318e+01 -6.46990633e+00 -9.48222160e+00 -9.46549988e+00\n",
      "   8.94326496e+00 -2.64461923e+00]\n",
      " [-1.59385653e+01 -5.47279215e+00  1.16817532e+01 -9.31864357e+00\n",
      "  -3.20314884e+00 -1.55931997e+01 -9.24672318e+00 -1.06855698e+01\n",
      "  -1.05891495e+01 -1.38382530e+01]\n",
      " [-7.09654188e+00  9.47659397e+00 -4.18431187e+00 -8.02222252e+00\n",
      "  -2.01371288e+00 -8.35336685e+00 -8.68998241e+00 -3.77586460e+00\n",
      "  -5.40102816e+00 -6.00882149e+00]\n",
      " [-6.69735003e+00 -1.58812609e+01  5.43381357e+00 -3.20854974e+00\n",
      "  -1.06753540e+01 -1.35239315e+01 -1.46393719e+01 -6.28005028e+00\n",
      "  -1.39006329e+00 -5.41141939e+00]\n",
      " [-9.91707706e+00 -1.13110924e+01  7.80492067e+00 -5.99030542e+00\n",
      "  -5.48745871e+00 -1.00453596e+01 -1.41224813e+01 -2.50915551e+00\n",
      "  -8.71061707e+00 -7.80272293e+00]\n",
      " [ 9.55421686e-01 -2.31303539e+01 -1.11134672e+01 -3.99578333e+00\n",
      "  -1.71339779e+01 -3.14917421e+00 -4.58711290e+00 -1.50684233e+01\n",
      "  -4.20409155e+00 -1.60781193e+00]\n",
      " [-1.45994873e+01 -1.05648766e+01 -9.42932892e+00 -9.21445656e+00\n",
      "   9.12096977e+00 -9.85254860e+00 -1.33890285e+01 -2.46928000e+00\n",
      "  -3.01858783e+00  3.69218588e-01]\n",
      " [-8.09129047e+00  9.34189606e+00 -2.70490599e+00 -8.59512615e+00\n",
      "  -4.45816010e-01 -1.10735607e+01 -1.19630814e+01 -3.10119224e+00\n",
      "  -5.87029886e+00 -7.22414160e+00]\n",
      " [-1.20236883e+01 -9.99244404e+00 -7.33442450e+00 -7.31890821e+00\n",
      "  -4.52043724e+00 -1.43678455e+01 -2.31674576e+01  9.19517231e+00\n",
      "  -1.06907692e+01 -2.00117564e+00]\n",
      " [ 1.26810856e+01 -1.41911888e+01 -4.61688566e+00 -1.07343740e+01\n",
      "  -1.03334980e+01 -3.86227822e+00  7.51465201e-01 -8.62263107e+00\n",
      "  -3.39509869e+00 -1.88437414e+00]\n",
      " [ 7.55251646e+00 -1.82073250e+01 -2.09185553e+00 -1.22584934e+01\n",
      "  -9.01562214e+00 -1.76453247e+01 -4.26791334e+00 -9.28930283e+00\n",
      "  -7.67596340e+00 -3.65517855e+00]\n",
      " [ 1.10980034e+01 -1.40430193e+01  1.39805639e+00 -1.17617922e+01\n",
      "  -1.02660999e+01 -1.38337269e+01  1.11134160e+00 -1.26460495e+01\n",
      "  -5.03160954e+00 -6.86859274e+00]\n",
      " [-7.13535929e+00  8.30250454e+00 -3.59526014e+00 -6.72565508e+00\n",
      "  -1.89834428e+00 -7.61926842e+00 -1.32393141e+01  1.63854659e+00\n",
      "  -3.86797953e+00 -2.73362947e+00]\n",
      " [-1.26122923e+01 -1.33111935e+01 -9.85097790e+00 -5.25278807e+00\n",
      "  -1.21939456e+00 -1.28103485e+01 -1.43717966e+01 -4.06966162e+00\n",
      "  -2.58614326e+00  5.45091057e+00]\n",
      " [ 1.18841686e+01 -1.45516472e+01 -3.27601123e+00 -1.51156950e+01\n",
      "  -1.32452068e+01 -4.19414854e+00 -2.08725259e-01 -8.32500458e+00\n",
      "  -5.20891094e+00 -5.49985886e+00]\n",
      " [-5.42480803e+00  9.38800335e+00 -5.71100140e+00 -7.67702103e+00\n",
      "  -2.09631801e+00 -6.52296877e+00 -6.08051920e+00 -6.96591091e+00\n",
      "  -4.09031725e+00 -6.66525602e+00]\n",
      " [-2.81422329e+00 -1.24916029e+01 -1.05181522e+01 -9.88765049e+00\n",
      "  -9.81533241e+00 -1.90170705e-01  1.39291344e+01 -1.49755554e+01\n",
      "  -1.57656801e+00 -7.53212833e+00]\n",
      " [-1.74496460e+01 -1.43228016e+01 -1.09707336e+01 -2.00459766e+00\n",
      "  -1.26473618e+01  9.91325378e+00 -1.03581486e+01 -1.60474472e+01\n",
      "  -2.93752480e+00 -8.66266072e-01]\n",
      " [-1.26775179e+01 -1.84200840e+01 -4.10447788e+00 -7.61033595e-01\n",
      "  -1.59033308e+01 -6.47326946e+00 -1.65863934e+01 -1.32997856e+01\n",
      "   9.36075687e+00 -5.71132183e+00]\n",
      " [-8.94408131e+00 -1.53841181e+01 -1.64566982e+00 -4.28402567e+00\n",
      "  -1.12327824e+01 -9.00377655e+00 -4.87820005e+00 -1.25000095e+01\n",
      "   1.30378551e+01 -3.50799370e+00]\n",
      " [-1.65550137e+01 -6.13937616e+00  7.21861649e+00 -3.51930118e+00\n",
      "  -8.43534660e+00 -1.52441702e+01 -1.89638443e+01 -6.31834209e-01\n",
      "  -3.98242259e+00 -7.80096483e+00]\n",
      " [ 9.93991852e-01 -1.82595196e+01 -6.61395216e+00 -5.41768837e+00\n",
      "  -1.44216137e+01 -8.34031677e+00 -1.10647354e+01 -1.86828823e+01\n",
      "   9.25201797e+00  3.73276687e+00]\n",
      " [-1.39936018e+01 -2.13224297e+01 -7.66133308e+00  2.48889756e+00\n",
      "  -1.20035954e+01 -1.12967777e+00 -1.76874046e+01 -1.34970627e+01\n",
      "   3.04316968e-01  7.58314490e-01]\n",
      " [-1.43300352e+01 -1.69328041e+01 -1.14070501e+01 -3.44749880e+00\n",
      "  -5.85605919e-01 -3.31505418e+00 -1.91051788e+01 -1.92747629e+00\n",
      "  -4.69577312e-01  9.40207386e+00]\n",
      " [-1.54878817e+01 -5.49419260e+00  1.07202148e+01 -4.43822765e+00\n",
      "  -7.32569885e+00 -1.27014303e+01 -1.06504574e+01 -6.32141352e+00\n",
      "  -9.16736031e+00 -1.42266197e+01]\n",
      " [-1.52416687e+01 -1.60026798e+01 -3.01734900e+00  8.58234215e+00\n",
      "  -1.83687611e+01 -9.71858692e+00 -8.61346149e+00 -9.56177235e+00\n",
      "  -1.17342448e+00 -1.14559870e+01]\n",
      " [-1.30817375e+01 -1.25534897e+01 -1.36964054e+01 -2.87487483e+00\n",
      "  -1.12780428e+01  1.15295458e+01 -7.45012522e+00 -1.53410549e+01\n",
      "  -3.09344697e+00 -1.24245667e+00]\n",
      " [-7.92942524e+00 -6.68333817e+00 -5.83107281e+00 -6.94316053e+00\n",
      "   1.84400880e+00 -8.87770844e+00 -9.19060612e+00 -1.10800397e+00\n",
      "  -5.21806765e+00  4.69903886e-01]\n",
      " [-4.54393005e+00  1.07087898e+01 -5.60519123e+00 -8.96032810e+00\n",
      "   1.16441834e+00 -6.27383995e+00 -4.59220505e+00 -1.88092458e+00\n",
      "  -4.02307272e+00 -4.23809338e+00]\n",
      " [ 7.88212681e+00 -1.66077042e+01 -1.70266569e+00 -8.92977428e+00\n",
      "  -1.31505966e+01 -9.90190601e+00 -6.91587734e+00 -7.81945181e+00\n",
      "   3.42150903e+00 -8.64401102e-01]\n",
      " [-1.25184422e+01 -1.28996363e+01 -4.87523270e+00 -5.05019379e+00\n",
      "  -8.98691237e-01 -8.93863201e+00 -1.74940014e+01 -4.48710823e+00\n",
      "  -4.03536463e+00  8.19290447e+00]\n",
      " [-8.77809811e+00 -6.47422314e+00  1.04086552e+01 -5.90250063e+00\n",
      "  -1.30832644e+01 -1.76847687e+01 -6.76111460e+00 -9.98475266e+00\n",
      "  -4.44939184e+00 -1.33067007e+01]\n",
      " [-1.50678873e+01 -1.18601112e+01 -9.29417706e+00 -9.58020782e+00\n",
      "   1.02728376e+01 -1.03378181e+01 -7.19997692e+00 -8.95764732e+00\n",
      "  -6.72616100e+00 -4.48122621e-02]\n",
      " [-1.45688677e+01 -1.35030222e+01 -6.20817995e+00  7.34791899e+00\n",
      "  -1.85800285e+01 -6.15481472e+00 -2.23778877e+01 -1.58699000e+00\n",
      "  -4.13376427e+00 -7.29298592e+00]\n",
      " [-5.28585482e+00 -7.04930305e+00 -5.47638941e+00 -1.42355576e+01\n",
      "  -8.21135789e-02 -3.67805195e+00  1.44564037e+01 -1.21615191e+01\n",
      "  -8.18415833e+00 -1.08899288e+01]\n",
      " [-8.03988647e+00 -1.04709654e+01 -8.92302704e+00 -7.19340611e+00\n",
      "  -2.99362373e+00 -1.07344446e+01 -2.29647064e+01  8.36859894e+00\n",
      "  -1.07560596e+01 -1.18406224e+00]\n",
      " [-1.52661028e+01 -9.50314426e+00  1.09989634e+01 -4.09558392e+00\n",
      "  -1.84241276e+01 -1.48976421e+01 -1.15740557e+01 -1.30585623e+01\n",
      "  -9.59422946e-01 -1.11805716e+01]\n",
      " [ 1.03123083e+01 -1.09955206e+01 -6.92508841e+00 -1.08314257e+01\n",
      "  -2.17764339e+01 -3.83421326e+00  9.06576812e-01 -1.63003864e+01\n",
      "  -7.41694403e+00 -3.74612713e+00]\n",
      " [-5.50822258e+00 -1.26077309e+01 -9.91959667e+00 -6.02662325e+00\n",
      "  -6.80452394e+00 -3.61902937e-02  1.57442846e+01 -1.84615154e+01\n",
      "  -1.75355399e+00 -1.02027969e+01]\n",
      " [-5.60601616e+00 -1.36773758e+01 -9.27303696e+00 -8.74139881e+00\n",
      "  -7.93290520e+00 -1.49116427e-01  1.52584953e+01 -1.93070812e+01\n",
      "  -8.15614045e-01 -8.96646309e+00]\n",
      " [-5.54070950e+00  2.60017490e+00 -6.43968153e+00 -1.27037277e+01\n",
      "  -3.20807052e+00 -1.04951305e+01 -8.44599533e+00 -9.22508955e-01\n",
      "  -3.46248507e+00 -7.30490208e+00]\n",
      " [-1.25903988e+01 -5.80655670e+00 -9.66536617e+00 -6.94724035e+00\n",
      "   7.56693745e+00 -6.76717901e+00 -1.03354053e+01 -4.15474319e+00\n",
      "  -5.99393511e+00  8.92828882e-01]\n",
      " [-1.88520756e+01 -1.44098463e+01 -1.13859482e+01  1.15638666e+01\n",
      "  -1.55562029e+01 -2.77319145e+00 -2.62106438e+01 -5.36917734e+00\n",
      "  -2.01347017e+00 -1.42425966e+00]\n",
      " [-1.35736046e+01 -1.75157375e+01 -1.37623367e+01 -6.68451786e+00\n",
      "  -7.21026421e+00 -7.09806442e+00 -2.11117554e+01 -4.22333431e+00\n",
      "   1.81623101e-02  5.08330584e+00]\n",
      " [-1.15705814e+01 -1.11352425e+01 -4.28443241e+00 -3.85054588e+00\n",
      "  -8.29530907e+00 -8.24508858e+00 -2.24156342e+01  1.11709747e+01\n",
      "  -9.47648907e+00 -3.81996250e+00]\n",
      " [-1.84738464e+01 -1.10646124e+01 -1.19349337e+01 -1.45630589e+01\n",
      "   1.23748741e+01 -1.01002808e+01 -8.43909550e+00 -1.54222345e+00\n",
      "  -2.02166772e+00  1.09302866e+00]]\n"
     ]
    }
   ],
   "source": [
    "print (answers[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 9 0 3 7 0 3 0 3 5 7 4 0 4 3 3 1 9 0 9 1 1 5 7 4 2 7 4 7 7 5 4 2 6 2 5\n",
      " 5 1 6 7 7 4 9 8 7 8 2 6 7 6 8 8 3 8 2 1 2 2 0 4 1 7 0 0 0 1 9 0 1 6 5 8 8\n",
      " 2 8 3 9 2 3 5 4 1 0 9 2 4 3 6 7 2 0 6 6 1 4 3 9 7 4 0 9 2 0 7 3 0 5 0 8 0\n",
      " 0 4 7 1 7 1 1 3 3 3 7 2 8 6 3 8 7 8 4 3 5 6 0 0 0 3 1 3 6 4 3 4 5 5 8 7 7\n",
      " 2 8 4 3 5 6 5 3 7 5 7 8 3 0 4 5 1 3 7 6 3 0 2 7 8 6 1 3 7 4 1 2 4 8 5 2 4\n",
      " 9 2 1 6 0 6 1 4 9 6 0 9 7 6 9 1 9 0 9 9 0 8 4 6 2 0 9 3 6 3 2 1 6 3 4 2 3\n",
      " 1 2 2 0 4 6 1 0 0 4 9 1 7 3 2 3 8 6 8 6 2 8 5 5 4 8 3 5 9 7 1 3 8 4 5 1 4\n",
      " 5 6 3 3 5 7 0 6 8 3 1 6 0 6 3 9 3 1 5 8 4 0 9 2 0 5 3 7 1 9 9 5 7 7 9 9 6\n",
      " 3 0 3 3 6 9 8 2 6 3 7 1 4 5 8 5 9 0 0 3 8 4 1 8 4 1 1 9 8 4 5 1 5 3 6 3 1\n",
      " 3 0 9 0 0 6 0 6 3 1 8 6 0 6 5 2 2 6 7 7 2 5 8 3 9 2 7 8 6 3 8 4 2 3 8 1 6\n",
      " 4 8 7 9 7 6 9 5 3 7 6 5 5 4 2 6 2 1 3 7 1 7 9 9 6 1 1 1 7 3 9 7 6 1 1 1 9\n",
      " 3 8 5 5 0 4 1 2 3 1 1 3 5 9 6 6 5 3 1 4 7 4 7 4 8 5 2 6 1 3 9 5 0 8 4 7 4\n",
      " 4 4 1 5 3 9 9 7 6 9 5 9 2 3 5 6 6 7 5 0 5 1 7 4 4 1 1 4 9 5 6 0 1 3 1 0 4\n",
      " 8 1 2 7 9 4 8 3 7 7 4 2 4 6 7 6 3 2 0 6 5 9 4 1 8 3 3 0 2 7 8 8 7 5 3 5 7\n",
      " 4 3 6 9 0 7 7 1 0 1 1 7 0 5 3 8 3 5 6 5 7 3 0 2 8 2 0 3 0 9 2 1 1 3 0 5 5\n",
      " 0 7 5 6 2 0 3 8 1 6 5 4 1 1 4 6 5 3 6 0 4 8 2 4 3 5 1 7 6 9 1 7 3 8 0 8 8\n",
      " 4 5 3 6 6 6 0 3 5 1 7 1 6 2 8 5 6 4 7 4 3 3 2 4 7 0 0 9 8 5 9 4 0 8 8 3 6\n",
      " 2 6 1 8 6 1 4 7 7 8 3 0 9 9 6 7 7 4 4 1 8 4 8 0 2 8 2 4 3 3 7 2 3 4 0 4 8\n",
      " 1 3 3 6 3 9 4 3 8 7 7 2 6 0 6 9 8 8 1 3 4 6 9 9 2 6 0 1 8 4 3 9 8 8 4 0 5\n",
      " 0 6 0 4 4 6 5 1 8 1 5 3 6 2 3 7 8 9 3 1 0 1 0 6 4 7 5 7 1 3 2 7 7 1 5 1 5\n",
      " 4 4 3 4 3 9 0 7 8 6 4 9 4 4 1 4 7 1 1 8 3 0 4 0 4 0 0 5 1 8 6 5 0 1 5 3 4\n",
      " 6 3 1 1 6 9 8 3 5 5 4 8 8 5 0 4 0 4 3 1 6 9 9 1 1 3 3 1 4 9 6 9 1 5 4 2 3\n",
      " 2 4 0 9 7 4 3 0 5 0 1 9 0 4 5 2 8 8 5 9 3 9 6 1 5 5 1 9 0 8 4 6 7 2 8 5 8\n",
      " 9 7 7 2 8 1 3 4 5 0 4 1 4 2 3 6 9 2 3 4 5 4 2 3 3 1 1 0 1 4 9 1 1 2 7 1 5\n",
      " 4 9 1 7 6 0 4 2 9 4 1 1 5 3 5 7 4 7 8 3 2 7 2 0 4 7 1 6 4 6 1 5 7 3 5 9 4\n",
      " 7 9 6 6 3 3 2 1 4 5 3 7 7 9 5 6 2 6 1 0 9 3 2 9 2 6 7 5 2 3 2 8 3 0 2 7 9\n",
      " 4 0 9 5 1 8 8 5 3 2 9 6 7 0 8 0 7 4 5 8 7 9 7 7 0 5 3 2 1 9 0 6 8 3 6 2 2\n",
      " 9]\n"
     ]
    }
   ],
   "source": [
    "print (vals[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission2.txt\", \"w\") as fout:\n",
    "    fout.write(\"ImageId,Label\\n\")\n",
    "    for i, val in enumerate(vals):\n",
    "        fout.write(\"{},{}\\n\".format(str(i+1), str(val)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
