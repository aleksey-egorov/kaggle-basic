{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import PIL\n",
    "\n",
    "#from utils import *\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y: [1. 0. 1. 4. 0. 0. 7. 3. 5. 3.]\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt('./train.csv', delimiter=',')\n",
    "data = np.delete(data, (0), axis=0)\n",
    "data_np_y = data[:,0]\n",
    "data = np.delete(data, (0), axis=1)\n",
    "data_np_x = data / 255\n",
    "\n",
    "\n",
    "pos = int(data_np_x.shape[0] * 0.9)\n",
    "\n",
    "train_np_y = data_np_y[:pos]\n",
    "train_np_x = data_np_x[:pos]\n",
    "test_np_y = data_np_y[pos:]\n",
    "test_np_x = data_np_x[pos:]\n",
    "\n",
    "print (\"Y: {}\".format(train_np_y[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784) (42000,)\n",
      "(37800, 784) (37800,)\n",
      "(4200, 784) (4200,)\n"
     ]
    }
   ],
   "source": [
    "print (data_np_x.shape, data_np_y.shape)\n",
    "print (train_np_x.shape, train_np_y.shape)\n",
    "print (test_np_x.shape, test_np_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 784) (4200,)\n"
     ]
    }
   ],
   "source": [
    "print (test_np_x.shape, test_np_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118d10748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 4. 0. 0. 7. 3. 5. 3.]\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,10), dpi=80)\n",
    "shape = (1,10)\n",
    "for j in range(10):\n",
    "    ax = fig.add_subplot(shape[0], shape[1], j+1)\n",
    "    image = train_np_x[j].reshape(28,28)\n",
    "    ax.matshow(image, cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "plt.show()\n",
    "\n",
    "print (train_np_y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x = (x - 0.1307) / 0.3081    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.Tensor(np.expand_dims(normalize(train_np_x), axis=1))\n",
    "train_y = torch.LongTensor(train_np_y)\n",
    "\n",
    "test_x = torch.Tensor(np.expand_dims(normalize(test_np_x), axis=1))\n",
    "test_y = torch.LongTensor(test_np_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(x, y, batch_size=32):\n",
    "    for i in range(0, x.size(0) - 1, batch_size):\n",
    "        data = x[i:i+batch_size]\n",
    "        if data.shape[0] == batch_size:\n",
    "            data = data.reshape(batch_size, 1, 28, 28)\n",
    "            targets = y[i:i+batch_size]\n",
    "\n",
    "            yield data, targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
    "                 bn=False, dropout=False, activation_fn=nn.ReLU()):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        layers = []        \n",
    "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding))\n",
    "        if pool_layer is not None:\n",
    "            layers.append(pool_layer)\n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm2d(size[1]))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout2d())\n",
    "        layers.append(activation_fn)\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout())\n",
    "            layers.append(activation_fn())\n",
    "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
    "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, batchnorm=False, dropout=False, lr=5e-5, l2=0.):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "        self._conv1 = ConvLayer([1, 32, 3], bn=batchnorm)\n",
    "        self._conv2 = ConvLayer([32, 64, 3], bn=batchnorm, activation_fn=nn.Sigmoid())\n",
    "        \n",
    "        self.fc = FullyConnected([64*7*7, 512, 10], dropout=dropout)\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.lr_base = lr\n",
    "        self.l2 = l2\n",
    "        self._loss = None\n",
    "        self.optim = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.l2)\n",
    "    \n",
    "    def conv(self, x):\n",
    "        x = self._conv1(x)\n",
    "        x = self._conv2(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):        \n",
    "        self._loss = F.cross_entropy(output, target, **kwargs)\n",
    "        self._correct = output.data.max(1, keepdim=True)[1]\n",
    "        self._correct = self._correct.eq(target.data.view_as(self._correct)).to(torch.float).cpu().mean()\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, y):\n",
    "    r = np.random.permutation(len(y))\n",
    "    return X[r], y[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, models, log=None):    \n",
    "    for model in models.values():\n",
    "        model.lr = model.lr_base * (100 / (epoch + 100))        \n",
    "        model.optim = optim.Adam(model.parameters(), lr=model.lr, weight_decay=model.l2)\n",
    "    print (\"LR: {}\".format(models['bn'].lr))    \n",
    "    train_size = len(train_x)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(loader(train_x, train_y)):        \n",
    "        for model in models.values():                             \n",
    "            model.optim.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = model.loss(output, target)\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "            \n",
    "        if batch_idx % 200 == 0:\n",
    "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / train_size)\n",
    "            losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "            print(line + losses)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / train_size)\n",
    "        losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "        if log is not None:\n",
    "            for k in models:\n",
    "                log[k].append((models[k]._loss, models[k]._correct))\n",
    "        print(line + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, log=None):\n",
    "    test_size = len(test_x)\n",
    "    avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
    "    acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, test_size, p)\n",
    "    line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
    "\n",
    "    test_loss = {k: 0. for k in models}\n",
    "    correct = {k: 0. for k in models}\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader(test_x, test_y):\n",
    "            output = {k: m(data) for k, m in models.items()}           \n",
    "            for k, m in models.items():     \n",
    "                #print (output[k].shape, target.shape)\n",
    "                test_loss[k] += m.loss(output[k], target, size_average=False).item() # sum up batch loss\n",
    "                pred = output[k].data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "                correct[k] += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "    \n",
    "    for k in models:\n",
    "        test_loss[k] /= test_size\n",
    "    correct_pct = {k: c / test_size for k, c in correct.items()}\n",
    "    lines = '\\n'.join([line(k, test_loss[k], correct[k], 100*correct_pct[k]) for k in models]) + '\\n'\n",
    "    report = 'Test set:\\n' + lines\n",
    "    if log is not None:\n",
    "        for k in models:\n",
    "            log[k].append((test_loss[k], correct_pct[k]))\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(log, tpe='loss'):\n",
    "    keys = log.keys()\n",
    "    logs = {k:[z for z in zip(*log[k])] for k in keys}\n",
    "    epochs = {k:range(len(log[k])) for k in keys}\n",
    " \n",
    "    \n",
    "    if tpe == 'loss':\n",
    "        handlers, = zip(*[plt.plot(epochs[k], logs[k][0], label=k) for k in keys])\n",
    "        plt.title('errors')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('error')\n",
    "        plt.legend(handles=handlers)\n",
    "        plt.show()\n",
    "    elif tpe == 'accuracy':\n",
    "        handlers, = zip(*[plt.plot(epochs[k], logs[k][1], label=k) for k in log.keys()])\n",
    "        plt.title('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.legend(handles=handlers)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'bn': Net(True), 'drop': Net(False, True), 'bndrop': Net(True, True), 'plain': Net()}\n",
    "train_log = {k: [] for k in models}\n",
    "test_log = {k: [] for k in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 4.950495049504951e-05\n",
      "Train Epoch: 1 [0/37800 (0%)]\tLosses bn: 2.323382 drop: 2.336625 bndrop: 2.259632 plain: 2.361825\n",
      "Train Epoch: 1 [6400/37800 (1%)]\tLosses bn: 0.919566 drop: 2.106853 bndrop: 1.200318 plain: 1.710013\n",
      "Train Epoch: 1 [12800/37800 (1%)]\tLosses bn: 0.440318 drop: 1.215299 bndrop: 0.621952 plain: 0.793385\n",
      "Train Epoch: 1 [19200/37800 (2%)]\tLosses bn: 0.240221 drop: 0.714859 bndrop: 0.372376 plain: 0.435877\n",
      "Train Epoch: 1 [25600/37800 (2%)]\tLosses bn: 0.104382 drop: 0.482311 bndrop: 0.200921 plain: 0.285753\n",
      "Train Epoch: 1 [32000/37800 (3%)]\tLosses bn: 0.244675 drop: 0.573352 bndrop: 0.310906 plain: 0.460408\n",
      "Train Epoch: 1 [37792/37800 (3%)]\tLosses bn: 0.236456 drop: 0.516539 bndrop: 0.294491 plain: 0.434818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksey/anaconda3/envs/learning/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "bn: Loss: 0.1603\tAccuracy: 4021.0/4200 (96%)\n",
      "drop: Loss: 0.3484\tAccuracy: 3789.0/4200 (90%)\n",
      "bndrop: Loss: 0.1852\tAccuracy: 3983.0/4200 (95%)\n",
      "plain: Loss: 0.3183\tAccuracy: 3822.0/4200 (91%)\n",
      "\n",
      "LR: 4.901960784313725e-05\n",
      "Train Epoch: 2 [0/37800 (0%)]\tLosses bn: 0.238083 drop: 0.471508 bndrop: 0.299616 plain: 0.445528\n",
      "Train Epoch: 2 [6400/37800 (1%)]\tLosses bn: 0.164835 drop: 0.413311 bndrop: 0.186913 plain: 0.349515\n",
      "Train Epoch: 2 [12800/37800 (1%)]\tLosses bn: 0.230964 drop: 0.382299 bndrop: 0.313414 plain: 0.344423\n",
      "Train Epoch: 2 [19200/37800 (2%)]\tLosses bn: 0.065044 drop: 0.197598 bndrop: 0.144994 plain: 0.141267\n",
      "Train Epoch: 2 [25600/37800 (2%)]\tLosses bn: 0.067986 drop: 0.220977 bndrop: 0.114234 plain: 0.174761\n",
      "Train Epoch: 2 [32000/37800 (3%)]\tLosses bn: 0.102187 drop: 0.214201 bndrop: 0.129111 plain: 0.213251\n",
      "Train Epoch: 2 [37792/37800 (3%)]\tLosses bn: 0.201521 drop: 0.317748 bndrop: 0.174544 plain: 0.296968\n",
      "Test set:\n",
      "bn: Loss: 0.0959\tAccuracy: 4060.0/4200 (97%)\n",
      "drop: Loss: 0.1918\tAccuracy: 3936.0/4200 (94%)\n",
      "bndrop: Loss: 0.0999\tAccuracy: 4053.0/4200 (96%)\n",
      "plain: Loss: 0.1940\tAccuracy: 3939.0/4200 (94%)\n",
      "\n",
      "LR: 4.854368932038835e-05\n",
      "Train Epoch: 3 [0/37800 (0%)]\tLosses bn: 0.043395 drop: 0.116114 bndrop: 0.058628 plain: 0.067585\n",
      "Train Epoch: 3 [6400/37800 (1%)]\tLosses bn: 0.032893 drop: 0.182862 bndrop: 0.077358 plain: 0.155115\n",
      "Train Epoch: 3 [12800/37800 (1%)]\tLosses bn: 0.089289 drop: 0.125377 bndrop: 0.147517 plain: 0.119466\n",
      "Train Epoch: 3 [19200/37800 (2%)]\tLosses bn: 0.078594 drop: 0.208079 bndrop: 0.132335 plain: 0.176967\n",
      "Train Epoch: 3 [25600/37800 (2%)]\tLosses bn: 0.010426 drop: 0.078718 bndrop: 0.024261 plain: 0.081346\n",
      "Train Epoch: 3 [32000/37800 (3%)]\tLosses bn: 0.006885 drop: 0.114466 bndrop: 0.021917 plain: 0.080413\n",
      "Train Epoch: 3 [37792/37800 (3%)]\tLosses bn: 0.024376 drop: 0.064465 bndrop: 0.038815 plain: 0.040369\n",
      "Test set:\n",
      "bn: Loss: 0.0804\tAccuracy: 4086.0/4200 (97%)\n",
      "drop: Loss: 0.1418\tAccuracy: 4003.0/4200 (95%)\n",
      "bndrop: Loss: 0.0821\tAccuracy: 4081.0/4200 (97%)\n",
      "plain: Loss: 0.1413\tAccuracy: 4003.0/4200 (95%)\n",
      "\n",
      "LR: 4.8076923076923084e-05\n",
      "Train Epoch: 4 [0/37800 (0%)]\tLosses bn: 0.057003 drop: 0.114394 bndrop: 0.059553 plain: 0.137451\n",
      "Train Epoch: 4 [6400/37800 (1%)]\tLosses bn: 0.019728 drop: 0.079215 bndrop: 0.033917 plain: 0.091117\n",
      "Train Epoch: 4 [12800/37800 (1%)]\tLosses bn: 0.227454 drop: 0.331831 bndrop: 0.253108 plain: 0.393449\n",
      "Train Epoch: 4 [19200/37800 (2%)]\tLosses bn: 0.068705 drop: 0.123677 bndrop: 0.087849 plain: 0.108781\n",
      "Train Epoch: 4 [25600/37800 (2%)]\tLosses bn: 0.008230 drop: 0.042089 bndrop: 0.014443 plain: 0.024007\n",
      "Train Epoch: 4 [32000/37800 (3%)]\tLosses bn: 0.158330 drop: 0.270533 bndrop: 0.146003 plain: 0.237168\n",
      "Train Epoch: 4 [37792/37800 (3%)]\tLosses bn: 0.018798 drop: 0.026638 bndrop: 0.054590 plain: 0.018641\n",
      "Test set:\n",
      "bn: Loss: 0.0675\tAccuracy: 4093.0/4200 (97%)\n",
      "drop: Loss: 0.1113\tAccuracy: 4046.0/4200 (96%)\n",
      "bndrop: Loss: 0.0699\tAccuracy: 4094.0/4200 (97%)\n",
      "plain: Loss: 0.1085\tAccuracy: 4054.0/4200 (97%)\n",
      "\n",
      "LR: 4.761904761904762e-05\n",
      "Train Epoch: 5 [0/37800 (0%)]\tLosses bn: 0.006321 drop: 0.028650 bndrop: 0.020408 plain: 0.011494\n",
      "Train Epoch: 5 [6400/37800 (1%)]\tLosses bn: 0.060965 drop: 0.215921 bndrop: 0.116317 plain: 0.219646\n",
      "Train Epoch: 5 [12800/37800 (1%)]\tLosses bn: 0.034068 drop: 0.080846 bndrop: 0.053644 plain: 0.075968\n",
      "Train Epoch: 5 [19200/37800 (2%)]\tLosses bn: 0.006240 drop: 0.036766 bndrop: 0.026565 plain: 0.013450\n",
      "Train Epoch: 5 [25600/37800 (2%)]\tLosses bn: 0.012387 drop: 0.075699 bndrop: 0.014255 plain: 0.049901\n",
      "Train Epoch: 5 [32000/37800 (3%)]\tLosses bn: 0.013356 drop: 0.034318 bndrop: 0.058958 plain: 0.034253\n",
      "Train Epoch: 5 [37792/37800 (3%)]\tLosses bn: 0.141293 drop: 0.243685 bndrop: 0.142653 plain: 0.247319\n",
      "Test set:\n",
      "bn: Loss: 0.0507\tAccuracy: 4119.0/4200 (98%)\n",
      "drop: Loss: 0.0873\tAccuracy: 4069.0/4200 (97%)\n",
      "bndrop: Loss: 0.0570\tAccuracy: 4119.0/4200 (98%)\n",
      "plain: Loss: 0.0846\tAccuracy: 4080.0/4200 (97%)\n",
      "\n",
      "LR: 4.716981132075472e-05\n",
      "Train Epoch: 6 [0/37800 (0%)]\tLosses bn: 0.016618 drop: 0.114437 bndrop: 0.028876 plain: 0.043744\n",
      "Train Epoch: 6 [6400/37800 (1%)]\tLosses bn: 0.030763 drop: 0.048016 bndrop: 0.056358 plain: 0.037400\n",
      "Train Epoch: 6 [12800/37800 (1%)]\tLosses bn: 0.001811 drop: 0.021629 bndrop: 0.011105 plain: 0.012774\n",
      "Train Epoch: 6 [19200/37800 (2%)]\tLosses bn: 0.009331 drop: 0.036759 bndrop: 0.009710 plain: 0.030653\n",
      "Train Epoch: 6 [25600/37800 (2%)]\tLosses bn: 0.027443 drop: 0.114286 bndrop: 0.047797 plain: 0.102343\n",
      "Train Epoch: 6 [32000/37800 (3%)]\tLosses bn: 0.016472 drop: 0.035013 bndrop: 0.045361 plain: 0.059147\n",
      "Train Epoch: 6 [37792/37800 (3%)]\tLosses bn: 0.136725 drop: 0.206707 bndrop: 0.208269 plain: 0.202825\n",
      "Test set:\n",
      "bn: Loss: 0.0617\tAccuracy: 4104.0/4200 (98%)\n",
      "drop: Loss: 0.0788\tAccuracy: 4085.0/4200 (97%)\n",
      "bndrop: Loss: 0.0608\tAccuracy: 4107.0/4200 (98%)\n",
      "plain: Loss: 0.0796\tAccuracy: 4080.0/4200 (97%)\n",
      "\n",
      "LR: 4.672897196261683e-05\n",
      "Train Epoch: 7 [0/37800 (0%)]\tLosses bn: 0.030741 drop: 0.122493 bndrop: 0.047267 plain: 0.095229\n",
      "Train Epoch: 7 [6400/37800 (1%)]\tLosses bn: 0.006551 drop: 0.058637 bndrop: 0.017470 plain: 0.044136\n",
      "Train Epoch: 7 [12800/37800 (1%)]\tLosses bn: 0.017757 drop: 0.016676 bndrop: 0.029900 plain: 0.011838\n",
      "Train Epoch: 7 [19200/37800 (2%)]\tLosses bn: 0.016100 drop: 0.040922 bndrop: 0.021709 plain: 0.034243\n",
      "Train Epoch: 7 [25600/37800 (2%)]\tLosses bn: 0.006297 drop: 0.014166 bndrop: 0.014913 plain: 0.011607\n",
      "Train Epoch: 7 [32000/37800 (3%)]\tLosses bn: 0.008615 drop: 0.048270 bndrop: 0.016548 plain: 0.048242\n",
      "Train Epoch: 7 [37792/37800 (3%)]\tLosses bn: 0.006486 drop: 0.018143 bndrop: 0.017621 plain: 0.019401\n",
      "Test set:\n",
      "bn: Loss: 0.0448\tAccuracy: 4128.0/4200 (98%)\n",
      "drop: Loss: 0.0635\tAccuracy: 4105.0/4200 (98%)\n",
      "bndrop: Loss: 0.0468\tAccuracy: 4123.0/4200 (98%)\n",
      "plain: Loss: 0.0614\tAccuracy: 4101.0/4200 (98%)\n",
      "\n",
      "LR: 4.62962962962963e-05\n",
      "Train Epoch: 8 [0/37800 (0%)]\tLosses bn: 0.031627 drop: 0.076258 bndrop: 0.035220 plain: 0.043136\n",
      "Train Epoch: 8 [6400/37800 (1%)]\tLosses bn: 0.010738 drop: 0.030696 bndrop: 0.033319 plain: 0.025003\n",
      "Train Epoch: 8 [12800/37800 (1%)]\tLosses bn: 0.111939 drop: 0.086960 bndrop: 0.106953 plain: 0.038577\n",
      "Train Epoch: 8 [19200/37800 (2%)]\tLosses bn: 0.071442 drop: 0.208283 bndrop: 0.167072 plain: 0.212893\n",
      "Train Epoch: 8 [25600/37800 (2%)]\tLosses bn: 0.038015 drop: 0.115352 bndrop: 0.045880 plain: 0.099960\n",
      "Train Epoch: 8 [32000/37800 (3%)]\tLosses bn: 0.017592 drop: 0.069297 bndrop: 0.024205 plain: 0.046578\n",
      "Train Epoch: 8 [37792/37800 (3%)]\tLosses bn: 0.041063 drop: 0.055757 bndrop: 0.096716 plain: 0.048671\n",
      "Test set:\n",
      "bn: Loss: 0.0713\tAccuracy: 4094.0/4200 (97%)\n",
      "drop: Loss: 0.0700\tAccuracy: 4091.0/4200 (97%)\n",
      "bndrop: Loss: 0.0543\tAccuracy: 4116.0/4200 (98%)\n",
      "plain: Loss: 0.0780\tAccuracy: 4074.0/4200 (97%)\n",
      "\n",
      "LR: 4.587155963302753e-05\n",
      "Train Epoch: 9 [0/37800 (0%)]\tLosses bn: 0.018207 drop: 0.020097 bndrop: 0.017400 plain: 0.017154\n",
      "Train Epoch: 9 [6400/37800 (1%)]\tLosses bn: 0.004849 drop: 0.081696 bndrop: 0.009801 plain: 0.061713\n",
      "Train Epoch: 9 [12800/37800 (1%)]\tLosses bn: 0.016483 drop: 0.023821 bndrop: 0.013574 plain: 0.021672\n",
      "Train Epoch: 9 [19200/37800 (2%)]\tLosses bn: 0.008304 drop: 0.020478 bndrop: 0.021342 plain: 0.025414\n",
      "Train Epoch: 9 [25600/37800 (2%)]\tLosses bn: 0.016333 drop: 0.022602 bndrop: 0.070351 plain: 0.019947\n",
      "Train Epoch: 9 [32000/37800 (3%)]\tLosses bn: 0.001338 drop: 0.004146 bndrop: 0.002495 plain: 0.001110\n",
      "Train Epoch: 9 [37792/37800 (3%)]\tLosses bn: 0.002846 drop: 0.014791 bndrop: 0.008188 plain: 0.010354\n",
      "Test set:\n",
      "bn: Loss: 0.0434\tAccuracy: 4135.0/4200 (98%)\n",
      "drop: Loss: 0.0570\tAccuracy: 4109.0/4200 (98%)\n",
      "bndrop: Loss: 0.0462\tAccuracy: 4121.0/4200 (98%)\n",
      "plain: Loss: 0.0528\tAccuracy: 4113.0/4200 (98%)\n",
      "\n",
      "LR: 4.545454545454546e-05\n",
      "Train Epoch: 10 [0/37800 (0%)]\tLosses bn: 0.022102 drop: 0.049792 bndrop: 0.039028 plain: 0.050828\n",
      "Train Epoch: 10 [6400/37800 (1%)]\tLosses bn: 0.088278 drop: 0.111456 bndrop: 0.112976 plain: 0.106699\n",
      "Train Epoch: 10 [12800/37800 (1%)]\tLosses bn: 0.070548 drop: 0.139127 bndrop: 0.124395 plain: 0.156741\n",
      "Train Epoch: 10 [19200/37800 (2%)]\tLosses bn: 0.000294 drop: 0.003608 bndrop: 0.003106 plain: 0.000929\n",
      "Train Epoch: 10 [25600/37800 (2%)]\tLosses bn: 0.004707 drop: 0.008385 bndrop: 0.007976 plain: 0.002891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [32000/37800 (3%)]\tLosses bn: 0.059980 drop: 0.054334 bndrop: 0.083307 plain: 0.048129\n",
      "Train Epoch: 10 [37792/37800 (3%)]\tLosses bn: 0.001153 drop: 0.006644 bndrop: 0.003483 plain: 0.005394\n",
      "Test set:\n",
      "bn: Loss: 0.0402\tAccuracy: 4134.0/4200 (98%)\n",
      "drop: Loss: 0.0532\tAccuracy: 4116.0/4200 (98%)\n",
      "bndrop: Loss: 0.0433\tAccuracy: 4123.0/4200 (98%)\n",
      "plain: Loss: 0.0509\tAccuracy: 4119.0/4200 (98%)\n",
      "\n",
      "LR: 4.5045045045045046e-05\n",
      "Train Epoch: 11 [0/37800 (0%)]\tLosses bn: 0.001992 drop: 0.042070 bndrop: 0.006470 plain: 0.010844\n",
      "Train Epoch: 11 [6400/37800 (1%)]\tLosses bn: 0.009249 drop: 0.014134 bndrop: 0.009113 plain: 0.008301\n",
      "Train Epoch: 11 [12800/37800 (1%)]\tLosses bn: 0.009882 drop: 0.057983 bndrop: 0.079546 plain: 0.050383\n",
      "Train Epoch: 11 [19200/37800 (2%)]\tLosses bn: 0.046158 drop: 0.131316 bndrop: 0.049899 plain: 0.132836\n",
      "Train Epoch: 11 [25600/37800 (2%)]\tLosses bn: 0.004337 drop: 0.005332 bndrop: 0.013984 plain: 0.004226\n",
      "Train Epoch: 11 [32000/37800 (3%)]\tLosses bn: 0.001216 drop: 0.004106 bndrop: 0.014650 plain: 0.003918\n",
      "Train Epoch: 11 [37792/37800 (3%)]\tLosses bn: 0.017212 drop: 0.043836 bndrop: 0.023682 plain: 0.031815\n",
      "Test set:\n",
      "bn: Loss: 0.0385\tAccuracy: 4134.0/4200 (98%)\n",
      "drop: Loss: 0.0509\tAccuracy: 4121.0/4200 (98%)\n",
      "bndrop: Loss: 0.0378\tAccuracy: 4137.0/4200 (98%)\n",
      "plain: Loss: 0.0467\tAccuracy: 4124.0/4200 (98%)\n",
      "\n",
      "LR: 4.464285714285715e-05\n",
      "Train Epoch: 12 [0/37800 (0%)]\tLosses bn: 0.008479 drop: 0.061551 bndrop: 0.017594 plain: 0.076610\n",
      "Train Epoch: 12 [6400/37800 (1%)]\tLosses bn: 0.005707 drop: 0.021708 bndrop: 0.008582 plain: 0.009420\n",
      "Train Epoch: 12 [12800/37800 (1%)]\tLosses bn: 0.002624 drop: 0.004201 bndrop: 0.004165 plain: 0.002020\n",
      "Train Epoch: 12 [19200/37800 (2%)]\tLosses bn: 0.009722 drop: 0.021056 bndrop: 0.022385 plain: 0.012525\n",
      "Train Epoch: 12 [25600/37800 (2%)]\tLosses bn: 0.003774 drop: 0.005950 bndrop: 0.009444 plain: 0.004646\n",
      "Train Epoch: 12 [32000/37800 (3%)]\tLosses bn: 0.007605 drop: 0.028737 bndrop: 0.007948 plain: 0.030321\n",
      "Train Epoch: 12 [37792/37800 (3%)]\tLosses bn: 0.036642 drop: 0.027777 bndrop: 0.056669 plain: 0.053698\n",
      "Test set:\n",
      "bn: Loss: 0.0426\tAccuracy: 4127.0/4200 (98%)\n",
      "drop: Loss: 0.0501\tAccuracy: 4123.0/4200 (98%)\n",
      "bndrop: Loss: 0.0425\tAccuracy: 4128.0/4200 (98%)\n",
      "plain: Loss: 0.0479\tAccuracy: 4129.0/4200 (98%)\n",
      "\n",
      "LR: 4.4247787610619477e-05\n",
      "Train Epoch: 13 [0/37800 (0%)]\tLosses bn: 0.001806 drop: 0.010921 bndrop: 0.008468 plain: 0.012415\n",
      "Train Epoch: 13 [6400/37800 (1%)]\tLosses bn: 0.002262 drop: 0.017119 bndrop: 0.007226 plain: 0.037706\n",
      "Train Epoch: 13 [12800/37800 (1%)]\tLosses bn: 0.002917 drop: 0.004590 bndrop: 0.004536 plain: 0.003034\n",
      "Train Epoch: 13 [19200/37800 (2%)]\tLosses bn: 0.024673 drop: 0.048196 bndrop: 0.016262 plain: 0.023712\n",
      "Train Epoch: 13 [25600/37800 (2%)]\tLosses bn: 0.028582 drop: 0.043136 bndrop: 0.017676 plain: 0.022676\n",
      "Train Epoch: 13 [32000/37800 (3%)]\tLosses bn: 0.017034 drop: 0.130210 bndrop: 0.044158 plain: 0.080382\n",
      "Train Epoch: 13 [37792/37800 (3%)]\tLosses bn: 0.007496 drop: 0.017707 bndrop: 0.007771 plain: 0.023735\n",
      "Test set:\n",
      "bn: Loss: 0.0347\tAccuracy: 4139.0/4200 (99%)\n",
      "drop: Loss: 0.0450\tAccuracy: 4128.0/4200 (98%)\n",
      "bndrop: Loss: 0.0369\tAccuracy: 4137.0/4200 (98%)\n",
      "plain: Loss: 0.0454\tAccuracy: 4131.0/4200 (98%)\n",
      "\n",
      "LR: 4.3859649122807014e-05\n",
      "Train Epoch: 14 [0/37800 (0%)]\tLosses bn: 0.019165 drop: 0.043778 bndrop: 0.043004 plain: 0.029679\n",
      "Train Epoch: 14 [6400/37800 (1%)]\tLosses bn: 0.014321 drop: 0.087914 bndrop: 0.048951 plain: 0.057470\n",
      "Train Epoch: 14 [12800/37800 (1%)]\tLosses bn: 0.002221 drop: 0.003136 bndrop: 0.002347 plain: 0.000864\n",
      "Train Epoch: 14 [19200/37800 (2%)]\tLosses bn: 0.011659 drop: 0.112720 bndrop: 0.111487 plain: 0.080766\n",
      "Train Epoch: 14 [25600/37800 (2%)]\tLosses bn: 0.006286 drop: 0.007025 bndrop: 0.003592 plain: 0.002908\n",
      "Train Epoch: 14 [32000/37800 (3%)]\tLosses bn: 0.007708 drop: 0.056830 bndrop: 0.083100 plain: 0.018893\n",
      "Train Epoch: 14 [37792/37800 (3%)]\tLosses bn: 0.006921 drop: 0.031450 bndrop: 0.029332 plain: 0.036227\n",
      "Test set:\n",
      "bn: Loss: 0.0374\tAccuracy: 4131.0/4200 (98%)\n",
      "drop: Loss: 0.0483\tAccuracy: 4128.0/4200 (98%)\n",
      "bndrop: Loss: 0.0394\tAccuracy: 4134.0/4200 (98%)\n",
      "plain: Loss: 0.0431\tAccuracy: 4132.0/4200 (98%)\n",
      "\n",
      "LR: 4.347826086956522e-05\n",
      "Train Epoch: 15 [0/37800 (0%)]\tLosses bn: 0.010032 drop: 0.010810 bndrop: 0.006889 plain: 0.008138\n",
      "Train Epoch: 15 [6400/37800 (1%)]\tLosses bn: 0.001316 drop: 0.001609 bndrop: 0.001288 plain: 0.002588\n",
      "Train Epoch: 15 [12800/37800 (1%)]\tLosses bn: 0.000344 drop: 0.003942 bndrop: 0.002843 plain: 0.002668\n",
      "Train Epoch: 15 [19200/37800 (2%)]\tLosses bn: 0.003643 drop: 0.019425 bndrop: 0.009451 plain: 0.010594\n",
      "Train Epoch: 15 [25600/37800 (2%)]\tLosses bn: 0.000811 drop: 0.012770 bndrop: 0.006084 plain: 0.004647\n",
      "Train Epoch: 15 [32000/37800 (3%)]\tLosses bn: 0.001540 drop: 0.016467 bndrop: 0.002618 plain: 0.003505\n",
      "Train Epoch: 15 [37792/37800 (3%)]\tLosses bn: 0.000897 drop: 0.007031 bndrop: 0.005404 plain: 0.008122\n",
      "Test set:\n",
      "bn: Loss: 0.0476\tAccuracy: 4121.0/4200 (98%)\n",
      "drop: Loss: 0.0433\tAccuracy: 4137.0/4200 (98%)\n",
      "bndrop: Loss: 0.0414\tAccuracy: 4132.0/4200 (98%)\n",
      "plain: Loss: 0.0446\tAccuracy: 4127.0/4200 (98%)\n",
      "\n",
      "LR: 4.3103448275862066e-05\n",
      "Train Epoch: 16 [0/37800 (0%)]\tLosses bn: 0.045876 drop: 0.035141 bndrop: 0.041382 plain: 0.043954\n",
      "Train Epoch: 16 [6400/37800 (1%)]\tLosses bn: 0.072855 drop: 0.083580 bndrop: 0.131472 plain: 0.073893\n",
      "Train Epoch: 16 [12800/37800 (1%)]\tLosses bn: 0.021017 drop: 0.024141 bndrop: 0.020856 plain: 0.038226\n",
      "Train Epoch: 16 [19200/37800 (2%)]\tLosses bn: 0.013808 drop: 0.028724 bndrop: 0.014516 plain: 0.019283\n",
      "Train Epoch: 16 [25600/37800 (2%)]\tLosses bn: 0.000312 drop: 0.010238 bndrop: 0.008519 plain: 0.003902\n",
      "Train Epoch: 16 [32000/37800 (3%)]\tLosses bn: 0.010388 drop: 0.016792 bndrop: 0.006964 plain: 0.011423\n",
      "Train Epoch: 16 [37792/37800 (3%)]\tLosses bn: 0.008346 drop: 0.023556 bndrop: 0.021084 plain: 0.033337\n",
      "Test set:\n",
      "bn: Loss: 0.0370\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0449\tAccuracy: 4132.0/4200 (98%)\n",
      "bndrop: Loss: 0.0396\tAccuracy: 4137.0/4200 (98%)\n",
      "plain: Loss: 0.0384\tAccuracy: 4137.0/4200 (98%)\n",
      "\n",
      "LR: 4.2735042735042735e-05\n",
      "Train Epoch: 17 [0/37800 (0%)]\tLosses bn: 0.016112 drop: 0.044441 bndrop: 0.021727 plain: 0.025567\n",
      "Train Epoch: 17 [6400/37800 (1%)]\tLosses bn: 0.010226 drop: 0.033425 bndrop: 0.048826 plain: 0.017644\n",
      "Train Epoch: 17 [12800/37800 (1%)]\tLosses bn: 0.006985 drop: 0.020737 bndrop: 0.020011 plain: 0.027309\n",
      "Train Epoch: 17 [19200/37800 (2%)]\tLosses bn: 0.010218 drop: 0.055435 bndrop: 0.086189 plain: 0.074044\n",
      "Train Epoch: 17 [25600/37800 (2%)]\tLosses bn: 0.060221 drop: 0.094154 bndrop: 0.030655 plain: 0.100040\n",
      "Train Epoch: 17 [32000/37800 (3%)]\tLosses bn: 0.005511 drop: 0.035352 bndrop: 0.011171 plain: 0.016269\n",
      "Train Epoch: 17 [37792/37800 (3%)]\tLosses bn: 0.013936 drop: 0.046622 bndrop: 0.065578 plain: 0.036794\n",
      "Test set:\n",
      "bn: Loss: 0.0359\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0397\tAccuracy: 4138.0/4200 (99%)\n",
      "bndrop: Loss: 0.0368\tAccuracy: 4141.0/4200 (99%)\n",
      "plain: Loss: 0.0361\tAccuracy: 4139.0/4200 (99%)\n",
      "\n",
      "LR: 4.2372881355932206e-05\n",
      "Train Epoch: 18 [0/37800 (0%)]\tLosses bn: 0.000436 drop: 0.003380 bndrop: 0.002097 plain: 0.002709\n",
      "Train Epoch: 18 [6400/37800 (1%)]\tLosses bn: 0.024032 drop: 0.011076 bndrop: 0.012674 plain: 0.010688\n",
      "Train Epoch: 18 [12800/37800 (1%)]\tLosses bn: 0.001445 drop: 0.021105 bndrop: 0.008786 plain: 0.016004\n",
      "Train Epoch: 18 [19200/37800 (2%)]\tLosses bn: 0.001927 drop: 0.005004 bndrop: 0.004328 plain: 0.005345\n",
      "Train Epoch: 18 [25600/37800 (2%)]\tLosses bn: 0.002563 drop: 0.017092 bndrop: 0.004919 plain: 0.006753\n",
      "Train Epoch: 18 [32000/37800 (3%)]\tLosses bn: 0.001391 drop: 0.009311 bndrop: 0.004269 plain: 0.003063\n",
      "Train Epoch: 18 [37792/37800 (3%)]\tLosses bn: 0.001547 drop: 0.011376 bndrop: 0.015291 plain: 0.005980\n",
      "Test set:\n",
      "bn: Loss: 0.0423\tAccuracy: 4131.0/4200 (98%)\n",
      "drop: Loss: 0.0446\tAccuracy: 4132.0/4200 (98%)\n",
      "bndrop: Loss: 0.0381\tAccuracy: 4136.0/4200 (98%)\n",
      "plain: Loss: 0.0412\tAccuracy: 4132.0/4200 (98%)\n",
      "\n",
      "LR: 4.201680672268908e-05\n",
      "Train Epoch: 19 [0/37800 (0%)]\tLosses bn: 0.000443 drop: 0.002265 bndrop: 0.000877 plain: 0.000845\n",
      "Train Epoch: 19 [6400/37800 (1%)]\tLosses bn: 0.002086 drop: 0.008379 bndrop: 0.003102 plain: 0.004256\n",
      "Train Epoch: 19 [12800/37800 (1%)]\tLosses bn: 0.001423 drop: 0.003362 bndrop: 0.007173 plain: 0.003238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19 [19200/37800 (2%)]\tLosses bn: 0.000729 drop: 0.004804 bndrop: 0.002018 plain: 0.000955\n",
      "Train Epoch: 19 [25600/37800 (2%)]\tLosses bn: 0.000846 drop: 0.004303 bndrop: 0.002619 plain: 0.001853\n",
      "Train Epoch: 19 [32000/37800 (3%)]\tLosses bn: 0.018855 drop: 0.028385 bndrop: 0.005876 plain: 0.051768\n",
      "Train Epoch: 19 [37792/37800 (3%)]\tLosses bn: 0.000797 drop: 0.015221 bndrop: 0.001467 plain: 0.008911\n",
      "Test set:\n",
      "bn: Loss: 0.0388\tAccuracy: 4133.0/4200 (98%)\n",
      "drop: Loss: 0.0455\tAccuracy: 4130.0/4200 (98%)\n",
      "bndrop: Loss: 0.0427\tAccuracy: 4129.0/4200 (98%)\n",
      "plain: Loss: 0.0387\tAccuracy: 4138.0/4200 (99%)\n",
      "\n",
      "LR: 4.166666666666667e-05\n",
      "Train Epoch: 20 [0/37800 (0%)]\tLosses bn: 0.004063 drop: 0.010223 bndrop: 0.002592 plain: 0.005246\n",
      "Train Epoch: 20 [6400/37800 (1%)]\tLosses bn: 0.002379 drop: 0.016702 bndrop: 0.010013 plain: 0.019044\n",
      "Train Epoch: 20 [12800/37800 (1%)]\tLosses bn: 0.000237 drop: 0.001581 bndrop: 0.001472 plain: 0.000819\n",
      "Train Epoch: 20 [19200/37800 (2%)]\tLosses bn: 0.000338 drop: 0.004884 bndrop: 0.001250 plain: 0.001020\n",
      "Train Epoch: 20 [25600/37800 (2%)]\tLosses bn: 0.000440 drop: 0.003992 bndrop: 0.001059 plain: 0.001038\n",
      "Train Epoch: 20 [32000/37800 (3%)]\tLosses bn: 0.001880 drop: 0.029449 bndrop: 0.012568 plain: 0.020620\n",
      "Train Epoch: 20 [37792/37800 (3%)]\tLosses bn: 0.004242 drop: 0.049214 bndrop: 0.024832 plain: 0.037151\n",
      "Test set:\n",
      "bn: Loss: 0.0362\tAccuracy: 4140.0/4200 (99%)\n",
      "drop: Loss: 0.0389\tAccuracy: 4140.0/4200 (99%)\n",
      "bndrop: Loss: 0.0370\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0351\tAccuracy: 4139.0/4200 (99%)\n",
      "\n",
      "LR: 4.132231404958678e-05\n",
      "Train Epoch: 21 [0/37800 (0%)]\tLosses bn: 0.051999 drop: 0.069516 bndrop: 0.052742 plain: 0.057808\n",
      "Train Epoch: 21 [6400/37800 (1%)]\tLosses bn: 0.000636 drop: 0.004948 bndrop: 0.002076 plain: 0.005490\n",
      "Train Epoch: 21 [12800/37800 (1%)]\tLosses bn: 0.005909 drop: 0.018366 bndrop: 0.020469 plain: 0.020413\n",
      "Train Epoch: 21 [19200/37800 (2%)]\tLosses bn: 0.002645 drop: 0.005554 bndrop: 0.001041 plain: 0.003866\n",
      "Train Epoch: 21 [25600/37800 (2%)]\tLosses bn: 0.009475 drop: 0.013643 bndrop: 0.027547 plain: 0.017375\n",
      "Train Epoch: 21 [32000/37800 (3%)]\tLosses bn: 0.001543 drop: 0.040258 bndrop: 0.018858 plain: 0.024796\n",
      "Train Epoch: 21 [37792/37800 (3%)]\tLosses bn: 0.010643 drop: 0.018854 bndrop: 0.015620 plain: 0.022387\n",
      "Test set:\n",
      "bn: Loss: 0.0344\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0371\tAccuracy: 4146.0/4200 (99%)\n",
      "bndrop: Loss: 0.0332\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0343\tAccuracy: 4144.0/4200 (99%)\n",
      "\n",
      "LR: 4.098360655737705e-05\n",
      "Train Epoch: 22 [0/37800 (0%)]\tLosses bn: 0.001120 drop: 0.003902 bndrop: 0.016140 plain: 0.005290\n",
      "Train Epoch: 22 [6400/37800 (1%)]\tLosses bn: 0.000816 drop: 0.011066 bndrop: 0.002944 plain: 0.007844\n",
      "Train Epoch: 22 [12800/37800 (1%)]\tLosses bn: 0.009316 drop: 0.003774 bndrop: 0.004129 plain: 0.003834\n",
      "Train Epoch: 22 [19200/37800 (2%)]\tLosses bn: 0.000852 drop: 0.005007 bndrop: 0.006904 plain: 0.001008\n",
      "Train Epoch: 22 [25600/37800 (2%)]\tLosses bn: 0.002203 drop: 0.037319 bndrop: 0.017789 plain: 0.036758\n",
      "Train Epoch: 22 [32000/37800 (3%)]\tLosses bn: 0.042201 drop: 0.094961 bndrop: 0.023510 plain: 0.078394\n",
      "Train Epoch: 22 [37792/37800 (3%)]\tLosses bn: 0.000929 drop: 0.005112 bndrop: 0.002305 plain: 0.001769\n",
      "Test set:\n",
      "bn: Loss: 0.0441\tAccuracy: 4132.0/4200 (98%)\n",
      "drop: Loss: 0.0375\tAccuracy: 4143.0/4200 (99%)\n",
      "bndrop: Loss: 0.0415\tAccuracy: 4135.0/4200 (98%)\n",
      "plain: Loss: 0.0356\tAccuracy: 4136.0/4200 (98%)\n",
      "\n",
      "LR: 4.065040650406504e-05\n",
      "Train Epoch: 23 [0/37800 (0%)]\tLosses bn: 0.023989 drop: 0.005937 bndrop: 0.017251 plain: 0.010567\n",
      "Train Epoch: 23 [6400/37800 (1%)]\tLosses bn: 0.001382 drop: 0.075302 bndrop: 0.020857 plain: 0.025671\n",
      "Train Epoch: 23 [12800/37800 (1%)]\tLosses bn: 0.006038 drop: 0.011588 bndrop: 0.004142 plain: 0.006817\n",
      "Train Epoch: 23 [19200/37800 (2%)]\tLosses bn: 0.000460 drop: 0.012683 bndrop: 0.011094 plain: 0.005581\n",
      "Train Epoch: 23 [25600/37800 (2%)]\tLosses bn: 0.000534 drop: 0.012753 bndrop: 0.008812 plain: 0.001754\n",
      "Train Epoch: 23 [32000/37800 (3%)]\tLosses bn: 0.043384 drop: 0.035044 bndrop: 0.011336 plain: 0.080681\n",
      "Train Epoch: 23 [37792/37800 (3%)]\tLosses bn: 0.007731 drop: 0.015679 bndrop: 0.002063 plain: 0.017341\n",
      "Test set:\n",
      "bn: Loss: 0.0472\tAccuracy: 4134.0/4200 (98%)\n",
      "drop: Loss: 0.0391\tAccuracy: 4142.0/4200 (99%)\n",
      "bndrop: Loss: 0.0363\tAccuracy: 4136.0/4200 (98%)\n",
      "plain: Loss: 0.0395\tAccuracy: 4139.0/4200 (99%)\n",
      "\n",
      "LR: 4.032258064516129e-05\n",
      "Train Epoch: 24 [0/37800 (0%)]\tLosses bn: 0.006605 drop: 0.058799 bndrop: 0.010746 plain: 0.025898\n",
      "Train Epoch: 24 [6400/37800 (1%)]\tLosses bn: 0.002983 drop: 0.002243 bndrop: 0.000932 plain: 0.001099\n",
      "Train Epoch: 24 [12800/37800 (1%)]\tLosses bn: 0.007015 drop: 0.020254 bndrop: 0.010940 plain: 0.016606\n",
      "Train Epoch: 24 [19200/37800 (2%)]\tLosses bn: 0.000844 drop: 0.088822 bndrop: 0.002379 plain: 0.024749\n",
      "Train Epoch: 24 [25600/37800 (2%)]\tLosses bn: 0.052553 drop: 0.106582 bndrop: 0.068419 plain: 0.058751\n",
      "Train Epoch: 24 [32000/37800 (3%)]\tLosses bn: 0.001208 drop: 0.002446 bndrop: 0.001707 plain: 0.001460\n",
      "Train Epoch: 24 [37792/37800 (3%)]\tLosses bn: 0.000138 drop: 0.007311 bndrop: 0.002002 plain: 0.004747\n",
      "Test set:\n",
      "bn: Loss: 0.0382\tAccuracy: 4140.0/4200 (99%)\n",
      "drop: Loss: 0.0373\tAccuracy: 4147.0/4200 (99%)\n",
      "bndrop: Loss: 0.0353\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0349\tAccuracy: 4145.0/4200 (99%)\n",
      "\n",
      "LR: 4e-05\n",
      "Train Epoch: 25 [0/37800 (0%)]\tLosses bn: 0.003938 drop: 0.008451 bndrop: 0.001688 plain: 0.004524\n",
      "Train Epoch: 25 [6400/37800 (1%)]\tLosses bn: 0.000200 drop: 0.006386 bndrop: 0.001863 plain: 0.002061\n",
      "Train Epoch: 25 [12800/37800 (1%)]\tLosses bn: 0.000125 drop: 0.003394 bndrop: 0.000748 plain: 0.003984\n",
      "Train Epoch: 25 [19200/37800 (2%)]\tLosses bn: 0.000876 drop: 0.001487 bndrop: 0.001409 plain: 0.000578\n",
      "Train Epoch: 25 [25600/37800 (2%)]\tLosses bn: 0.003355 drop: 0.004750 bndrop: 0.005514 plain: 0.013509\n",
      "Train Epoch: 25 [32000/37800 (3%)]\tLosses bn: 0.001330 drop: 0.011519 bndrop: 0.020740 plain: 0.002990\n",
      "Train Epoch: 25 [37792/37800 (3%)]\tLosses bn: 0.001439 drop: 0.002032 bndrop: 0.000443 plain: 0.004363\n",
      "Test set:\n",
      "bn: Loss: 0.0430\tAccuracy: 4136.0/4200 (98%)\n",
      "drop: Loss: 0.0402\tAccuracy: 4142.0/4200 (99%)\n",
      "bndrop: Loss: 0.0413\tAccuracy: 4133.0/4200 (98%)\n",
      "plain: Loss: 0.0398\tAccuracy: 4141.0/4200 (99%)\n",
      "\n",
      "LR: 3.968253968253968e-05\n",
      "Train Epoch: 26 [0/37800 (0%)]\tLosses bn: 0.004278 drop: 0.017500 bndrop: 0.014122 plain: 0.009272\n",
      "Train Epoch: 26 [6400/37800 (1%)]\tLosses bn: 0.004266 drop: 0.025331 bndrop: 0.101033 plain: 0.020303\n",
      "Train Epoch: 26 [12800/37800 (1%)]\tLosses bn: 0.000230 drop: 0.002323 bndrop: 0.007223 plain: 0.018982\n",
      "Train Epoch: 26 [19200/37800 (2%)]\tLosses bn: 0.000333 drop: 0.023015 bndrop: 0.005096 plain: 0.006659\n",
      "Train Epoch: 26 [25600/37800 (2%)]\tLosses bn: 0.036598 drop: 0.021618 bndrop: 0.029382 plain: 0.039255\n",
      "Train Epoch: 26 [32000/37800 (3%)]\tLosses bn: 0.000350 drop: 0.001169 bndrop: 0.000463 plain: 0.000968\n",
      "Train Epoch: 26 [37792/37800 (3%)]\tLosses bn: 0.000128 drop: 0.004467 bndrop: 0.001425 plain: 0.001033\n",
      "Test set:\n",
      "bn: Loss: 0.0336\tAccuracy: 4147.0/4200 (99%)\n",
      "drop: Loss: 0.0368\tAccuracy: 4144.0/4200 (99%)\n",
      "bndrop: Loss: 0.0376\tAccuracy: 4141.0/4200 (99%)\n",
      "plain: Loss: 0.0353\tAccuracy: 4144.0/4200 (99%)\n",
      "\n",
      "LR: 3.9370078740157485e-05\n",
      "Train Epoch: 27 [0/37800 (0%)]\tLosses bn: 0.001683 drop: 0.003208 bndrop: 0.010185 plain: 0.004021\n",
      "Train Epoch: 27 [6400/37800 (1%)]\tLosses bn: 0.008565 drop: 0.012781 bndrop: 0.013274 plain: 0.006163\n",
      "Train Epoch: 27 [12800/37800 (1%)]\tLosses bn: 0.000851 drop: 0.009181 bndrop: 0.004121 plain: 0.013190\n",
      "Train Epoch: 27 [19200/37800 (2%)]\tLosses bn: 0.000829 drop: 0.001013 bndrop: 0.000538 plain: 0.000317\n",
      "Train Epoch: 27 [25600/37800 (2%)]\tLosses bn: 0.000172 drop: 0.002949 bndrop: 0.003782 plain: 0.000435\n",
      "Train Epoch: 27 [32000/37800 (3%)]\tLosses bn: 0.002690 drop: 0.003113 bndrop: 0.003606 plain: 0.020749\n",
      "Train Epoch: 27 [37792/37800 (3%)]\tLosses bn: 0.001244 drop: 0.012827 bndrop: 0.004498 plain: 0.002169\n",
      "Test set:\n",
      "bn: Loss: 0.0394\tAccuracy: 4139.0/4200 (99%)\n",
      "drop: Loss: 0.0362\tAccuracy: 4146.0/4200 (99%)\n",
      "bndrop: Loss: 0.0442\tAccuracy: 4130.0/4200 (98%)\n",
      "plain: Loss: 0.0331\tAccuracy: 4147.0/4200 (99%)\n",
      "\n",
      "LR: 3.90625e-05\n",
      "Train Epoch: 28 [0/37800 (0%)]\tLosses bn: 0.011957 drop: 0.004369 bndrop: 0.015939 plain: 0.008046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 28 [6400/37800 (1%)]\tLosses bn: 0.001368 drop: 0.009481 bndrop: 0.000679 plain: 0.008027\n",
      "Train Epoch: 28 [12800/37800 (1%)]\tLosses bn: 0.035717 drop: 0.005933 bndrop: 0.004136 plain: 0.007172\n",
      "Train Epoch: 28 [19200/37800 (2%)]\tLosses bn: 0.006809 drop: 0.013646 bndrop: 0.005620 plain: 0.015892\n",
      "Train Epoch: 28 [25600/37800 (2%)]\tLosses bn: 0.000573 drop: 0.021228 bndrop: 0.008987 plain: 0.023004\n",
      "Train Epoch: 28 [32000/37800 (3%)]\tLosses bn: 0.000871 drop: 0.008439 bndrop: 0.006668 plain: 0.002149\n",
      "Train Epoch: 28 [37792/37800 (3%)]\tLosses bn: 0.003892 drop: 0.007972 bndrop: 0.023261 plain: 0.001805\n",
      "Test set:\n",
      "bn: Loss: 0.0423\tAccuracy: 4135.0/4200 (98%)\n",
      "drop: Loss: 0.0344\tAccuracy: 4150.0/4200 (99%)\n",
      "bndrop: Loss: 0.0473\tAccuracy: 4132.0/4200 (98%)\n",
      "plain: Loss: 0.0366\tAccuracy: 4142.0/4200 (99%)\n",
      "\n",
      "LR: 3.875968992248062e-05\n",
      "Train Epoch: 29 [0/37800 (0%)]\tLosses bn: 0.000180 drop: 0.005245 bndrop: 0.015898 plain: 0.004073\n",
      "Train Epoch: 29 [6400/37800 (1%)]\tLosses bn: 0.000247 drop: 0.002457 bndrop: 0.001782 plain: 0.001568\n",
      "Train Epoch: 29 [12800/37800 (1%)]\tLosses bn: 0.000314 drop: 0.001084 bndrop: 0.000951 plain: 0.001314\n",
      "Train Epoch: 29 [19200/37800 (2%)]\tLosses bn: 0.000279 drop: 0.001100 bndrop: 0.000762 plain: 0.000536\n",
      "Train Epoch: 29 [25600/37800 (2%)]\tLosses bn: 0.001964 drop: 0.008432 bndrop: 0.008205 plain: 0.005479\n",
      "Train Epoch: 29 [32000/37800 (3%)]\tLosses bn: 0.001725 drop: 0.001025 bndrop: 0.001828 plain: 0.001708\n",
      "Train Epoch: 29 [37792/37800 (3%)]\tLosses bn: 0.002312 drop: 0.008392 bndrop: 0.001152 plain: 0.000661\n",
      "Test set:\n",
      "bn: Loss: 0.0359\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0415\tAccuracy: 4141.0/4200 (99%)\n",
      "bndrop: Loss: 0.0442\tAccuracy: 4139.0/4200 (99%)\n",
      "plain: Loss: 0.0360\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "LR: 3.846153846153846e-05\n",
      "Train Epoch: 30 [0/37800 (0%)]\tLosses bn: 0.006671 drop: 0.098412 bndrop: 0.053048 plain: 0.030512\n",
      "Train Epoch: 30 [6400/37800 (1%)]\tLosses bn: 0.001071 drop: 0.001253 bndrop: 0.002969 plain: 0.000287\n",
      "Train Epoch: 30 [12800/37800 (1%)]\tLosses bn: 0.000117 drop: 0.000265 bndrop: 0.000310 plain: 0.000609\n",
      "Train Epoch: 30 [19200/37800 (2%)]\tLosses bn: 0.000819 drop: 0.019441 bndrop: 0.003006 plain: 0.013300\n",
      "Train Epoch: 30 [25600/37800 (2%)]\tLosses bn: 0.018424 drop: 0.020639 bndrop: 0.024004 plain: 0.007287\n",
      "Train Epoch: 30 [32000/37800 (3%)]\tLosses bn: 0.002540 drop: 0.010501 bndrop: 0.021919 plain: 0.012336\n",
      "Train Epoch: 30 [37792/37800 (3%)]\tLosses bn: 0.006996 drop: 0.069337 bndrop: 0.040615 plain: 0.032191\n",
      "Test set:\n",
      "bn: Loss: 0.0348\tAccuracy: 4150.0/4200 (99%)\n",
      "drop: Loss: 0.0384\tAccuracy: 4146.0/4200 (99%)\n",
      "bndrop: Loss: 0.0396\tAccuracy: 4141.0/4200 (99%)\n",
      "plain: Loss: 0.0366\tAccuracy: 4142.0/4200 (99%)\n",
      "\n",
      "LR: 3.816793893129771e-05\n",
      "Train Epoch: 31 [0/37800 (0%)]\tLosses bn: 0.001405 drop: 0.004503 bndrop: 0.011693 plain: 0.002088\n",
      "Train Epoch: 31 [6400/37800 (1%)]\tLosses bn: 0.004479 drop: 0.075071 bndrop: 0.022335 plain: 0.019987\n",
      "Train Epoch: 31 [12800/37800 (1%)]\tLosses bn: 0.000592 drop: 0.059055 bndrop: 0.005411 plain: 0.023619\n",
      "Train Epoch: 31 [19200/37800 (2%)]\tLosses bn: 0.000159 drop: 0.003393 bndrop: 0.001311 plain: 0.002057\n",
      "Train Epoch: 31 [25600/37800 (2%)]\tLosses bn: 0.006124 drop: 0.007076 bndrop: 0.015418 plain: 0.021006\n",
      "Train Epoch: 31 [32000/37800 (3%)]\tLosses bn: 0.080424 drop: 0.054709 bndrop: 0.047922 plain: 0.036710\n",
      "Train Epoch: 31 [37792/37800 (3%)]\tLosses bn: 0.000184 drop: 0.003572 bndrop: 0.002142 plain: 0.000836\n",
      "Test set:\n",
      "bn: Loss: 0.0337\tAccuracy: 4149.0/4200 (99%)\n",
      "drop: Loss: 0.0381\tAccuracy: 4141.0/4200 (99%)\n",
      "bndrop: Loss: 0.0349\tAccuracy: 4145.0/4200 (99%)\n",
      "plain: Loss: 0.0348\tAccuracy: 4146.0/4200 (99%)\n",
      "\n",
      "LR: 3.787878787878788e-05\n",
      "Train Epoch: 32 [0/37800 (0%)]\tLosses bn: 0.001042 drop: 0.054336 bndrop: 0.002024 plain: 0.019770\n",
      "Train Epoch: 32 [6400/37800 (1%)]\tLosses bn: 0.003835 drop: 0.003787 bndrop: 0.029119 plain: 0.011736\n",
      "Train Epoch: 32 [12800/37800 (1%)]\tLosses bn: 0.001922 drop: 0.004507 bndrop: 0.000649 plain: 0.000263\n",
      "Train Epoch: 32 [19200/37800 (2%)]\tLosses bn: 0.000300 drop: 0.000686 bndrop: 0.000527 plain: 0.000461\n",
      "Train Epoch: 32 [25600/37800 (2%)]\tLosses bn: 0.001386 drop: 0.005084 bndrop: 0.009442 plain: 0.004359\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-65e1429df624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-36c9f48051a7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, models, log)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learning/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 101):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    train_x, train_y = shuffle_data(train_x, train_y)    \n",
    "    train(epoch, models, train_log)\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    test(models, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(test_log, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(test_log, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('./test.csv', delimiter=',')\n",
    "data = np.delete(data, (0), axis=0)\n",
    "data_np_x = data / 255\n",
    "\n",
    "testing_x = torch.Tensor(np.expand_dims(normalize(data_np_x), axis=1))\n",
    "testing_y = torch.LongTensor(np.zeros((testing_x.shape[0], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (testing_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "answers = np.empty((0, 10))\n",
    "model = models['drop']\n",
    "print (model)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(loader(testing_x, testing_y)):\n",
    "    output = model(data)  \n",
    "    output_np = output.detach().numpy()        \n",
    "    answers = np.vstack((answers, output_np))\n",
    "        \n",
    "print (answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.argmax(answers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (answers[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (vals[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission2.txt\", \"w\") as fout:\n",
    "    fout.write(\"ImageId,Label\\n\")\n",
    "    for i, val in enumerate(vals):\n",
    "        fout.write(\"{},{}\\n\".format(str(i+1), str(val)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
