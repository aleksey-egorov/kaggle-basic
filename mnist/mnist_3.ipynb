{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import PIL\n",
    "\n",
    "#from utils import *\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y: [1. 0. 1. 4. 0. 0. 7. 3. 5. 3.]\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt('./train.csv', delimiter=',')\n",
    "data = np.delete(data, (0), axis=0)\n",
    "data_np_y = data[:,0]\n",
    "data = np.delete(data, (0), axis=1)\n",
    "data_np_x = data / 255\n",
    "\n",
    "\n",
    "pos = int(data_np_x.shape[0] * 0.9)\n",
    "\n",
    "train_np_y = data_np_y[:pos]\n",
    "train_np_x = data_np_x[:pos]\n",
    "test_np_y = data_np_y[pos:]\n",
    "test_np_x = data_np_x[pos:]\n",
    "\n",
    "print (\"Y: {}\".format(train_np_y[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784) (42000,)\n",
      "(37800, 784) (37800,)\n",
      "(4200, 784) (4200,)\n"
     ]
    }
   ],
   "source": [
    "print (data_np_x.shape, data_np_y.shape)\n",
    "print (train_np_x.shape, train_np_y.shape)\n",
    "print (test_np_x.shape, test_np_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 784) (4200,)\n"
     ]
    }
   ],
   "source": [
    "print (test_np_x.shape, test_np_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 4. 0. 0. 7. 3. 5. 3.]\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,10), dpi=80)\n",
    "shape = (1,10)\n",
    "for j in range(10):\n",
    "    ax = fig.add_subplot(shape[0], shape[1], j+1)\n",
    "    image = train_np_x[j].reshape(28,28)\n",
    "    ax.matshow(image, cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "plt.show()\n",
    "\n",
    "print (train_np_y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x = (x - 0.1307) / 0.3081    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.Tensor(np.expand_dims(normalize(train_np_x), axis=1))\n",
    "train_y = torch.LongTensor(train_np_y)\n",
    "\n",
    "test_x = torch.Tensor(np.expand_dims(normalize(test_np_x), axis=1))\n",
    "test_y = torch.LongTensor(test_np_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(x, y, batch_size=32):\n",
    "    for i in range(0, x.size(0) - 1, batch_size):\n",
    "        data = x[i:i+batch_size]\n",
    "        if data.shape[0] == batch_size:\n",
    "            data = data.reshape(batch_size, 1, 28, 28)\n",
    "            targets = y[i:i+batch_size]\n",
    "\n",
    "            yield data, targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
    "                 bn=False, dropout=False, activation_fn=nn.ReLU()):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        layers = []        \n",
    "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding))\n",
    "        if pool_layer is not None:\n",
    "            layers.append(pool_layer)\n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm2d(size[1]))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout2d())\n",
    "        layers.append(activation_fn)\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout())\n",
    "            layers.append(activation_fn())\n",
    "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
    "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, batchnorm=False, dropout=False, lr=5e-5, l2=0.):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "        self._conv1 = ConvLayer([1, 32, 3], bn=batchnorm)\n",
    "        self._conv2 = ConvLayer([32, 64, 3], bn=batchnorm, activation_fn=nn.Sigmoid())\n",
    "        \n",
    "        self.fc = FullyConnected([64*7*7, 512, 10], dropout=dropout)\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.lr_base = lr\n",
    "        self.l2 = l2\n",
    "        self._loss = None\n",
    "        self.optim = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.l2)\n",
    "    \n",
    "    def conv(self, x):\n",
    "        x = self._conv1(x)\n",
    "        x = self._conv2(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):        \n",
    "        self._loss = F.cross_entropy(output, target, **kwargs)\n",
    "        self._correct = output.data.max(1, keepdim=True)[1]\n",
    "        self._correct = self._correct.eq(target.data.view_as(self._correct)).to(torch.float).cpu().mean()\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, y):\n",
    "    r = np.random.permutation(len(y))\n",
    "    return X[r], y[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, models, log=None):    \n",
    "    for model in models.values():\n",
    "        model.lr = model.lr_base * (100 / (epoch + 100))        \n",
    "        model.optim = optim.Adam(model.parameters(), lr=model.lr, weight_decay=model.l2)\n",
    "    print (\"LR: {}\".format(models['bn'].lr))    \n",
    "    train_size = len(train_x)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(loader(train_x, train_y)):        \n",
    "        for model in models.values():                             \n",
    "            model.optim.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = model.loss(output, target)\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "            \n",
    "        if batch_idx % 200 == 0:\n",
    "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / train_size)\n",
    "            losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "            print(line + losses)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / train_size)\n",
    "        losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "        if log is not None:\n",
    "            for k in models:\n",
    "                log[k].append((models[k]._loss, models[k]._correct))\n",
    "        print(line + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, log=None):\n",
    "    test_size = len(test_x)\n",
    "    avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
    "    acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, test_size, p)\n",
    "    line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
    "\n",
    "    test_loss = {k: 0. for k in models}\n",
    "    correct = {k: 0. for k in models}\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader(test_x, test_y):\n",
    "            output = {k: m(data) for k, m in models.items()}           \n",
    "            for k, m in models.items():     \n",
    "                #print (output[k].shape, target.shape)\n",
    "                test_loss[k] += m.loss(output[k], target, size_average=False).item() # sum up batch loss\n",
    "                pred = output[k].data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "                correct[k] += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "    \n",
    "    for k in models:\n",
    "        test_loss[k] /= test_size\n",
    "    correct_pct = {k: c / test_size for k, c in correct.items()}\n",
    "    lines = '\\n'.join([line(k, test_loss[k], correct[k], 100*correct_pct[k]) for k in models]) + '\\n'\n",
    "    report = 'Test set:\\n' + lines\n",
    "    if log is not None:\n",
    "        for k in models:\n",
    "            log[k].append((test_loss[k], correct_pct[k]))\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(log, tpe='loss'):\n",
    "    keys = log.keys()\n",
    "    logs = {k:[z for z in zip(*log[k])] for k in keys}\n",
    "    epochs = {k:range(len(log[k])) for k in keys}\n",
    " \n",
    "    \n",
    "    if tpe == 'loss':\n",
    "        handlers, = zip(*[plt.plot(epochs[k], logs[k][0], label=k) for k in keys])\n",
    "        plt.title('errors')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('error')\n",
    "        plt.legend(handles=handlers)\n",
    "        plt.show()\n",
    "    elif tpe == 'accuracy':\n",
    "        handlers, = zip(*[plt.plot(epochs[k], logs[k][1], label=k) for k in log.keys()])\n",
    "        plt.title('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.legend(handles=handlers)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'bn': Net(True), 'drop': Net(False, True), 'bndrop': Net(True, True), 'plain': Net()}\n",
    "train_log = {k: [] for k in models}\n",
    "test_log = {k: [] for k in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 4.950495049504951e-05\n",
      "Train Epoch: 1 [0/37800 (0%)]\tLosses bn: 0.279402 drop: 0.743752 bndrop: 0.445788 plain: 0.524532\n",
      "Train Epoch: 1 [6400/37800 (1%)]\tLosses bn: 0.144145 drop: 0.583169 bndrop: 0.278166 plain: 0.441812\n",
      "Train Epoch: 1 [12800/37800 (1%)]\tLosses bn: 0.182827 drop: 0.556349 bndrop: 0.314393 plain: 0.438191\n",
      "Train Epoch: 1 [19200/37800 (2%)]\tLosses bn: 0.063010 drop: 0.299776 bndrop: 0.125368 plain: 0.250972\n",
      "Train Epoch: 1 [25600/37800 (2%)]\tLosses bn: 0.126764 drop: 0.282950 bndrop: 0.178589 plain: 0.227756\n",
      "Train Epoch: 1 [32000/37800 (3%)]\tLosses bn: 0.070147 drop: 0.220338 bndrop: 0.098213 plain: 0.191837\n",
      "Train Epoch: 1 [37792/37800 (3%)]\tLosses bn: 0.128282 drop: 0.185848 bndrop: 0.135452 plain: 0.157860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksey/.conda/envs/otus-dl-engineer/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "bn: Loss: 0.1229\tAccuracy: 4039.0/4200 (96%)\n",
      "drop: Loss: 0.2457\tAccuracy: 3894.0/4200 (93%)\n",
      "bndrop: Loss: 0.1188\tAccuracy: 4043.0/4200 (96%)\n",
      "plain: Loss: 0.2469\tAccuracy: 3901.0/4200 (93%)\n",
      "\n",
      "LR: 4.901960784313725e-05\n",
      "Train Epoch: 2 [0/37800 (0%)]\tLosses bn: 0.247675 drop: 0.476248 bndrop: 0.313107 plain: 0.468122\n",
      "Train Epoch: 2 [6400/37800 (1%)]\tLosses bn: 0.180749 drop: 0.284805 bndrop: 0.162781 plain: 0.306842\n",
      "Train Epoch: 2 [12800/37800 (1%)]\tLosses bn: 0.144295 drop: 0.201367 bndrop: 0.165109 plain: 0.213361\n",
      "Train Epoch: 2 [19200/37800 (2%)]\tLosses bn: 0.117942 drop: 0.175705 bndrop: 0.140865 plain: 0.138639\n",
      "Train Epoch: 2 [25600/37800 (2%)]\tLosses bn: 0.125536 drop: 0.351492 bndrop: 0.135902 plain: 0.359160\n",
      "Train Epoch: 2 [32000/37800 (3%)]\tLosses bn: 0.050321 drop: 0.172743 bndrop: 0.061005 plain: 0.135896\n",
      "Train Epoch: 2 [37792/37800 (3%)]\tLosses bn: 0.095079 drop: 0.207128 bndrop: 0.147944 plain: 0.172952\n",
      "Test set:\n",
      "bn: Loss: 0.0767\tAccuracy: 4100.0/4200 (98%)\n",
      "drop: Loss: 0.1590\tAccuracy: 3984.0/4200 (95%)\n",
      "bndrop: Loss: 0.0785\tAccuracy: 4087.0/4200 (97%)\n",
      "plain: Loss: 0.1584\tAccuracy: 3994.0/4200 (95%)\n",
      "\n",
      "LR: 4.854368932038835e-05\n",
      "Train Epoch: 3 [0/37800 (0%)]\tLosses bn: 0.035379 drop: 0.096322 bndrop: 0.041669 plain: 0.084710\n",
      "Train Epoch: 3 [6400/37800 (1%)]\tLosses bn: 0.022110 drop: 0.068044 bndrop: 0.038589 plain: 0.092595\n",
      "Train Epoch: 3 [12800/37800 (1%)]\tLosses bn: 0.096956 drop: 0.148930 bndrop: 0.107893 plain: 0.104261\n",
      "Train Epoch: 3 [19200/37800 (2%)]\tLosses bn: 0.132777 drop: 0.136563 bndrop: 0.088362 plain: 0.107642\n",
      "Train Epoch: 3 [25600/37800 (2%)]\tLosses bn: 0.041872 drop: 0.148248 bndrop: 0.072053 plain: 0.120686\n",
      "Train Epoch: 3 [32000/37800 (3%)]\tLosses bn: 0.035717 drop: 0.056376 bndrop: 0.039152 plain: 0.045415\n",
      "Train Epoch: 3 [37792/37800 (3%)]\tLosses bn: 0.007076 drop: 0.061087 bndrop: 0.026883 plain: 0.035143\n",
      "Test set:\n",
      "bn: Loss: 0.0670\tAccuracy: 4110.0/4200 (98%)\n",
      "drop: Loss: 0.1231\tAccuracy: 4023.0/4200 (96%)\n",
      "bndrop: Loss: 0.0685\tAccuracy: 4096.0/4200 (98%)\n",
      "plain: Loss: 0.1293\tAccuracy: 4019.0/4200 (96%)\n",
      "\n",
      "LR: 4.8076923076923084e-05\n",
      "Train Epoch: 4 [0/37800 (0%)]\tLosses bn: 0.049596 drop: 0.196647 bndrop: 0.071334 plain: 0.115302\n",
      "Train Epoch: 4 [6400/37800 (1%)]\tLosses bn: 0.039174 drop: 0.036067 bndrop: 0.026793 plain: 0.026995\n",
      "Train Epoch: 4 [12800/37800 (1%)]\tLosses bn: 0.127175 drop: 0.104082 bndrop: 0.104105 plain: 0.123981\n",
      "Train Epoch: 4 [19200/37800 (2%)]\tLosses bn: 0.548919 drop: 0.495416 bndrop: 0.442105 plain: 0.600421\n",
      "Train Epoch: 4 [25600/37800 (2%)]\tLosses bn: 0.029068 drop: 0.147795 bndrop: 0.042547 plain: 0.120757\n",
      "Train Epoch: 4 [32000/37800 (3%)]\tLosses bn: 0.015960 drop: 0.017178 bndrop: 0.023144 plain: 0.016599\n",
      "Train Epoch: 4 [37792/37800 (3%)]\tLosses bn: 0.004691 drop: 0.024034 bndrop: 0.006262 plain: 0.018545\n",
      "Test set:\n",
      "bn: Loss: 0.0622\tAccuracy: 4105.0/4200 (98%)\n",
      "drop: Loss: 0.0996\tAccuracy: 4055.0/4200 (97%)\n",
      "bndrop: Loss: 0.0602\tAccuracy: 4102.0/4200 (98%)\n",
      "plain: Loss: 0.1063\tAccuracy: 4044.0/4200 (96%)\n",
      "\n",
      "LR: 4.761904761904762e-05\n",
      "Train Epoch: 5 [0/37800 (0%)]\tLosses bn: 0.049963 drop: 0.117604 bndrop: 0.101725 plain: 0.103585\n",
      "Train Epoch: 5 [6400/37800 (1%)]\tLosses bn: 0.035039 drop: 0.096337 bndrop: 0.049642 plain: 0.098565\n",
      "Train Epoch: 5 [12800/37800 (1%)]\tLosses bn: 0.020186 drop: 0.188505 bndrop: 0.046541 plain: 0.144160\n",
      "Train Epoch: 5 [19200/37800 (2%)]\tLosses bn: 0.077426 drop: 0.165074 bndrop: 0.109318 plain: 0.151301\n",
      "Train Epoch: 5 [25600/37800 (2%)]\tLosses bn: 0.125568 drop: 0.117184 bndrop: 0.141071 plain: 0.137932\n",
      "Train Epoch: 5 [32000/37800 (3%)]\tLosses bn: 0.019080 drop: 0.034682 bndrop: 0.014475 plain: 0.038073\n",
      "Train Epoch: 5 [37792/37800 (3%)]\tLosses bn: 0.012977 drop: 0.063544 bndrop: 0.061490 plain: 0.056640\n",
      "Test set:\n",
      "bn: Loss: 0.0517\tAccuracy: 4119.0/4200 (98%)\n",
      "drop: Loss: 0.0844\tAccuracy: 4079.0/4200 (97%)\n",
      "bndrop: Loss: 0.0492\tAccuracy: 4120.0/4200 (98%)\n",
      "plain: Loss: 0.0842\tAccuracy: 4074.0/4200 (97%)\n",
      "\n",
      "LR: 4.716981132075472e-05\n",
      "Train Epoch: 6 [0/37800 (0%)]\tLosses bn: 0.003788 drop: 0.028275 bndrop: 0.013006 plain: 0.014068\n",
      "Train Epoch: 6 [6400/37800 (1%)]\tLosses bn: 0.037668 drop: 0.043986 bndrop: 0.040418 plain: 0.066800\n",
      "Train Epoch: 6 [12800/37800 (1%)]\tLosses bn: 0.135359 drop: 0.158845 bndrop: 0.107949 plain: 0.190442\n",
      "Train Epoch: 6 [19200/37800 (2%)]\tLosses bn: 0.012451 drop: 0.025438 bndrop: 0.017867 plain: 0.033201\n",
      "Train Epoch: 6 [25600/37800 (2%)]\tLosses bn: 0.099582 drop: 0.119950 bndrop: 0.034103 plain: 0.204861\n",
      "Train Epoch: 6 [32000/37800 (3%)]\tLosses bn: 0.017756 drop: 0.113692 bndrop: 0.034572 plain: 0.099105\n",
      "Train Epoch: 6 [37792/37800 (3%)]\tLosses bn: 0.003586 drop: 0.039498 bndrop: 0.008512 plain: 0.031682\n",
      "Test set:\n",
      "bn: Loss: 0.0491\tAccuracy: 4118.0/4200 (98%)\n",
      "drop: Loss: 0.0708\tAccuracy: 4092.0/4200 (97%)\n",
      "bndrop: Loss: 0.0514\tAccuracy: 4118.0/4200 (98%)\n",
      "plain: Loss: 0.0729\tAccuracy: 4092.0/4200 (97%)\n",
      "\n",
      "LR: 4.672897196261683e-05\n",
      "Train Epoch: 7 [0/37800 (0%)]\tLosses bn: 0.006501 drop: 0.034066 bndrop: 0.013671 plain: 0.027586\n",
      "Train Epoch: 7 [6400/37800 (1%)]\tLosses bn: 0.076708 drop: 0.235340 bndrop: 0.138610 plain: 0.196947\n",
      "Train Epoch: 7 [12800/37800 (1%)]\tLosses bn: 0.090104 drop: 0.082753 bndrop: 0.070038 plain: 0.073446\n",
      "Train Epoch: 7 [19200/37800 (2%)]\tLosses bn: 0.061093 drop: 0.078559 bndrop: 0.062086 plain: 0.069827\n",
      "Train Epoch: 7 [25600/37800 (2%)]\tLosses bn: 0.030031 drop: 0.123495 bndrop: 0.076111 plain: 0.167003\n",
      "Train Epoch: 7 [32000/37800 (3%)]\tLosses bn: 0.002466 drop: 0.017001 bndrop: 0.011357 plain: 0.012966\n",
      "Train Epoch: 7 [37792/37800 (3%)]\tLosses bn: 0.006009 drop: 0.003817 bndrop: 0.004374 plain: 0.002799\n",
      "Test set:\n",
      "bn: Loss: 0.0583\tAccuracy: 4103.0/4200 (98%)\n",
      "drop: Loss: 0.0657\tAccuracy: 4092.0/4200 (97%)\n",
      "bndrop: Loss: 0.0501\tAccuracy: 4127.0/4200 (98%)\n",
      "plain: Loss: 0.0770\tAccuracy: 4085.0/4200 (97%)\n",
      "\n",
      "LR: 4.62962962962963e-05\n",
      "Train Epoch: 8 [0/37800 (0%)]\tLosses bn: 0.013920 drop: 0.060550 bndrop: 0.022846 plain: 0.046952\n",
      "Train Epoch: 8 [6400/37800 (1%)]\tLosses bn: 0.001551 drop: 0.005958 bndrop: 0.003740 plain: 0.002565\n",
      "Train Epoch: 8 [12800/37800 (1%)]\tLosses bn: 0.018351 drop: 0.011868 bndrop: 0.025558 plain: 0.015416\n",
      "Train Epoch: 8 [19200/37800 (2%)]\tLosses bn: 0.012100 drop: 0.049067 bndrop: 0.018899 plain: 0.051324\n",
      "Train Epoch: 8 [25600/37800 (2%)]\tLosses bn: 0.015733 drop: 0.026759 bndrop: 0.024579 plain: 0.055145\n",
      "Train Epoch: 8 [32000/37800 (3%)]\tLosses bn: 0.021105 drop: 0.127309 bndrop: 0.023745 plain: 0.069549\n",
      "Train Epoch: 8 [37792/37800 (3%)]\tLosses bn: 0.055528 drop: 0.024105 bndrop: 0.091548 plain: 0.022652\n",
      "Test set:\n",
      "bn: Loss: 0.0448\tAccuracy: 4127.0/4200 (98%)\n",
      "drop: Loss: 0.0595\tAccuracy: 4111.0/4200 (98%)\n",
      "bndrop: Loss: 0.0492\tAccuracy: 4124.0/4200 (98%)\n",
      "plain: Loss: 0.0588\tAccuracy: 4117.0/4200 (98%)\n",
      "\n",
      "LR: 4.587155963302753e-05\n",
      "Train Epoch: 9 [0/37800 (0%)]\tLosses bn: 0.018771 drop: 0.065682 bndrop: 0.032168 plain: 0.108108\n",
      "Train Epoch: 9 [6400/37800 (1%)]\tLosses bn: 0.017505 drop: 0.072534 bndrop: 0.031761 plain: 0.043809\n",
      "Train Epoch: 9 [12800/37800 (1%)]\tLosses bn: 0.008160 drop: 0.027019 bndrop: 0.012929 plain: 0.019906\n",
      "Train Epoch: 9 [19200/37800 (2%)]\tLosses bn: 0.010101 drop: 0.051125 bndrop: 0.067206 plain: 0.059189\n",
      "Train Epoch: 9 [25600/37800 (2%)]\tLosses bn: 0.020416 drop: 0.029180 bndrop: 0.030054 plain: 0.025828\n",
      "Train Epoch: 9 [32000/37800 (3%)]\tLosses bn: 0.070034 drop: 0.061786 bndrop: 0.068006 plain: 0.125977\n",
      "Train Epoch: 9 [37792/37800 (3%)]\tLosses bn: 0.002536 drop: 0.004765 bndrop: 0.006241 plain: 0.004014\n",
      "Test set:\n",
      "bn: Loss: 0.0391\tAccuracy: 4136.0/4200 (98%)\n",
      "drop: Loss: 0.0580\tAccuracy: 4111.0/4200 (98%)\n",
      "bndrop: Loss: 0.0390\tAccuracy: 4136.0/4200 (98%)\n",
      "plain: Loss: 0.0553\tAccuracy: 4118.0/4200 (98%)\n",
      "\n",
      "LR: 4.545454545454546e-05\n",
      "Train Epoch: 10 [0/37800 (0%)]\tLosses bn: 0.013876 drop: 0.048860 bndrop: 0.021619 plain: 0.050019\n",
      "Train Epoch: 10 [6400/37800 (1%)]\tLosses bn: 0.024967 drop: 0.054316 bndrop: 0.055177 plain: 0.031622\n",
      "Train Epoch: 10 [12800/37800 (1%)]\tLosses bn: 0.000926 drop: 0.004382 bndrop: 0.003743 plain: 0.002644\n",
      "Train Epoch: 10 [19200/37800 (2%)]\tLosses bn: 0.129449 drop: 0.150222 bndrop: 0.118658 plain: 0.102290\n",
      "Train Epoch: 10 [25600/37800 (2%)]\tLosses bn: 0.005137 drop: 0.016397 bndrop: 0.005333 plain: 0.004239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [32000/37800 (3%)]\tLosses bn: 0.010959 drop: 0.027825 bndrop: 0.018555 plain: 0.038023\n",
      "Train Epoch: 10 [37792/37800 (3%)]\tLosses bn: 0.001407 drop: 0.009236 bndrop: 0.004673 plain: 0.004597\n",
      "Test set:\n",
      "bn: Loss: 0.0406\tAccuracy: 4129.0/4200 (98%)\n",
      "drop: Loss: 0.0517\tAccuracy: 4119.0/4200 (98%)\n",
      "bndrop: Loss: 0.0395\tAccuracy: 4133.0/4200 (98%)\n",
      "plain: Loss: 0.0492\tAccuracy: 4127.0/4200 (98%)\n",
      "\n",
      "LR: 4.5045045045045046e-05\n",
      "Train Epoch: 11 [0/37800 (0%)]\tLosses bn: 0.018935 drop: 0.023792 bndrop: 0.028347 plain: 0.022891\n",
      "Train Epoch: 11 [6400/37800 (1%)]\tLosses bn: 0.012720 drop: 0.044605 bndrop: 0.009428 plain: 0.019480\n",
      "Train Epoch: 11 [12800/37800 (1%)]\tLosses bn: 0.033706 drop: 0.075966 bndrop: 0.075501 plain: 0.068025\n",
      "Train Epoch: 11 [19200/37800 (2%)]\tLosses bn: 0.001104 drop: 0.004352 bndrop: 0.002416 plain: 0.002428\n",
      "Train Epoch: 11 [25600/37800 (2%)]\tLosses bn: 0.003014 drop: 0.022706 bndrop: 0.009330 plain: 0.019654\n",
      "Train Epoch: 11 [32000/37800 (3%)]\tLosses bn: 0.001553 drop: 0.009011 bndrop: 0.003951 plain: 0.004589\n",
      "Train Epoch: 11 [37792/37800 (3%)]\tLosses bn: 0.059711 drop: 0.031601 bndrop: 0.020676 plain: 0.033016\n",
      "Test set:\n",
      "bn: Loss: 0.0494\tAccuracy: 4125.0/4200 (98%)\n",
      "drop: Loss: 0.0478\tAccuracy: 4123.0/4200 (98%)\n",
      "bndrop: Loss: 0.0369\tAccuracy: 4135.0/4200 (98%)\n",
      "plain: Loss: 0.0542\tAccuracy: 4112.0/4200 (98%)\n",
      "\n",
      "LR: 4.464285714285715e-05\n",
      "Train Epoch: 12 [0/37800 (0%)]\tLosses bn: 0.001196 drop: 0.003296 bndrop: 0.005105 plain: 0.002259\n",
      "Train Epoch: 12 [6400/37800 (1%)]\tLosses bn: 0.107458 drop: 0.149760 bndrop: 0.164557 plain: 0.123240\n",
      "Train Epoch: 12 [12800/37800 (1%)]\tLosses bn: 0.191780 drop: 0.114509 bndrop: 0.179281 plain: 0.149986\n",
      "Train Epoch: 12 [19200/37800 (2%)]\tLosses bn: 0.024983 drop: 0.034292 bndrop: 0.043977 plain: 0.027868\n",
      "Train Epoch: 12 [25600/37800 (2%)]\tLosses bn: 0.016803 drop: 0.020450 bndrop: 0.006240 plain: 0.016731\n",
      "Train Epoch: 12 [32000/37800 (3%)]\tLosses bn: 0.004773 drop: 0.039724 bndrop: 0.007450 plain: 0.018383\n",
      "Train Epoch: 12 [37792/37800 (3%)]\tLosses bn: 0.002113 drop: 0.009295 bndrop: 0.002442 plain: 0.003648\n",
      "Test set:\n",
      "bn: Loss: 0.0474\tAccuracy: 4128.0/4200 (98%)\n",
      "drop: Loss: 0.0454\tAccuracy: 4127.0/4200 (98%)\n",
      "bndrop: Loss: 0.0419\tAccuracy: 4136.0/4200 (98%)\n",
      "plain: Loss: 0.0478\tAccuracy: 4126.0/4200 (98%)\n",
      "\n",
      "LR: 4.4247787610619477e-05\n",
      "Train Epoch: 13 [0/37800 (0%)]\tLosses bn: 0.027693 drop: 0.062822 bndrop: 0.033346 plain: 0.032958\n",
      "Train Epoch: 13 [6400/37800 (1%)]\tLosses bn: 0.070581 drop: 0.132107 bndrop: 0.077330 plain: 0.096066\n",
      "Train Epoch: 13 [12800/37800 (1%)]\tLosses bn: 0.036343 drop: 0.041181 bndrop: 0.047424 plain: 0.033609\n",
      "Train Epoch: 13 [19200/37800 (2%)]\tLosses bn: 0.055543 drop: 0.008806 bndrop: 0.012399 plain: 0.017239\n",
      "Train Epoch: 13 [25600/37800 (2%)]\tLosses bn: 0.014756 drop: 0.017930 bndrop: 0.017200 plain: 0.039240\n",
      "Train Epoch: 13 [32000/37800 (3%)]\tLosses bn: 0.066576 drop: 0.032846 bndrop: 0.040758 plain: 0.051177\n",
      "Train Epoch: 13 [37792/37800 (3%)]\tLosses bn: 0.004062 drop: 0.020254 bndrop: 0.009878 plain: 0.013185\n",
      "Test set:\n",
      "bn: Loss: 0.0371\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0459\tAccuracy: 4128.0/4200 (98%)\n",
      "bndrop: Loss: 0.0378\tAccuracy: 4138.0/4200 (99%)\n",
      "plain: Loss: 0.0477\tAccuracy: 4122.0/4200 (98%)\n",
      "\n",
      "LR: 4.3859649122807014e-05\n",
      "Train Epoch: 14 [0/37800 (0%)]\tLosses bn: 0.001240 drop: 0.006250 bndrop: 0.004439 plain: 0.002696\n",
      "Train Epoch: 14 [6400/37800 (1%)]\tLosses bn: 0.002321 drop: 0.030118 bndrop: 0.008421 plain: 0.022032\n",
      "Train Epoch: 14 [12800/37800 (1%)]\tLosses bn: 0.011615 drop: 0.052233 bndrop: 0.045177 plain: 0.041615\n",
      "Train Epoch: 14 [19200/37800 (2%)]\tLosses bn: 0.021815 drop: 0.013392 bndrop: 0.024049 plain: 0.019804\n",
      "Train Epoch: 14 [25600/37800 (2%)]\tLosses bn: 0.003989 drop: 0.007086 bndrop: 0.014315 plain: 0.007898\n",
      "Train Epoch: 14 [32000/37800 (3%)]\tLosses bn: 0.015410 drop: 0.032945 bndrop: 0.026596 plain: 0.033799\n",
      "Train Epoch: 14 [37792/37800 (3%)]\tLosses bn: 0.041899 drop: 0.007419 bndrop: 0.006995 plain: 0.009274\n",
      "Test set:\n",
      "bn: Loss: 0.0365\tAccuracy: 4141.0/4200 (99%)\n",
      "drop: Loss: 0.0428\tAccuracy: 4127.0/4200 (98%)\n",
      "bndrop: Loss: 0.0336\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0426\tAccuracy: 4134.0/4200 (98%)\n",
      "\n",
      "LR: 4.347826086956522e-05\n",
      "Train Epoch: 15 [0/37800 (0%)]\tLosses bn: 0.003441 drop: 0.006992 bndrop: 0.006005 plain: 0.015976\n",
      "Train Epoch: 15 [6400/37800 (1%)]\tLosses bn: 0.053667 drop: 0.098551 bndrop: 0.045295 plain: 0.055164\n",
      "Train Epoch: 15 [12800/37800 (1%)]\tLosses bn: 0.006086 drop: 0.017438 bndrop: 0.016023 plain: 0.016028\n",
      "Train Epoch: 15 [19200/37800 (2%)]\tLosses bn: 0.087583 drop: 0.142198 bndrop: 0.076173 plain: 0.173673\n",
      "Train Epoch: 15 [25600/37800 (2%)]\tLosses bn: 0.001080 drop: 0.006161 bndrop: 0.003912 plain: 0.005781\n",
      "Train Epoch: 15 [32000/37800 (3%)]\tLosses bn: 0.024433 drop: 0.061564 bndrop: 0.036361 plain: 0.126403\n",
      "Train Epoch: 15 [37792/37800 (3%)]\tLosses bn: 0.010390 drop: 0.032728 bndrop: 0.017518 plain: 0.011367\n",
      "Test set:\n",
      "bn: Loss: 0.0431\tAccuracy: 4133.0/4200 (98%)\n",
      "drop: Loss: 0.0431\tAccuracy: 4132.0/4200 (98%)\n",
      "bndrop: Loss: 0.0376\tAccuracy: 4138.0/4200 (99%)\n",
      "plain: Loss: 0.0467\tAccuracy: 4128.0/4200 (98%)\n",
      "\n",
      "LR: 4.3103448275862066e-05\n",
      "Train Epoch: 16 [0/37800 (0%)]\tLosses bn: 0.019203 drop: 0.041371 bndrop: 0.031699 plain: 0.035275\n",
      "Train Epoch: 16 [6400/37800 (1%)]\tLosses bn: 0.002210 drop: 0.020580 bndrop: 0.016592 plain: 0.020065\n",
      "Train Epoch: 16 [12800/37800 (1%)]\tLosses bn: 0.010745 drop: 0.009174 bndrop: 0.062855 plain: 0.010186\n",
      "Train Epoch: 16 [19200/37800 (2%)]\tLosses bn: 0.012128 drop: 0.033871 bndrop: 0.025952 plain: 0.063585\n",
      "Train Epoch: 16 [25600/37800 (2%)]\tLosses bn: 0.009486 drop: 0.007533 bndrop: 0.010960 plain: 0.007437\n",
      "Train Epoch: 16 [32000/37800 (3%)]\tLosses bn: 0.008689 drop: 0.043625 bndrop: 0.025925 plain: 0.007669\n",
      "Train Epoch: 16 [37792/37800 (3%)]\tLosses bn: 0.025915 drop: 0.029651 bndrop: 0.009087 plain: 0.029535\n",
      "Test set:\n",
      "bn: Loss: 0.0346\tAccuracy: 4146.0/4200 (99%)\n",
      "drop: Loss: 0.0432\tAccuracy: 4129.0/4200 (98%)\n",
      "bndrop: Loss: 0.0368\tAccuracy: 4142.0/4200 (99%)\n",
      "plain: Loss: 0.0474\tAccuracy: 4124.0/4200 (98%)\n",
      "\n",
      "LR: 4.2735042735042735e-05\n",
      "Train Epoch: 17 [0/37800 (0%)]\tLosses bn: 0.000785 drop: 0.009950 bndrop: 0.009330 plain: 0.002631\n",
      "Train Epoch: 17 [6400/37800 (1%)]\tLosses bn: 0.000220 drop: 0.005191 bndrop: 0.001026 plain: 0.003640\n",
      "Train Epoch: 17 [12800/37800 (1%)]\tLosses bn: 0.001123 drop: 0.003149 bndrop: 0.003536 plain: 0.004061\n",
      "Train Epoch: 17 [19200/37800 (2%)]\tLosses bn: 0.004119 drop: 0.026132 bndrop: 0.008320 plain: 0.011449\n",
      "Train Epoch: 17 [25600/37800 (2%)]\tLosses bn: 0.067693 drop: 0.174886 bndrop: 0.104140 plain: 0.143339\n",
      "Train Epoch: 17 [32000/37800 (3%)]\tLosses bn: 0.002139 drop: 0.002392 bndrop: 0.005243 plain: 0.001742\n",
      "Train Epoch: 17 [37792/37800 (3%)]\tLosses bn: 0.017677 drop: 0.034042 bndrop: 0.052179 plain: 0.028767\n",
      "Test set:\n",
      "bn: Loss: 0.0447\tAccuracy: 4132.0/4200 (98%)\n",
      "drop: Loss: 0.0485\tAccuracy: 4122.0/4200 (98%)\n",
      "bndrop: Loss: 0.0454\tAccuracy: 4135.0/4200 (98%)\n",
      "plain: Loss: 0.0469\tAccuracy: 4119.0/4200 (98%)\n",
      "\n",
      "LR: 4.2372881355932206e-05\n",
      "Train Epoch: 18 [0/37800 (0%)]\tLosses bn: 0.003675 drop: 0.025775 bndrop: 0.007058 plain: 0.008490\n",
      "Train Epoch: 18 [6400/37800 (1%)]\tLosses bn: 0.006274 drop: 0.130260 bndrop: 0.052168 plain: 0.038560\n",
      "Train Epoch: 18 [12800/37800 (1%)]\tLosses bn: 0.019603 drop: 0.070748 bndrop: 0.047586 plain: 0.044646\n",
      "Train Epoch: 18 [19200/37800 (2%)]\tLosses bn: 0.066485 drop: 0.014238 bndrop: 0.014216 plain: 0.025231\n",
      "Train Epoch: 18 [25600/37800 (2%)]\tLosses bn: 0.003592 drop: 0.004790 bndrop: 0.011616 plain: 0.006573\n",
      "Train Epoch: 18 [32000/37800 (3%)]\tLosses bn: 0.019003 drop: 0.035533 bndrop: 0.004149 plain: 0.007945\n",
      "Train Epoch: 18 [37792/37800 (3%)]\tLosses bn: 0.000439 drop: 0.002067 bndrop: 0.000618 plain: 0.000958\n",
      "Test set:\n",
      "bn: Loss: 0.0358\tAccuracy: 4139.0/4200 (99%)\n",
      "drop: Loss: 0.0427\tAccuracy: 4133.0/4200 (98%)\n",
      "bndrop: Loss: 0.0350\tAccuracy: 4147.0/4200 (99%)\n",
      "plain: Loss: 0.0408\tAccuracy: 4135.0/4200 (98%)\n",
      "\n",
      "LR: 4.201680672268908e-05\n",
      "Train Epoch: 19 [0/37800 (0%)]\tLosses bn: 0.001121 drop: 0.002257 bndrop: 0.001201 plain: 0.001014\n",
      "Train Epoch: 19 [6400/37800 (1%)]\tLosses bn: 0.000740 drop: 0.003915 bndrop: 0.003776 plain: 0.001859\n",
      "Train Epoch: 19 [12800/37800 (1%)]\tLosses bn: 0.078028 drop: 0.002721 bndrop: 0.001483 plain: 0.002155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19 [19200/37800 (2%)]\tLosses bn: 0.018955 drop: 0.070695 bndrop: 0.010008 plain: 0.009390\n",
      "Train Epoch: 19 [25600/37800 (2%)]\tLosses bn: 0.004384 drop: 0.133742 bndrop: 0.028563 plain: 0.052396\n",
      "Train Epoch: 19 [32000/37800 (3%)]\tLosses bn: 0.007454 drop: 0.028785 bndrop: 0.004896 plain: 0.009328\n",
      "Train Epoch: 19 [37792/37800 (3%)]\tLosses bn: 0.010465 drop: 0.005529 bndrop: 0.022784 plain: 0.031198\n",
      "Test set:\n",
      "bn: Loss: 0.0398\tAccuracy: 4139.0/4200 (99%)\n",
      "drop: Loss: 0.0420\tAccuracy: 4135.0/4200 (98%)\n",
      "bndrop: Loss: 0.0385\tAccuracy: 4136.0/4200 (98%)\n",
      "plain: Loss: 0.0399\tAccuracy: 4136.0/4200 (98%)\n",
      "\n",
      "LR: 4.166666666666667e-05\n",
      "Train Epoch: 20 [0/37800 (0%)]\tLosses bn: 0.001880 drop: 0.001622 bndrop: 0.001503 plain: 0.000672\n",
      "Train Epoch: 20 [6400/37800 (1%)]\tLosses bn: 0.002555 drop: 0.030386 bndrop: 0.015662 plain: 0.007667\n",
      "Train Epoch: 20 [12800/37800 (1%)]\tLosses bn: 0.001546 drop: 0.005919 bndrop: 0.002207 plain: 0.003718\n",
      "Train Epoch: 20 [19200/37800 (2%)]\tLosses bn: 0.010103 drop: 0.013919 bndrop: 0.009363 plain: 0.013104\n",
      "Train Epoch: 20 [25600/37800 (2%)]\tLosses bn: 0.000718 drop: 0.006618 bndrop: 0.003558 plain: 0.003447\n",
      "Train Epoch: 20 [32000/37800 (3%)]\tLosses bn: 0.005380 drop: 0.049005 bndrop: 0.032187 plain: 0.013381\n",
      "Train Epoch: 20 [37792/37800 (3%)]\tLosses bn: 0.023526 drop: 0.098315 bndrop: 0.020095 plain: 0.112206\n",
      "Test set:\n",
      "bn: Loss: 0.0441\tAccuracy: 4132.0/4200 (98%)\n",
      "drop: Loss: 0.0389\tAccuracy: 4142.0/4200 (99%)\n",
      "bndrop: Loss: 0.0380\tAccuracy: 4147.0/4200 (99%)\n",
      "plain: Loss: 0.0408\tAccuracy: 4138.0/4200 (99%)\n",
      "\n",
      "LR: 4.132231404958678e-05\n",
      "Train Epoch: 21 [0/37800 (0%)]\tLosses bn: 0.001577 drop: 0.039277 bndrop: 0.007947 plain: 0.012671\n",
      "Train Epoch: 21 [6400/37800 (1%)]\tLosses bn: 0.003087 drop: 0.076775 bndrop: 0.004449 plain: 0.017662\n",
      "Train Epoch: 21 [12800/37800 (1%)]\tLosses bn: 0.013291 drop: 0.075371 bndrop: 0.039027 plain: 0.055310\n",
      "Train Epoch: 21 [19200/37800 (2%)]\tLosses bn: 0.042875 drop: 0.003981 bndrop: 0.009842 plain: 0.001476\n",
      "Train Epoch: 21 [25600/37800 (2%)]\tLosses bn: 0.003462 drop: 0.009553 bndrop: 0.001894 plain: 0.001313\n",
      "Train Epoch: 21 [32000/37800 (3%)]\tLosses bn: 0.003212 drop: 0.010322 bndrop: 0.017895 plain: 0.006300\n",
      "Train Epoch: 21 [37792/37800 (3%)]\tLosses bn: 0.005021 drop: 0.031726 bndrop: 0.007233 plain: 0.054768\n",
      "Test set:\n",
      "bn: Loss: 0.0379\tAccuracy: 4132.0/4200 (98%)\n",
      "drop: Loss: 0.0425\tAccuracy: 4136.0/4200 (98%)\n",
      "bndrop: Loss: 0.0408\tAccuracy: 4135.0/4200 (98%)\n",
      "plain: Loss: 0.0398\tAccuracy: 4140.0/4200 (99%)\n",
      "\n",
      "LR: 4.098360655737705e-05\n",
      "Train Epoch: 22 [0/37800 (0%)]\tLosses bn: 0.003851 drop: 0.017694 bndrop: 0.005463 plain: 0.014911\n",
      "Train Epoch: 22 [6400/37800 (1%)]\tLosses bn: 0.000206 drop: 0.003720 bndrop: 0.000751 plain: 0.002290\n",
      "Train Epoch: 22 [12800/37800 (1%)]\tLosses bn: 0.022018 drop: 0.046093 bndrop: 0.015906 plain: 0.032855\n",
      "Train Epoch: 22 [19200/37800 (2%)]\tLosses bn: 0.000510 drop: 0.005060 bndrop: 0.001213 plain: 0.005379\n",
      "Train Epoch: 22 [25600/37800 (2%)]\tLosses bn: 0.001964 drop: 0.013150 bndrop: 0.004100 plain: 0.007174\n",
      "Train Epoch: 22 [32000/37800 (3%)]\tLosses bn: 0.001729 drop: 0.004553 bndrop: 0.002886 plain: 0.008048\n",
      "Train Epoch: 22 [37792/37800 (3%)]\tLosses bn: 0.001577 drop: 0.091404 bndrop: 0.095011 plain: 0.087927\n",
      "Test set:\n",
      "bn: Loss: 0.0370\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0377\tAccuracy: 4139.0/4200 (99%)\n",
      "bndrop: Loss: 0.0347\tAccuracy: 4146.0/4200 (99%)\n",
      "plain: Loss: 0.0371\tAccuracy: 4136.0/4200 (98%)\n",
      "\n",
      "LR: 4.065040650406504e-05\n",
      "Train Epoch: 23 [0/37800 (0%)]\tLosses bn: 0.001849 drop: 0.001027 bndrop: 0.008047 plain: 0.003580\n",
      "Train Epoch: 23 [6400/37800 (1%)]\tLosses bn: 0.003103 drop: 0.006186 bndrop: 0.002591 plain: 0.005557\n",
      "Train Epoch: 23 [12800/37800 (1%)]\tLosses bn: 0.000091 drop: 0.001748 bndrop: 0.000705 plain: 0.000954\n",
      "Train Epoch: 23 [19200/37800 (2%)]\tLosses bn: 0.026619 drop: 0.017145 bndrop: 0.003080 plain: 0.004399\n",
      "Train Epoch: 23 [25600/37800 (2%)]\tLosses bn: 0.000663 drop: 0.012386 bndrop: 0.000615 plain: 0.002342\n",
      "Train Epoch: 23 [32000/37800 (3%)]\tLosses bn: 0.003210 drop: 0.006180 bndrop: 0.003645 plain: 0.007709\n",
      "Train Epoch: 23 [37792/37800 (3%)]\tLosses bn: 0.001053 drop: 0.001734 bndrop: 0.003301 plain: 0.000521\n",
      "Test set:\n",
      "bn: Loss: 0.0368\tAccuracy: 4139.0/4200 (99%)\n",
      "drop: Loss: 0.0416\tAccuracy: 4139.0/4200 (99%)\n",
      "bndrop: Loss: 0.0533\tAccuracy: 4132.0/4200 (98%)\n",
      "plain: Loss: 0.0472\tAccuracy: 4125.0/4200 (98%)\n",
      "\n",
      "LR: 4.032258064516129e-05\n",
      "Train Epoch: 24 [0/37800 (0%)]\tLosses bn: 0.000329 drop: 0.002461 bndrop: 0.010218 plain: 0.002579\n",
      "Train Epoch: 24 [6400/37800 (1%)]\tLosses bn: 0.009602 drop: 0.034497 bndrop: 0.031201 plain: 0.028103\n",
      "Train Epoch: 24 [12800/37800 (1%)]\tLosses bn: 0.001920 drop: 0.007508 bndrop: 0.002658 plain: 0.005820\n",
      "Train Epoch: 24 [19200/37800 (2%)]\tLosses bn: 0.000657 drop: 0.009236 bndrop: 0.000981 plain: 0.001341\n",
      "Train Epoch: 24 [25600/37800 (2%)]\tLosses bn: 0.000276 drop: 0.004616 bndrop: 0.005738 plain: 0.002291\n",
      "Train Epoch: 24 [32000/37800 (3%)]\tLosses bn: 0.000484 drop: 0.096844 bndrop: 0.011823 plain: 0.011769\n",
      "Train Epoch: 24 [37792/37800 (3%)]\tLosses bn: 0.000181 drop: 0.001522 bndrop: 0.002623 plain: 0.000435\n",
      "Test set:\n",
      "bn: Loss: 0.0391\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0400\tAccuracy: 4142.0/4200 (99%)\n",
      "bndrop: Loss: 0.0443\tAccuracy: 4138.0/4200 (99%)\n",
      "plain: Loss: 0.0389\tAccuracy: 4139.0/4200 (99%)\n",
      "\n",
      "LR: 4e-05\n",
      "Train Epoch: 25 [0/37800 (0%)]\tLosses bn: 0.000668 drop: 0.018965 bndrop: 0.003631 plain: 0.013681\n",
      "Train Epoch: 25 [6400/37800 (1%)]\tLosses bn: 0.001011 drop: 0.009693 bndrop: 0.008677 plain: 0.013678\n",
      "Train Epoch: 25 [12800/37800 (1%)]\tLosses bn: 0.000998 drop: 0.008435 bndrop: 0.001365 plain: 0.001907\n",
      "Train Epoch: 25 [19200/37800 (2%)]\tLosses bn: 0.000452 drop: 0.001549 bndrop: 0.001265 plain: 0.001169\n",
      "Train Epoch: 25 [25600/37800 (2%)]\tLosses bn: 0.000763 drop: 0.034109 bndrop: 0.006222 plain: 0.004025\n",
      "Train Epoch: 25 [32000/37800 (3%)]\tLosses bn: 0.006830 drop: 0.065237 bndrop: 0.080188 plain: 0.064677\n",
      "Train Epoch: 25 [37792/37800 (3%)]\tLosses bn: 0.000058 drop: 0.001662 bndrop: 0.000636 plain: 0.000920\n",
      "Test set:\n",
      "bn: Loss: 0.0388\tAccuracy: 4140.0/4200 (99%)\n",
      "drop: Loss: 0.0377\tAccuracy: 4141.0/4200 (99%)\n",
      "bndrop: Loss: 0.0378\tAccuracy: 4142.0/4200 (99%)\n",
      "plain: Loss: 0.0368\tAccuracy: 4136.0/4200 (98%)\n",
      "\n",
      "LR: 3.968253968253968e-05\n",
      "Train Epoch: 26 [0/37800 (0%)]\tLosses bn: 0.000654 drop: 0.001089 bndrop: 0.002785 plain: 0.000690\n",
      "Train Epoch: 26 [6400/37800 (1%)]\tLosses bn: 0.003954 drop: 0.012151 bndrop: 0.002900 plain: 0.006623\n",
      "Train Epoch: 26 [12800/37800 (1%)]\tLosses bn: 0.000516 drop: 0.011507 bndrop: 0.005723 plain: 0.005151\n",
      "Train Epoch: 26 [19200/37800 (2%)]\tLosses bn: 0.000352 drop: 0.010086 bndrop: 0.000620 plain: 0.003420\n",
      "Train Epoch: 26 [25600/37800 (2%)]\tLosses bn: 0.001637 drop: 0.103054 bndrop: 0.036566 plain: 0.050589\n",
      "Train Epoch: 26 [32000/37800 (3%)]\tLosses bn: 0.004043 drop: 0.003329 bndrop: 0.001939 plain: 0.000975\n",
      "Train Epoch: 26 [37792/37800 (3%)]\tLosses bn: 0.000538 drop: 0.001989 bndrop: 0.005544 plain: 0.000831\n",
      "Test set:\n",
      "bn: Loss: 0.0430\tAccuracy: 4135.0/4200 (98%)\n",
      "drop: Loss: 0.0379\tAccuracy: 4143.0/4200 (99%)\n",
      "bndrop: Loss: 0.0370\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0381\tAccuracy: 4147.0/4200 (99%)\n",
      "\n",
      "LR: 3.9370078740157485e-05\n",
      "Train Epoch: 27 [0/37800 (0%)]\tLosses bn: 0.005644 drop: 0.003376 bndrop: 0.003077 plain: 0.001666\n",
      "Train Epoch: 27 [6400/37800 (1%)]\tLosses bn: 0.001140 drop: 0.075299 bndrop: 0.001675 plain: 0.019551\n",
      "Train Epoch: 27 [12800/37800 (1%)]\tLosses bn: 0.000308 drop: 0.004386 bndrop: 0.002150 plain: 0.004374\n",
      "Train Epoch: 27 [19200/37800 (2%)]\tLosses bn: 0.013053 drop: 0.048024 bndrop: 0.010667 plain: 0.031281\n",
      "Train Epoch: 27 [25600/37800 (2%)]\tLosses bn: 0.001030 drop: 0.002529 bndrop: 0.001708 plain: 0.003028\n",
      "Train Epoch: 27 [32000/37800 (3%)]\tLosses bn: 0.000408 drop: 0.001539 bndrop: 0.001664 plain: 0.001317\n",
      "Train Epoch: 27 [37792/37800 (3%)]\tLosses bn: 0.001687 drop: 0.008490 bndrop: 0.026765 plain: 0.019501\n",
      "Test set:\n",
      "bn: Loss: 0.0508\tAccuracy: 4126.0/4200 (98%)\n",
      "drop: Loss: 0.0404\tAccuracy: 4140.0/4200 (99%)\n",
      "bndrop: Loss: 0.0378\tAccuracy: 4144.0/4200 (99%)\n",
      "plain: Loss: 0.0365\tAccuracy: 4146.0/4200 (99%)\n",
      "\n",
      "LR: 3.90625e-05\n",
      "Train Epoch: 28 [0/37800 (0%)]\tLosses bn: 0.000990 drop: 0.000422 bndrop: 0.000478 plain: 0.000205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 28 [6400/37800 (1%)]\tLosses bn: 0.002387 drop: 0.044704 bndrop: 0.006881 plain: 0.019514\n",
      "Train Epoch: 28 [12800/37800 (1%)]\tLosses bn: 0.000386 drop: 0.001997 bndrop: 0.001571 plain: 0.005575\n",
      "Train Epoch: 28 [19200/37800 (2%)]\tLosses bn: 0.003313 drop: 0.023882 bndrop: 0.052360 plain: 0.008314\n",
      "Train Epoch: 28 [25600/37800 (2%)]\tLosses bn: 0.001133 drop: 0.009568 bndrop: 0.002661 plain: 0.002652\n",
      "Train Epoch: 28 [32000/37800 (3%)]\tLosses bn: 0.001445 drop: 0.005650 bndrop: 0.010440 plain: 0.004457\n",
      "Train Epoch: 28 [37792/37800 (3%)]\tLosses bn: 0.000604 drop: 0.003259 bndrop: 0.001275 plain: 0.000797\n",
      "Test set:\n",
      "bn: Loss: 0.0407\tAccuracy: 4139.0/4200 (99%)\n",
      "drop: Loss: 0.0380\tAccuracy: 4150.0/4200 (99%)\n",
      "bndrop: Loss: 0.0381\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0387\tAccuracy: 4141.0/4200 (99%)\n",
      "\n",
      "LR: 3.875968992248062e-05\n",
      "Train Epoch: 29 [0/37800 (0%)]\tLosses bn: 0.001363 drop: 0.003565 bndrop: 0.001114 plain: 0.001214\n",
      "Train Epoch: 29 [6400/37800 (1%)]\tLosses bn: 0.000379 drop: 0.007812 bndrop: 0.000729 plain: 0.003519\n",
      "Train Epoch: 29 [12800/37800 (1%)]\tLosses bn: 0.002667 drop: 0.092101 bndrop: 0.014809 plain: 0.016274\n",
      "Train Epoch: 29 [19200/37800 (2%)]\tLosses bn: 0.001698 drop: 0.042652 bndrop: 0.003397 plain: 0.008113\n",
      "Train Epoch: 29 [25600/37800 (2%)]\tLosses bn: 0.000981 drop: 0.006703 bndrop: 0.002998 plain: 0.007250\n",
      "Train Epoch: 29 [32000/37800 (3%)]\tLosses bn: 0.000036 drop: 0.008289 bndrop: 0.000392 plain: 0.001634\n",
      "Train Epoch: 29 [37792/37800 (3%)]\tLosses bn: 0.002696 drop: 0.008425 bndrop: 0.002619 plain: 0.002767\n",
      "Test set:\n",
      "bn: Loss: 0.0431\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0392\tAccuracy: 4147.0/4200 (99%)\n",
      "bndrop: Loss: 0.0374\tAccuracy: 4145.0/4200 (99%)\n",
      "plain: Loss: 0.0468\tAccuracy: 4126.0/4200 (98%)\n",
      "\n",
      "LR: 3.846153846153846e-05\n",
      "Train Epoch: 30 [0/37800 (0%)]\tLosses bn: 0.000916 drop: 0.016624 bndrop: 0.006585 plain: 0.008368\n",
      "Train Epoch: 30 [6400/37800 (1%)]\tLosses bn: 0.000937 drop: 0.000858 bndrop: 0.025587 plain: 0.004738\n",
      "Train Epoch: 30 [12800/37800 (1%)]\tLosses bn: 0.000571 drop: 0.015586 bndrop: 0.002446 plain: 0.016522\n",
      "Train Epoch: 30 [19200/37800 (2%)]\tLosses bn: 0.001010 drop: 0.014820 bndrop: 0.007035 plain: 0.009198\n",
      "Train Epoch: 30 [25600/37800 (2%)]\tLosses bn: 0.000080 drop: 0.000591 bndrop: 0.000573 plain: 0.000703\n",
      "Train Epoch: 30 [32000/37800 (3%)]\tLosses bn: 0.008802 drop: 0.008761 bndrop: 0.005594 plain: 0.004262\n",
      "Train Epoch: 30 [37792/37800 (3%)]\tLosses bn: 0.001290 drop: 0.004387 bndrop: 0.001247 plain: 0.002529\n",
      "Test set:\n",
      "bn: Loss: 0.0373\tAccuracy: 4142.0/4200 (99%)\n",
      "drop: Loss: 0.0403\tAccuracy: 4146.0/4200 (99%)\n",
      "bndrop: Loss: 0.0361\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0384\tAccuracy: 4142.0/4200 (99%)\n",
      "\n",
      "LR: 3.816793893129771e-05\n",
      "Train Epoch: 31 [0/37800 (0%)]\tLosses bn: 0.000086 drop: 0.002079 bndrop: 0.002041 plain: 0.001085\n",
      "Train Epoch: 31 [6400/37800 (1%)]\tLosses bn: 0.002194 drop: 0.045902 bndrop: 0.010044 plain: 0.022091\n",
      "Train Epoch: 31 [12800/37800 (1%)]\tLosses bn: 0.003734 drop: 0.008683 bndrop: 0.006559 plain: 0.003479\n",
      "Train Epoch: 31 [19200/37800 (2%)]\tLosses bn: 0.018183 drop: 0.012992 bndrop: 0.000394 plain: 0.012439\n",
      "Train Epoch: 31 [25600/37800 (2%)]\tLosses bn: 0.002566 drop: 0.002495 bndrop: 0.014425 plain: 0.004213\n",
      "Train Epoch: 31 [32000/37800 (3%)]\tLosses bn: 0.000491 drop: 0.006618 bndrop: 0.001903 plain: 0.003277\n",
      "Train Epoch: 31 [37792/37800 (3%)]\tLosses bn: 0.000145 drop: 0.003967 bndrop: 0.001387 plain: 0.000873\n",
      "Test set:\n",
      "bn: Loss: 0.0333\tAccuracy: 4147.0/4200 (99%)\n",
      "drop: Loss: 0.0378\tAccuracy: 4146.0/4200 (99%)\n",
      "bndrop: Loss: 0.0375\tAccuracy: 4144.0/4200 (99%)\n",
      "plain: Loss: 0.0354\tAccuracy: 4146.0/4200 (99%)\n",
      "\n",
      "LR: 3.787878787878788e-05\n",
      "Train Epoch: 32 [0/37800 (0%)]\tLosses bn: 0.000199 drop: 0.005945 bndrop: 0.008894 plain: 0.005171\n",
      "Train Epoch: 32 [6400/37800 (1%)]\tLosses bn: 0.000033 drop: 0.002435 bndrop: 0.000934 plain: 0.000632\n",
      "Train Epoch: 32 [12800/37800 (1%)]\tLosses bn: 0.000259 drop: 0.000945 bndrop: 0.000888 plain: 0.001021\n",
      "Train Epoch: 32 [19200/37800 (2%)]\tLosses bn: 0.003950 drop: 0.013482 bndrop: 0.021391 plain: 0.007920\n",
      "Train Epoch: 32 [25600/37800 (2%)]\tLosses bn: 0.001784 drop: 0.004800 bndrop: 0.005476 plain: 0.002296\n",
      "Train Epoch: 32 [32000/37800 (3%)]\tLosses bn: 0.004109 drop: 0.006669 bndrop: 0.003584 plain: 0.002773\n",
      "Train Epoch: 32 [37792/37800 (3%)]\tLosses bn: 0.007025 drop: 0.086221 bndrop: 0.027020 plain: 0.075411\n",
      "Test set:\n",
      "bn: Loss: 0.0337\tAccuracy: 4151.0/4200 (99%)\n",
      "drop: Loss: 0.0395\tAccuracy: 4146.0/4200 (99%)\n",
      "bndrop: Loss: 0.0414\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0352\tAccuracy: 4146.0/4200 (99%)\n",
      "\n",
      "LR: 3.759398496240601e-05\n",
      "Train Epoch: 33 [0/37800 (0%)]\tLosses bn: 0.000303 drop: 0.003280 bndrop: 0.001683 plain: 0.001189\n",
      "Train Epoch: 33 [6400/37800 (1%)]\tLosses bn: 0.000066 drop: 0.019564 bndrop: 0.000500 plain: 0.005590\n",
      "Train Epoch: 33 [12800/37800 (1%)]\tLosses bn: 0.000274 drop: 0.000990 bndrop: 0.001291 plain: 0.000658\n",
      "Train Epoch: 33 [19200/37800 (2%)]\tLosses bn: 0.000274 drop: 0.001321 bndrop: 0.002278 plain: 0.000377\n",
      "Train Epoch: 33 [25600/37800 (2%)]\tLosses bn: 0.000098 drop: 0.001249 bndrop: 0.000265 plain: 0.001927\n",
      "Train Epoch: 33 [32000/37800 (3%)]\tLosses bn: 0.001939 drop: 0.017796 bndrop: 0.003960 plain: 0.035584\n",
      "Train Epoch: 33 [37792/37800 (3%)]\tLosses bn: 0.000045 drop: 0.000973 bndrop: 0.000461 plain: 0.003490\n",
      "Test set:\n",
      "bn: Loss: 0.0343\tAccuracy: 4148.0/4200 (99%)\n",
      "drop: Loss: 0.0382\tAccuracy: 4150.0/4200 (99%)\n",
      "bndrop: Loss: 0.0382\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0344\tAccuracy: 4146.0/4200 (99%)\n",
      "\n",
      "LR: 3.73134328358209e-05\n",
      "Train Epoch: 34 [0/37800 (0%)]\tLosses bn: 0.000648 drop: 0.003389 bndrop: 0.001490 plain: 0.004986\n",
      "Train Epoch: 34 [6400/37800 (1%)]\tLosses bn: 0.001200 drop: 0.004300 bndrop: 0.000561 plain: 0.002073\n",
      "Train Epoch: 34 [12800/37800 (1%)]\tLosses bn: 0.000046 drop: 0.000658 bndrop: 0.000378 plain: 0.000209\n",
      "Train Epoch: 34 [19200/37800 (2%)]\tLosses bn: 0.009083 drop: 0.011653 bndrop: 0.010832 plain: 0.011538\n",
      "Train Epoch: 34 [25600/37800 (2%)]\tLosses bn: 0.000090 drop: 0.000909 bndrop: 0.000144 plain: 0.000782\n",
      "Train Epoch: 34 [32000/37800 (3%)]\tLosses bn: 0.001856 drop: 0.021409 bndrop: 0.004354 plain: 0.032256\n",
      "Train Epoch: 34 [37792/37800 (3%)]\tLosses bn: 0.014173 drop: 0.000623 bndrop: 0.005176 plain: 0.001000\n",
      "Test set:\n",
      "bn: Loss: 0.0408\tAccuracy: 4143.0/4200 (99%)\n",
      "drop: Loss: 0.0385\tAccuracy: 4147.0/4200 (99%)\n",
      "bndrop: Loss: 0.0440\tAccuracy: 4139.0/4200 (99%)\n",
      "plain: Loss: 0.0375\tAccuracy: 4142.0/4200 (99%)\n",
      "\n",
      "LR: 3.7037037037037037e-05\n",
      "Train Epoch: 35 [0/37800 (0%)]\tLosses bn: 0.000446 drop: 0.001709 bndrop: 0.000338 plain: 0.000215\n",
      "Train Epoch: 35 [6400/37800 (1%)]\tLosses bn: 0.007523 drop: 0.009799 bndrop: 0.017189 plain: 0.008386\n",
      "Train Epoch: 35 [12800/37800 (1%)]\tLosses bn: 0.000224 drop: 0.001050 bndrop: 0.000620 plain: 0.000954\n",
      "Train Epoch: 35 [19200/37800 (2%)]\tLosses bn: 0.003653 drop: 0.064341 bndrop: 0.008197 plain: 0.014799\n",
      "Train Epoch: 35 [25600/37800 (2%)]\tLosses bn: 0.000009 drop: 0.000873 bndrop: 0.000132 plain: 0.000226\n",
      "Train Epoch: 35 [32000/37800 (3%)]\tLosses bn: 0.000157 drop: 0.003245 bndrop: 0.004008 plain: 0.001102\n",
      "Train Epoch: 35 [37792/37800 (3%)]\tLosses bn: 0.000243 drop: 0.025198 bndrop: 0.003502 plain: 0.008224\n",
      "Test set:\n",
      "bn: Loss: 0.0325\tAccuracy: 4146.0/4200 (99%)\n",
      "drop: Loss: 0.0382\tAccuracy: 4145.0/4200 (99%)\n",
      "bndrop: Loss: 0.0370\tAccuracy: 4151.0/4200 (99%)\n",
      "plain: Loss: 0.0329\tAccuracy: 4147.0/4200 (99%)\n",
      "\n",
      "LR: 3.6764705882352945e-05\n",
      "Train Epoch: 36 [0/37800 (0%)]\tLosses bn: 0.001480 drop: 0.036011 bndrop: 0.007992 plain: 0.022224\n",
      "Train Epoch: 36 [6400/37800 (1%)]\tLosses bn: 0.000745 drop: 0.002170 bndrop: 0.000403 plain: 0.000878\n",
      "Train Epoch: 36 [12800/37800 (1%)]\tLosses bn: 0.000723 drop: 0.000616 bndrop: 0.000272 plain: 0.001489\n",
      "Train Epoch: 36 [19200/37800 (2%)]\tLosses bn: 0.001824 drop: 0.002847 bndrop: 0.021797 plain: 0.003635\n",
      "Train Epoch: 36 [25600/37800 (2%)]\tLosses bn: 0.002688 drop: 0.123014 bndrop: 0.002829 plain: 0.013459\n",
      "Train Epoch: 36 [32000/37800 (3%)]\tLosses bn: 0.007945 drop: 0.010923 bndrop: 0.016358 plain: 0.032782\n",
      "Train Epoch: 36 [37792/37800 (3%)]\tLosses bn: 0.000041 drop: 0.001859 bndrop: 0.000448 plain: 0.000677\n",
      "Test set:\n",
      "bn: Loss: 0.0372\tAccuracy: 4148.0/4200 (99%)\n",
      "drop: Loss: 0.0393\tAccuracy: 4147.0/4200 (99%)\n",
      "bndrop: Loss: 0.0394\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0366\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "LR: 3.649635036496351e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 37 [0/37800 (0%)]\tLosses bn: 0.009075 drop: 0.039631 bndrop: 0.004059 plain: 0.010534\n",
      "Train Epoch: 37 [6400/37800 (1%)]\tLosses bn: 0.000088 drop: 0.001526 bndrop: 0.000591 plain: 0.000246\n",
      "Train Epoch: 37 [12800/37800 (1%)]\tLosses bn: 0.000048 drop: 0.007473 bndrop: 0.000364 plain: 0.000714\n",
      "Train Epoch: 37 [19200/37800 (2%)]\tLosses bn: 0.000393 drop: 0.002074 bndrop: 0.001338 plain: 0.001772\n",
      "Train Epoch: 37 [25600/37800 (2%)]\tLosses bn: 0.000055 drop: 0.001989 bndrop: 0.000106 plain: 0.000379\n",
      "Train Epoch: 37 [32000/37800 (3%)]\tLosses bn: 0.000387 drop: 0.009759 bndrop: 0.001823 plain: 0.009241\n",
      "Train Epoch: 37 [37792/37800 (3%)]\tLosses bn: 0.000042 drop: 0.001760 bndrop: 0.003351 plain: 0.000811\n",
      "Test set:\n",
      "bn: Loss: 0.0359\tAccuracy: 4150.0/4200 (99%)\n",
      "drop: Loss: 0.0374\tAccuracy: 4149.0/4200 (99%)\n",
      "bndrop: Loss: 0.0461\tAccuracy: 4144.0/4200 (99%)\n",
      "plain: Loss: 0.0389\tAccuracy: 4144.0/4200 (99%)\n",
      "\n",
      "LR: 3.6231884057971014e-05\n",
      "Train Epoch: 38 [0/37800 (0%)]\tLosses bn: 0.000700 drop: 0.002304 bndrop: 0.001386 plain: 0.008367\n",
      "Train Epoch: 38 [6400/37800 (1%)]\tLosses bn: 0.000104 drop: 0.000812 bndrop: 0.000108 plain: 0.002168\n",
      "Train Epoch: 38 [12800/37800 (1%)]\tLosses bn: 0.000070 drop: 0.004007 bndrop: 0.000573 plain: 0.001758\n",
      "Train Epoch: 38 [19200/37800 (2%)]\tLosses bn: 0.000333 drop: 0.001456 bndrop: 0.001231 plain: 0.001326\n",
      "Train Epoch: 38 [25600/37800 (2%)]\tLosses bn: 0.000777 drop: 0.001781 bndrop: 0.004420 plain: 0.002278\n",
      "Train Epoch: 38 [32000/37800 (3%)]\tLosses bn: 0.001596 drop: 0.018495 bndrop: 0.000371 plain: 0.004370\n",
      "Train Epoch: 38 [37792/37800 (3%)]\tLosses bn: 0.000418 drop: 0.007644 bndrop: 0.004462 plain: 0.002315\n",
      "Test set:\n",
      "bn: Loss: 0.0368\tAccuracy: 4146.0/4200 (99%)\n",
      "drop: Loss: 0.0420\tAccuracy: 4142.0/4200 (99%)\n",
      "bndrop: Loss: 0.0487\tAccuracy: 4145.0/4200 (99%)\n",
      "plain: Loss: 0.0373\tAccuracy: 4141.0/4200 (99%)\n",
      "\n",
      "LR: 3.597122302158273e-05\n",
      "Train Epoch: 39 [0/37800 (0%)]\tLosses bn: 0.000083 drop: 0.000802 bndrop: 0.000575 plain: 0.000139\n",
      "Train Epoch: 39 [6400/37800 (1%)]\tLosses bn: 0.000097 drop: 0.003433 bndrop: 0.001074 plain: 0.000805\n",
      "Train Epoch: 39 [12800/37800 (1%)]\tLosses bn: 0.000298 drop: 0.001121 bndrop: 0.000256 plain: 0.000797\n",
      "Train Epoch: 39 [19200/37800 (2%)]\tLosses bn: 0.000402 drop: 0.008981 bndrop: 0.001918 plain: 0.000415\n",
      "Train Epoch: 39 [25600/37800 (2%)]\tLosses bn: 0.000304 drop: 0.001062 bndrop: 0.002114 plain: 0.002286\n",
      "Train Epoch: 39 [32000/37800 (3%)]\tLosses bn: 0.000026 drop: 0.011681 bndrop: 0.000499 plain: 0.003478\n",
      "Train Epoch: 39 [37792/37800 (3%)]\tLosses bn: 0.009454 drop: 0.043052 bndrop: 0.000585 plain: 0.003007\n",
      "Test set:\n",
      "bn: Loss: 0.0939\tAccuracy: 4090.0/4200 (97%)\n",
      "drop: Loss: 0.0388\tAccuracy: 4143.0/4200 (99%)\n",
      "bndrop: Loss: 0.0452\tAccuracy: 4146.0/4200 (99%)\n",
      "plain: Loss: 0.0386\tAccuracy: 4143.0/4200 (99%)\n",
      "\n",
      "LR: 3.571428571428572e-05\n",
      "Train Epoch: 40 [0/37800 (0%)]\tLosses bn: 0.002489 drop: 0.002094 bndrop: 0.008015 plain: 0.001850\n",
      "Train Epoch: 40 [6400/37800 (1%)]\tLosses bn: 0.005829 drop: 0.023160 bndrop: 0.001705 plain: 0.016647\n",
      "Train Epoch: 40 [12800/37800 (1%)]\tLosses bn: 0.000059 drop: 0.002574 bndrop: 0.000286 plain: 0.000760\n",
      "Train Epoch: 40 [19200/37800 (2%)]\tLosses bn: 0.000024 drop: 0.003043 bndrop: 0.003267 plain: 0.000227\n",
      "Train Epoch: 40 [25600/37800 (2%)]\tLosses bn: 0.000336 drop: 0.026219 bndrop: 0.013038 plain: 0.003861\n",
      "Train Epoch: 40 [32000/37800 (3%)]\tLosses bn: 0.000272 drop: 0.006453 bndrop: 0.003492 plain: 0.008405\n",
      "Train Epoch: 40 [37792/37800 (3%)]\tLosses bn: 0.000233 drop: 0.009216 bndrop: 0.002437 plain: 0.013742\n",
      "Test set:\n",
      "bn: Loss: 0.0463\tAccuracy: 4145.0/4200 (99%)\n",
      "drop: Loss: 0.0391\tAccuracy: 4150.0/4200 (99%)\n",
      "bndrop: Loss: 0.0402\tAccuracy: 4149.0/4200 (99%)\n",
      "plain: Loss: 0.0368\tAccuracy: 4152.0/4200 (99%)\n",
      "\n",
      "LR: 3.546099290780142e-05\n",
      "Train Epoch: 41 [0/37800 (0%)]\tLosses bn: 0.000291 drop: 0.000713 bndrop: 0.002593 plain: 0.000758\n",
      "Train Epoch: 41 [6400/37800 (1%)]\tLosses bn: 0.000403 drop: 0.001047 bndrop: 0.001044 plain: 0.000611\n",
      "Train Epoch: 41 [12800/37800 (1%)]\tLosses bn: 0.003286 drop: 0.013609 bndrop: 0.002513 plain: 0.005194\n",
      "Train Epoch: 41 [19200/37800 (2%)]\tLosses bn: 0.000535 drop: 0.010668 bndrop: 0.000927 plain: 0.002295\n",
      "Train Epoch: 41 [25600/37800 (2%)]\tLosses bn: 0.000936 drop: 0.027707 bndrop: 0.001153 plain: 0.004839\n",
      "Train Epoch: 41 [32000/37800 (3%)]\tLosses bn: 0.001026 drop: 0.003780 bndrop: 0.002792 plain: 0.004398\n",
      "Train Epoch: 41 [37792/37800 (3%)]\tLosses bn: 0.001672 drop: 0.004742 bndrop: 0.000762 plain: 0.001798\n",
      "Test set:\n",
      "bn: Loss: 0.0377\tAccuracy: 4147.0/4200 (99%)\n",
      "drop: Loss: 0.0424\tAccuracy: 4147.0/4200 (99%)\n",
      "bndrop: Loss: 0.0442\tAccuracy: 4147.0/4200 (99%)\n",
      "plain: Loss: 0.0360\tAccuracy: 4146.0/4200 (99%)\n",
      "\n",
      "LR: 3.5211267605633805e-05\n",
      "Train Epoch: 42 [0/37800 (0%)]\tLosses bn: 0.000033 drop: 0.002931 bndrop: 0.000585 plain: 0.000513\n",
      "Train Epoch: 42 [6400/37800 (1%)]\tLosses bn: 0.000197 drop: 0.002639 bndrop: 0.003171 plain: 0.000381\n",
      "Train Epoch: 42 [12800/37800 (1%)]\tLosses bn: 0.000276 drop: 0.004621 bndrop: 0.005130 plain: 0.002106\n",
      "Train Epoch: 42 [19200/37800 (2%)]\tLosses bn: 0.001308 drop: 0.013159 bndrop: 0.003187 plain: 0.000588\n",
      "Train Epoch: 42 [25600/37800 (2%)]\tLosses bn: 0.000140 drop: 0.001827 bndrop: 0.000211 plain: 0.000616\n",
      "Train Epoch: 42 [32000/37800 (3%)]\tLosses bn: 0.000051 drop: 0.001125 bndrop: 0.000338 plain: 0.000162\n",
      "Train Epoch: 42 [37792/37800 (3%)]\tLosses bn: 0.000004 drop: 0.002342 bndrop: 0.000127 plain: 0.000239\n",
      "Test set:\n",
      "bn: Loss: 0.0394\tAccuracy: 4147.0/4200 (99%)\n",
      "drop: Loss: 0.0441\tAccuracy: 4142.0/4200 (99%)\n",
      "bndrop: Loss: 0.0416\tAccuracy: 4148.0/4200 (99%)\n",
      "plain: Loss: 0.0372\tAccuracy: 4151.0/4200 (99%)\n",
      "\n",
      "LR: 3.4965034965034965e-05\n",
      "Train Epoch: 43 [0/37800 (0%)]\tLosses bn: 0.000285 drop: 0.018764 bndrop: 0.005851 plain: 0.003541\n",
      "Train Epoch: 43 [6400/37800 (1%)]\tLosses bn: 0.000188 drop: 0.001410 bndrop: 0.000912 plain: 0.000503\n",
      "Train Epoch: 43 [12800/37800 (1%)]\tLosses bn: 0.000174 drop: 0.002879 bndrop: 0.002122 plain: 0.001367\n",
      "Train Epoch: 43 [19200/37800 (2%)]\tLosses bn: 0.000255 drop: 0.006451 bndrop: 0.000573 plain: 0.006215\n",
      "Train Epoch: 43 [25600/37800 (2%)]\tLosses bn: 0.000034 drop: 0.000237 bndrop: 0.000127 plain: 0.000150\n",
      "Train Epoch: 43 [32000/37800 (3%)]\tLosses bn: 0.000533 drop: 0.064740 bndrop: 0.004596 plain: 0.038432\n",
      "Train Epoch: 43 [37792/37800 (3%)]\tLosses bn: 0.000014 drop: 0.000689 bndrop: 0.000053 plain: 0.000385\n",
      "Test set:\n",
      "bn: Loss: 0.0390\tAccuracy: 4146.0/4200 (99%)\n",
      "drop: Loss: 0.0434\tAccuracy: 4144.0/4200 (99%)\n",
      "bndrop: Loss: 0.0475\tAccuracy: 4143.0/4200 (99%)\n",
      "plain: Loss: 0.0366\tAccuracy: 4149.0/4200 (99%)\n",
      "\n",
      "LR: 3.472222222222222e-05\n",
      "Train Epoch: 44 [0/37800 (0%)]\tLosses bn: 0.000646 drop: 0.006654 bndrop: 0.001541 plain: 0.003356\n",
      "Train Epoch: 44 [6400/37800 (1%)]\tLosses bn: 0.000032 drop: 0.000897 bndrop: 0.001659 plain: 0.000432\n",
      "Train Epoch: 44 [12800/37800 (1%)]\tLosses bn: 0.000281 drop: 0.065494 bndrop: 0.005043 plain: 0.033684\n",
      "Train Epoch: 44 [19200/37800 (2%)]\tLosses bn: 0.011573 drop: 0.002181 bndrop: 0.000924 plain: 0.003116\n",
      "Train Epoch: 44 [25600/37800 (2%)]\tLosses bn: 0.000004 drop: 0.000689 bndrop: 0.000052 plain: 0.001037\n",
      "Train Epoch: 44 [32000/37800 (3%)]\tLosses bn: 0.001399 drop: 0.011566 bndrop: 0.000788 plain: 0.002498\n",
      "Train Epoch: 44 [37792/37800 (3%)]\tLosses bn: 0.000527 drop: 0.013276 bndrop: 0.002801 plain: 0.002577\n",
      "Test set:\n",
      "bn: Loss: 0.0404\tAccuracy: 4147.0/4200 (99%)\n",
      "drop: Loss: 0.0432\tAccuracy: 4143.0/4200 (99%)\n",
      "bndrop: Loss: 0.0437\tAccuracy: 4145.0/4200 (99%)\n",
      "plain: Loss: 0.0358\tAccuracy: 4147.0/4200 (99%)\n",
      "\n",
      "LR: 3.4482758620689657e-05\n",
      "Train Epoch: 45 [0/37800 (0%)]\tLosses bn: 0.000038 drop: 0.004350 bndrop: 0.000080 plain: 0.000378\n",
      "Train Epoch: 45 [6400/37800 (1%)]\tLosses bn: 0.000088 drop: 0.007538 bndrop: 0.001613 plain: 0.002526\n",
      "Train Epoch: 45 [12800/37800 (1%)]\tLosses bn: 0.000249 drop: 0.020411 bndrop: 0.000173 plain: 0.027491\n",
      "Train Epoch: 45 [19200/37800 (2%)]\tLosses bn: 0.002389 drop: 0.001181 bndrop: 0.000331 plain: 0.000348\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8ec102207223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-36c9f48051a7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, models, log)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/otus-dl-engineer/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/otus-dl-engineer/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    train_x, train_y = shuffle_data(train_x, train_y)    \n",
    "    train(epoch, models, train_log)\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    test(models, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(test_log, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(test_log, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('./test.csv', delimiter=',')\n",
    "data = np.delete(data, (0), axis=0)\n",
    "data_np_x = data / 255\n",
    "\n",
    "testing_x = torch.Tensor(np.expand_dims(normalize(data_np_x), axis=1))\n",
    "testing_y = torch.LongTensor(np.zeros((testing_x.shape[0], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (testing_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "answers = np.empty((0, 10))\n",
    "model = models['drop']\n",
    "print (model)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(loader(testing_x, testing_y)):\n",
    "    output = model(data)  \n",
    "    output_np = output.detach().numpy()        \n",
    "    answers = np.vstack((answers, output_np))\n",
    "        \n",
    "print (answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.argmax(answers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (answers[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (vals[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission2.txt\", \"w\") as fout:\n",
    "    fout.write(\"ImageId,Label\\n\")\n",
    "    for i, val in enumerate(vals):\n",
    "        fout.write(\"{},{}\\n\".format(str(i+1), str(val)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
