{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksey/anaconda3/envs/learning/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import PIL\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0         0       3    male  22.0      1      0   7.2500        S\n",
       "1         1       1  female  38.0      1      0  71.2833        C\n",
       "2         1       3  female  26.0      0      0   7.9250        S\n",
       "3         1       1  female  35.0      1      0  53.1000        S\n",
       "4         0       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./train.csv')\n",
    "df = df.drop('PassengerId', 1)\n",
    "df = df.drop('Name', 1)\n",
    "df = df.drop('Ticket', 1)\n",
    "df = df.drop('Cabin', 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>SibSp_0</th>\n",
       "      <th>SibSp_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Parch_0</th>\n",
       "      <th>Parch_1</th>\n",
       "      <th>Parch_2</th>\n",
       "      <th>Parch_3</th>\n",
       "      <th>Parch_4</th>\n",
       "      <th>Parch_5</th>\n",
       "      <th>Parch_6</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived   Age     Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  \\\n",
       "0         0  22.0   7.2500         0         0         1           0   \n",
       "1         1  38.0  71.2833         1         0         0           1   \n",
       "2         1  26.0   7.9250         0         0         1           1   \n",
       "3         1  35.0  53.1000         1         0         0           1   \n",
       "4         0  35.0   8.0500         0         0         1           0   \n",
       "\n",
       "   Sex_male  SibSp_0  SibSp_1  ...  Parch_0  Parch_1  Parch_2  Parch_3  \\\n",
       "0         1        0        1  ...        1        0        0        0   \n",
       "1         0        0        1  ...        1        0        0        0   \n",
       "2         0        1        0  ...        1        0        0        0   \n",
       "3         0        0        1  ...        1        0        0        0   \n",
       "4         1        1        0  ...        1        0        0        0   \n",
       "\n",
       "   Parch_4  Parch_5  Parch_6  Embarked_C  Embarked_Q  Embarked_S  \n",
       "0        0        0        0           0           0           1  \n",
       "1        0        0        0           1           0           0  \n",
       "2        0        0        0           0           0           1  \n",
       "3        0        0        0           0           0           1  \n",
       "4        0        0        0           0           0           1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_features = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
    "for enc in encode_features:\n",
    "    one_hot = pd.get_dummies(df[enc], prefix=enc)\n",
    "    df = df.drop(enc,axis = 1)\n",
    "    df = df.join(one_hot)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>SibSp_0</th>\n",
       "      <th>SibSp_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Parch_0</th>\n",
       "      <th>Parch_1</th>\n",
       "      <th>Parch_2</th>\n",
       "      <th>Parch_3</th>\n",
       "      <th>Parch_4</th>\n",
       "      <th>Parch_5</th>\n",
       "      <th>Parch_6</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.206510</td>\n",
       "      <td>0.551066</td>\n",
       "      <td>0.352413</td>\n",
       "      <td>0.647587</td>\n",
       "      <td>0.682379</td>\n",
       "      <td>0.234568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760943</td>\n",
       "      <td>0.132435</td>\n",
       "      <td>0.089787</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.188552</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.722783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>0.428790</td>\n",
       "      <td>0.405028</td>\n",
       "      <td>0.497665</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.465813</td>\n",
       "      <td>0.423966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426747</td>\n",
       "      <td>0.339154</td>\n",
       "      <td>0.286037</td>\n",
       "      <td>0.074743</td>\n",
       "      <td>0.066890</td>\n",
       "      <td>0.074743</td>\n",
       "      <td>0.033501</td>\n",
       "      <td>0.391372</td>\n",
       "      <td>0.281141</td>\n",
       "      <td>0.447876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Survived         Age        Fare    Pclass_1    Pclass_2    Pclass_3  \\\n",
       "count  891.000000  714.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean     0.383838   29.699118   32.204208    0.242424    0.206510    0.551066   \n",
       "std      0.486592   14.526497   49.693429    0.428790    0.405028    0.497665   \n",
       "min      0.000000    0.420000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000   20.125000    7.910400    0.000000    0.000000    0.000000   \n",
       "50%      0.000000   28.000000   14.454200    0.000000    0.000000    1.000000   \n",
       "75%      1.000000   38.000000   31.000000    0.000000    0.000000    1.000000   \n",
       "max      1.000000   80.000000  512.329200    1.000000    1.000000    1.000000   \n",
       "\n",
       "       Sex_female    Sex_male     SibSp_0     SibSp_1  ...     Parch_0  \\\n",
       "count  891.000000  891.000000  891.000000  891.000000  ...  891.000000   \n",
       "mean     0.352413    0.647587    0.682379    0.234568  ...    0.760943   \n",
       "std      0.477990    0.477990    0.465813    0.423966  ...    0.426747   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000  ...    1.000000   \n",
       "50%      0.000000    1.000000    1.000000    0.000000  ...    1.000000   \n",
       "75%      1.000000    1.000000    1.000000    0.000000  ...    1.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
       "\n",
       "          Parch_1     Parch_2     Parch_3     Parch_4     Parch_5     Parch_6  \\\n",
       "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean     0.132435    0.089787    0.005612    0.004489    0.005612    0.001122   \n",
       "std      0.339154    0.286037    0.074743    0.066890    0.074743    0.033501   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "       Embarked_C  Embarked_Q  Embarked_S  \n",
       "count  891.000000  891.000000  891.000000  \n",
       "mean     0.188552    0.086420    0.722783  \n",
       "std      0.391372    0.281141    0.447876  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000  \n",
       "50%      0.000000    0.000000    1.000000  \n",
       "75%      0.000000    0.000000    1.000000  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Parch_0', 'Embarked_S', 'Embarked_Q', 'SibSp_3', 'Parch_6', 'Parch_3', 'Parch_5', 'SibSp_2', 'Embarked_C', 'Parch_2', 'Parch_4', 'SibSp_4', 'SibSp_0', 'SibSp_1', 'Sex_male', 'Pclass_2', 'Pclass_3', 'SibSp_5', 'Sex_female', 'Pclass_1', 'Parch_1', 'SibSp_8']\n"
     ]
    }
   ],
   "source": [
    "real_features = ['Age', 'Fare']\n",
    "cat_features = list(set(df.columns.values.tolist()) - set(real_features) )\n",
    "cat_features.remove('Survived')\n",
    "print (cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 25)\n"
     ]
    }
   ],
   "source": [
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_real: [22.    7.25] \n",
      "X_cat: [1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0] \n",
      "y: 0 \n"
     ]
    }
   ],
   "source": [
    "y = df['Survived'].to_numpy()\n",
    "df = df.drop('Survived', 1)\n",
    "\n",
    "X_real = df[real_features].to_numpy()\n",
    "X_cat = df[cat_features].to_numpy()\n",
    "\n",
    "print (\"X_real: {} \".format(X_real[0]))\n",
    "print (\"X_cat: {} \".format(X_cat[0]))\n",
    "print (\"y: {} \".format(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_real_scaled = scaler.fit_transform(X_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801, 2) (90, 2)\n",
      "(801, 22) (90, 22)\n"
     ]
    }
   ],
   "source": [
    "(X_train_real, X_test_real, X_train_cat, X_test_cat, \n",
    " y_train, y_test)= train_test_split(X_real_scaled, X_cat, y, test_size=0.1, random_state=0)\n",
    "print (X_train_real.shape, X_test_real.shape)\n",
    "print (X_train_cat.shape, X_test_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = (0, 1, 2, 5, 7, 9, 10, 12, 14, 16, 17, 18, 19, 21, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([801, 15]) torch.Size([90, 15])\n"
     ]
    }
   ],
   "source": [
    "X_train = np.hstack((X_train_real, X_train_cat))\n",
    "X_test = np.hstack((X_test_real, X_test_cat))\n",
    "\n",
    "X_train = X_train[:,selected_features]\n",
    "X_test = X_test[:,selected_features]\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.Tensor(y_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "print (X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, sizes, dropout=False, activation_fn=nn.ReLU):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout())\n",
    "            layers.append(activation_fn())\n",
    "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
    "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "            layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_vals(output):\n",
    "    vals = []\n",
    "    for out in output:\n",
    "        out = out.item()\n",
    "        if out >= 0.5:\n",
    "            vals.append(1)\n",
    "        else:\n",
    "            vals.append(0)   \n",
    "    return vals        \n",
    "\n",
    "def correct_vals(output, target):    \n",
    "    vals = torch.Tensor(output_vals(output))\n",
    "    correct = (vals == target).sum().item()         \n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, batchnorm=False, dropout=False, lr=1e-4, l2=0.):\n",
    "        super(Net, self).__init__()\n",
    "               \n",
    "        self.fc = FullyConnected([15, 32, 1], dropout=dropout)\n",
    "        \n",
    "        self._loss = None\n",
    "        self.optim = optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "        self.criterion = nn.BCELoss()\n",
    "           \n",
    "    def forward(self, x):        \n",
    "        out = self.fc(x)\n",
    "        return out.squeeze(1)\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):          \n",
    "        self._loss = self.criterion(output, target)\n",
    "        self._correct = correct_vals(output, target)   \n",
    "        \n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, models, log=None):\n",
    "    train_size = len(X_train)\n",
    "    for batch_idx, (data, target) in enumerate(loader(X_train, y_train)):\n",
    "        for model in models.values():                             \n",
    "            model.optim.zero_grad()            \n",
    "            output = model(data)\n",
    "            loss = model.loss(output, target)\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "            \n",
    "        if batch_idx % 200 == 0:\n",
    "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / train_size)\n",
    "            losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "            print(line + losses)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / train_size)\n",
    "        losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "        if log is not None:\n",
    "            for k in models:\n",
    "                log[k].append((models[k]._loss, models[k]._correct))\n",
    "        print(line + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, log=None):\n",
    "    test_size = len(X_test)\n",
    "    avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
    "    acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, test_size, p)\n",
    "    line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
    "\n",
    "    test_loss = {k: 0. for k in models}\n",
    "    correct = {k: 0. for k in models}\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader(X_test, y_test):\n",
    "            output = {k: m(data) for k, m in models.items()}           \n",
    "            for k, m in models.items():     \n",
    "                #print (output[k].shape, target.shape)\n",
    "                test_loss[k] += m.loss(output[k], target, size_average=False).item() # sum up batch loss                \n",
    "                correct[k] += correct_vals(output[k], target)\n",
    "    \n",
    "    for k in models:\n",
    "        test_loss[k] /= test_size\n",
    "    correct_pct = {k: c / test_size for k, c in correct.items()}\n",
    "    lines = '\\n'.join([line(k, test_loss[k], correct[k], 100*correct_pct[k]) for k in models]) + '\\n'\n",
    "    report = 'Test set:\\n' + lines\n",
    "    if log is not None:\n",
    "        for k in models:\n",
    "            log[k].append((test_loss[k], correct_pct[k]))\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(log, tpe='loss'):\n",
    "    keys = log.keys()\n",
    "    logs = {k:[z for z in zip(*log[k])] for k in keys}\n",
    "    epochs = {k:range(len(log[k])) for k in keys}\n",
    " \n",
    "    \n",
    "    if tpe == 'loss':\n",
    "        handlers, = zip(*[plt.plot(epochs[k], logs[k][0], label=k) for k in keys])\n",
    "        plt.title('errors')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('error')\n",
    "        plt.legend(handles=handlers)\n",
    "        plt.show()\n",
    "    elif tpe == 'accuracy':\n",
    "        handlers, = zip(*[plt.plot(epochs[k], logs[k][1], label=k) for k in log.keys()])\n",
    "        plt.title('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.legend(handles=handlers)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(x, y, batch_size=16):    \n",
    "    for i in range(0, x.shape[0] - 1, batch_size):\n",
    "        data = x[i:i+batch_size]\n",
    "        if data.shape[0] == batch_size:            \n",
    "            targets = y[i:i+batch_size]\n",
    "\n",
    "            yield data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc): FullyConnected(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=15, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=1, bias=True)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (criterion): BCELoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "models = {'bn': Net(True), 'drop': Net(False, True), 'plain': Net()}\n",
    "train_log = {k: [] for k in models}\n",
    "test_log = {k: [] for k in models}\n",
    "\n",
    "print (models['bn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/801 (0%)]\tLosses bn: 0.670982 drop: 0.732536 plain: 0.726693\n",
      "Train Epoch: 1 [800/801 (6%)]\tLosses bn: 0.684073 drop: 0.698222 plain: 0.689076\n",
      "Test set:\n",
      "bn: Loss: 0.0373\tAccuracy: 57.0/90 (63%)\n",
      "drop: Loss: 0.0388\tAccuracy: 34.0/90 (38%)\n",
      "plain: Loss: 0.0384\tAccuracy: 45.0/90 (50%)\n",
      "\n",
      "Train Epoch: 2 [0/801 (0%)]\tLosses bn: 0.662622 drop: 0.727876 plain: 0.712593\n",
      "Train Epoch: 2 [800/801 (6%)]\tLosses bn: 0.686947 drop: 0.687601 plain: 0.691785\n",
      "Test set:\n",
      "bn: Loss: 0.0370\tAccuracy: 56.0/90 (62%)\n",
      "drop: Loss: 0.0384\tAccuracy: 43.0/90 (48%)\n",
      "plain: Loss: 0.0379\tAccuracy: 46.0/90 (51%)\n",
      "\n",
      "Train Epoch: 3 [0/801 (0%)]\tLosses bn: 0.654964 drop: 0.697535 plain: 0.700224\n",
      "Train Epoch: 3 [800/801 (6%)]\tLosses bn: 0.689882 drop: 0.679175 plain: 0.694657\n",
      "Test set:\n",
      "bn: Loss: 0.0366\tAccuracy: 56.0/90 (62%)\n",
      "drop: Loss: 0.0381\tAccuracy: 48.0/90 (53%)\n",
      "plain: Loss: 0.0375\tAccuracy: 47.0/90 (52%)\n",
      "\n",
      "Train Epoch: 4 [0/801 (0%)]\tLosses bn: 0.647411 drop: 0.693717 plain: 0.688941\n",
      "Train Epoch: 4 [800/801 (6%)]\tLosses bn: 0.692799 drop: 0.712929 plain: 0.697676\n",
      "Test set:\n",
      "bn: Loss: 0.0363\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0378\tAccuracy: 52.0/90 (58%)\n",
      "plain: Loss: 0.0371\tAccuracy: 50.0/90 (56%)\n",
      "\n",
      "Train Epoch: 5 [0/801 (0%)]\tLosses bn: 0.639912 drop: 0.672486 plain: 0.678599\n",
      "Train Epoch: 5 [800/801 (6%)]\tLosses bn: 0.695745 drop: 0.703099 plain: 0.700739\n",
      "Test set:\n",
      "bn: Loss: 0.0360\tAccuracy: 54.0/90 (60%)\n",
      "drop: Loss: 0.0376\tAccuracy: 51.0/90 (57%)\n",
      "plain: Loss: 0.0367\tAccuracy: 48.0/90 (53%)\n",
      "\n",
      "Train Epoch: 6 [0/801 (0%)]\tLosses bn: 0.632261 drop: 0.640170 plain: 0.668914\n",
      "Train Epoch: 6 [800/801 (6%)]\tLosses bn: 0.698709 drop: 0.682873 plain: 0.703703\n",
      "Test set:\n",
      "bn: Loss: 0.0357\tAccuracy: 54.0/90 (60%)\n",
      "drop: Loss: 0.0373\tAccuracy: 52.0/90 (58%)\n",
      "plain: Loss: 0.0363\tAccuracy: 49.0/90 (54%)\n",
      "\n",
      "Train Epoch: 7 [0/801 (0%)]\tLosses bn: 0.624247 drop: 0.686497 plain: 0.659643\n",
      "Train Epoch: 7 [800/801 (6%)]\tLosses bn: 0.701652 drop: 0.702912 plain: 0.706587\n",
      "Test set:\n",
      "bn: Loss: 0.0354\tAccuracy: 54.0/90 (60%)\n",
      "drop: Loss: 0.0370\tAccuracy: 51.0/90 (57%)\n",
      "plain: Loss: 0.0360\tAccuracy: 51.0/90 (57%)\n",
      "\n",
      "Train Epoch: 8 [0/801 (0%)]\tLosses bn: 0.616517 drop: 0.683185 plain: 0.650797\n",
      "Train Epoch: 8 [800/801 (6%)]\tLosses bn: 0.704543 drop: 0.697831 plain: 0.709354\n",
      "Test set:\n",
      "bn: Loss: 0.0350\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0368\tAccuracy: 52.0/90 (58%)\n",
      "plain: Loss: 0.0356\tAccuracy: 51.0/90 (57%)\n",
      "\n",
      "Train Epoch: 9 [0/801 (0%)]\tLosses bn: 0.608893 drop: 0.667358 plain: 0.642252\n",
      "Train Epoch: 9 [800/801 (6%)]\tLosses bn: 0.707310 drop: 0.689411 plain: 0.712116\n",
      "Test set:\n",
      "bn: Loss: 0.0347\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0365\tAccuracy: 53.0/90 (59%)\n",
      "plain: Loss: 0.0353\tAccuracy: 52.0/90 (58%)\n",
      "\n",
      "Train Epoch: 10 [0/801 (0%)]\tLosses bn: 0.601405 drop: 0.654057 plain: 0.634191\n",
      "Train Epoch: 10 [800/801 (6%)]\tLosses bn: 0.709927 drop: 0.697148 plain: 0.714772\n",
      "Test set:\n",
      "bn: Loss: 0.0344\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0363\tAccuracy: 53.0/90 (59%)\n",
      "plain: Loss: 0.0350\tAccuracy: 51.0/90 (57%)\n",
      "\n",
      "Train Epoch: 11 [0/801 (0%)]\tLosses bn: 0.594151 drop: 0.639747 plain: 0.626485\n",
      "Train Epoch: 11 [800/801 (6%)]\tLosses bn: 0.712393 drop: 0.663225 plain: 0.717256\n",
      "Test set:\n",
      "bn: Loss: 0.0341\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0361\tAccuracy: 53.0/90 (59%)\n",
      "plain: Loss: 0.0347\tAccuracy: 51.0/90 (57%)\n",
      "\n",
      "Train Epoch: 12 [0/801 (0%)]\tLosses bn: 0.587148 drop: 0.629340 plain: 0.619091\n",
      "Train Epoch: 12 [800/801 (6%)]\tLosses bn: 0.714673 drop: 0.698003 plain: 0.719507\n",
      "Test set:\n",
      "bn: Loss: 0.0338\tAccuracy: 54.0/90 (60%)\n",
      "drop: Loss: 0.0358\tAccuracy: 53.0/90 (59%)\n",
      "plain: Loss: 0.0344\tAccuracy: 53.0/90 (59%)\n",
      "\n",
      "Train Epoch: 13 [0/801 (0%)]\tLosses bn: 0.580327 drop: 0.653123 plain: 0.612025\n",
      "Train Epoch: 13 [800/801 (6%)]\tLosses bn: 0.716734 drop: 0.696124 plain: 0.721523\n",
      "Test set:\n",
      "bn: Loss: 0.0336\tAccuracy: 54.0/90 (60%)\n",
      "drop: Loss: 0.0356\tAccuracy: 53.0/90 (59%)\n",
      "plain: Loss: 0.0341\tAccuracy: 53.0/90 (59%)\n",
      "\n",
      "Train Epoch: 14 [0/801 (0%)]\tLosses bn: 0.573816 drop: 0.638200 plain: 0.605263\n",
      "Train Epoch: 14 [800/801 (6%)]\tLosses bn: 0.718491 drop: 0.670867 plain: 0.723212\n",
      "Test set:\n",
      "bn: Loss: 0.0333\tAccuracy: 54.0/90 (60%)\n",
      "drop: Loss: 0.0354\tAccuracy: 53.0/90 (59%)\n",
      "plain: Loss: 0.0338\tAccuracy: 53.0/90 (59%)\n",
      "\n",
      "Train Epoch: 15 [0/801 (0%)]\tLosses bn: 0.567517 drop: 0.604810 plain: 0.598771\n",
      "Train Epoch: 15 [800/801 (6%)]\tLosses bn: 0.720007 drop: 0.673178 plain: 0.724567\n",
      "Test set:\n",
      "bn: Loss: 0.0331\tAccuracy: 54.0/90 (60%)\n",
      "drop: Loss: 0.0352\tAccuracy: 53.0/90 (59%)\n",
      "plain: Loss: 0.0336\tAccuracy: 53.0/90 (59%)\n",
      "\n",
      "Train Epoch: 16 [0/801 (0%)]\tLosses bn: 0.561307 drop: 0.647876 plain: 0.592524\n",
      "Train Epoch: 16 [800/801 (6%)]\tLosses bn: 0.721340 drop: 0.688617 plain: 0.725586\n",
      "Test set:\n",
      "bn: Loss: 0.0328\tAccuracy: 54.0/90 (60%)\n",
      "drop: Loss: 0.0350\tAccuracy: 53.0/90 (59%)\n",
      "plain: Loss: 0.0333\tAccuracy: 53.0/90 (59%)\n",
      "\n",
      "Train Epoch: 17 [0/801 (0%)]\tLosses bn: 0.555281 drop: 0.613953 plain: 0.586508\n",
      "Train Epoch: 17 [800/801 (6%)]\tLosses bn: 0.722315 drop: 0.764863 plain: 0.726291\n",
      "Test set:\n",
      "bn: Loss: 0.0326\tAccuracy: 54.0/90 (60%)\n",
      "drop: Loss: 0.0348\tAccuracy: 53.0/90 (59%)\n",
      "plain: Loss: 0.0331\tAccuracy: 52.0/90 (58%)\n",
      "\n",
      "Train Epoch: 18 [0/801 (0%)]\tLosses bn: 0.549500 drop: 0.592786 plain: 0.580713\n",
      "Train Epoch: 18 [800/801 (6%)]\tLosses bn: 0.722953 drop: 0.710499 plain: 0.726656\n",
      "Test set:\n",
      "bn: Loss: 0.0323\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0346\tAccuracy: 53.0/90 (59%)\n",
      "plain: Loss: 0.0329\tAccuracy: 53.0/90 (59%)\n",
      "\n",
      "Train Epoch: 19 [0/801 (0%)]\tLosses bn: 0.543900 drop: 0.617201 plain: 0.575142\n",
      "Train Epoch: 19 [800/801 (6%)]\tLosses bn: 0.723406 drop: 0.710091 plain: 0.726711\n",
      "Test set:\n",
      "bn: Loss: 0.0321\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0344\tAccuracy: 54.0/90 (60%)\n",
      "plain: Loss: 0.0326\tAccuracy: 54.0/90 (60%)\n",
      "\n",
      "Train Epoch: 20 [0/801 (0%)]\tLosses bn: 0.538459 drop: 0.594174 plain: 0.569719\n",
      "Train Epoch: 20 [800/801 (6%)]\tLosses bn: 0.723574 drop: 0.730247 plain: 0.726483\n",
      "Test set:\n",
      "bn: Loss: 0.0319\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0342\tAccuracy: 54.0/90 (60%)\n",
      "plain: Loss: 0.0324\tAccuracy: 55.0/90 (61%)\n",
      "\n",
      "Train Epoch: 21 [0/801 (0%)]\tLosses bn: 0.533154 drop: 0.583622 plain: 0.564439\n",
      "Train Epoch: 21 [800/801 (6%)]\tLosses bn: 0.723495 drop: 0.731487 plain: 0.726012\n",
      "Test set:\n",
      "bn: Loss: 0.0316\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0340\tAccuracy: 54.0/90 (60%)\n",
      "plain: Loss: 0.0322\tAccuracy: 56.0/90 (62%)\n",
      "\n",
      "Train Epoch: 22 [0/801 (0%)]\tLosses bn: 0.527954 drop: 0.597547 plain: 0.559296\n",
      "Train Epoch: 22 [800/801 (6%)]\tLosses bn: 0.723166 drop: 0.693699 plain: 0.725248\n",
      "Test set:\n",
      "bn: Loss: 0.0314\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0338\tAccuracy: 54.0/90 (60%)\n",
      "plain: Loss: 0.0320\tAccuracy: 56.0/90 (62%)\n",
      "\n",
      "Train Epoch: 23 [0/801 (0%)]\tLosses bn: 0.522943 drop: 0.588501 plain: 0.554293\n",
      "Train Epoch: 23 [800/801 (6%)]\tLosses bn: 0.722603 drop: 0.683425 plain: 0.724233\n",
      "Test set:\n",
      "bn: Loss: 0.0312\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0336\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0318\tAccuracy: 56.0/90 (62%)\n",
      "\n",
      "Train Epoch: 24 [0/801 (0%)]\tLosses bn: 0.518099 drop: 0.567874 plain: 0.549431\n",
      "Train Epoch: 24 [800/801 (6%)]\tLosses bn: 0.721770 drop: 0.706523 plain: 0.722958\n",
      "Test set:\n",
      "bn: Loss: 0.0310\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0335\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0316\tAccuracy: 57.0/90 (63%)\n",
      "\n",
      "Train Epoch: 25 [0/801 (0%)]\tLosses bn: 0.513389 drop: 0.594468 plain: 0.544683\n",
      "Train Epoch: 25 [800/801 (6%)]\tLosses bn: 0.720704 drop: 0.687430 plain: 0.721389\n",
      "Test set:\n",
      "bn: Loss: 0.0308\tAccuracy: 54.0/90 (60%)\n",
      "drop: Loss: 0.0333\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0313\tAccuracy: 58.0/90 (64%)\n",
      "\n",
      "Train Epoch: 26 [0/801 (0%)]\tLosses bn: 0.508833 drop: 0.585321 plain: 0.540042\n",
      "Train Epoch: 26 [800/801 (6%)]\tLosses bn: 0.719469 drop: 0.657179 plain: 0.719597\n",
      "Test set:\n",
      "bn: Loss: 0.0306\tAccuracy: 57.0/90 (63%)\n",
      "drop: Loss: 0.0331\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0311\tAccuracy: 59.0/90 (66%)\n",
      "\n",
      "Train Epoch: 27 [0/801 (0%)]\tLosses bn: 0.504414 drop: 0.583938 plain: 0.535496\n",
      "Train Epoch: 27 [800/801 (6%)]\tLosses bn: 0.718053 drop: 0.759171 plain: 0.717627\n",
      "Test set:\n",
      "bn: Loss: 0.0304\tAccuracy: 57.0/90 (63%)\n",
      "drop: Loss: 0.0329\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0309\tAccuracy: 59.0/90 (66%)\n",
      "\n",
      "Train Epoch: 28 [0/801 (0%)]\tLosses bn: 0.500131 drop: 0.590077 plain: 0.531016\n",
      "Train Epoch: 28 [800/801 (6%)]\tLosses bn: 0.716427 drop: 0.695806 plain: 0.715524\n",
      "Test set:\n",
      "bn: Loss: 0.0301\tAccuracy: 57.0/90 (63%)\n",
      "drop: Loss: 0.0327\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0307\tAccuracy: 59.0/90 (66%)\n",
      "\n",
      "Train Epoch: 29 [0/801 (0%)]\tLosses bn: 0.495993 drop: 0.579719 plain: 0.526608\n",
      "Train Epoch: 29 [800/801 (6%)]\tLosses bn: 0.714618 drop: 0.667759 plain: 0.713309\n",
      "Test set:\n",
      "bn: Loss: 0.0299\tAccuracy: 58.0/90 (64%)\n",
      "drop: Loss: 0.0325\tAccuracy: 58.0/90 (64%)\n",
      "plain: Loss: 0.0305\tAccuracy: 59.0/90 (66%)\n",
      "\n",
      "Train Epoch: 30 [0/801 (0%)]\tLosses bn: 0.491990 drop: 0.568398 plain: 0.522217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30 [800/801 (6%)]\tLosses bn: 0.712704 drop: 0.679083 plain: 0.711027\n",
      "Test set:\n",
      "bn: Loss: 0.0298\tAccuracy: 58.0/90 (64%)\n",
      "drop: Loss: 0.0324\tAccuracy: 58.0/90 (64%)\n",
      "plain: Loss: 0.0303\tAccuracy: 59.0/90 (66%)\n",
      "\n",
      "Train Epoch: 31 [0/801 (0%)]\tLosses bn: 0.488117 drop: 0.540485 plain: 0.517832\n",
      "Train Epoch: 31 [800/801 (6%)]\tLosses bn: 0.710609 drop: 0.712649 plain: 0.708696\n",
      "Test set:\n",
      "bn: Loss: 0.0296\tAccuracy: 59.0/90 (66%)\n",
      "drop: Loss: 0.0322\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0301\tAccuracy: 59.0/90 (66%)\n",
      "\n",
      "Train Epoch: 32 [0/801 (0%)]\tLosses bn: 0.484381 drop: 0.611379 plain: 0.513480\n",
      "Train Epoch: 32 [800/801 (6%)]\tLosses bn: 0.708399 drop: 0.714533 plain: 0.706375\n",
      "Test set:\n",
      "bn: Loss: 0.0294\tAccuracy: 59.0/90 (66%)\n",
      "drop: Loss: 0.0320\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0299\tAccuracy: 59.0/90 (66%)\n",
      "\n",
      "Train Epoch: 33 [0/801 (0%)]\tLosses bn: 0.480753 drop: 0.551018 plain: 0.509184\n",
      "Train Epoch: 33 [800/801 (6%)]\tLosses bn: 0.706131 drop: 0.683105 plain: 0.704019\n",
      "Test set:\n",
      "bn: Loss: 0.0292\tAccuracy: 59.0/90 (66%)\n",
      "drop: Loss: 0.0319\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0297\tAccuracy: 60.0/90 (67%)\n",
      "\n",
      "Train Epoch: 34 [0/801 (0%)]\tLosses bn: 0.477194 drop: 0.542361 plain: 0.504947\n",
      "Train Epoch: 34 [800/801 (6%)]\tLosses bn: 0.703683 drop: 0.654751 plain: 0.701634\n",
      "Test set:\n",
      "bn: Loss: 0.0290\tAccuracy: 59.0/90 (66%)\n",
      "drop: Loss: 0.0317\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0295\tAccuracy: 60.0/90 (67%)\n",
      "\n",
      "Train Epoch: 35 [0/801 (0%)]\tLosses bn: 0.473727 drop: 0.541675 plain: 0.500769\n",
      "Train Epoch: 35 [800/801 (6%)]\tLosses bn: 0.701141 drop: 0.718798 plain: 0.699190\n",
      "Test set:\n",
      "bn: Loss: 0.0288\tAccuracy: 60.0/90 (67%)\n",
      "drop: Loss: 0.0315\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0293\tAccuracy: 60.0/90 (67%)\n",
      "\n",
      "Train Epoch: 36 [0/801 (0%)]\tLosses bn: 0.470341 drop: 0.564604 plain: 0.496641\n",
      "Train Epoch: 36 [800/801 (6%)]\tLosses bn: 0.698558 drop: 0.712848 plain: 0.696766\n",
      "Test set:\n",
      "bn: Loss: 0.0286\tAccuracy: 60.0/90 (67%)\n",
      "drop: Loss: 0.0313\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0291\tAccuracy: 60.0/90 (67%)\n",
      "\n",
      "Train Epoch: 37 [0/801 (0%)]\tLosses bn: 0.467043 drop: 0.541795 plain: 0.492573\n",
      "Train Epoch: 37 [800/801 (6%)]\tLosses bn: 0.695975 drop: 0.706005 plain: 0.694204\n",
      "Test set:\n",
      "bn: Loss: 0.0285\tAccuracy: 60.0/90 (67%)\n",
      "drop: Loss: 0.0312\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0289\tAccuracy: 60.0/90 (67%)\n",
      "\n",
      "Train Epoch: 38 [0/801 (0%)]\tLosses bn: 0.463810 drop: 0.538169 plain: 0.488569\n",
      "Train Epoch: 38 [800/801 (6%)]\tLosses bn: 0.693298 drop: 0.677698 plain: 0.691765\n",
      "Test set:\n",
      "bn: Loss: 0.0283\tAccuracy: 62.0/90 (69%)\n",
      "drop: Loss: 0.0310\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0287\tAccuracy: 60.0/90 (67%)\n",
      "\n",
      "Train Epoch: 39 [0/801 (0%)]\tLosses bn: 0.460618 drop: 0.524915 plain: 0.484640\n",
      "Train Epoch: 39 [800/801 (6%)]\tLosses bn: 0.690467 drop: 0.685257 plain: 0.689402\n",
      "Test set:\n",
      "bn: Loss: 0.0281\tAccuracy: 62.0/90 (69%)\n",
      "drop: Loss: 0.0308\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0285\tAccuracy: 60.0/90 (67%)\n",
      "\n",
      "Train Epoch: 40 [0/801 (0%)]\tLosses bn: 0.457498 drop: 0.523309 plain: 0.480794\n",
      "Train Epoch: 40 [800/801 (6%)]\tLosses bn: 0.687447 drop: 0.706187 plain: 0.687017\n",
      "Test set:\n",
      "bn: Loss: 0.0279\tAccuracy: 62.0/90 (69%)\n",
      "drop: Loss: 0.0307\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0284\tAccuracy: 61.0/90 (68%)\n",
      "\n",
      "Train Epoch: 41 [0/801 (0%)]\tLosses bn: 0.454462 drop: 0.534650 plain: 0.477016\n",
      "Train Epoch: 41 [800/801 (6%)]\tLosses bn: 0.684429 drop: 0.720299 plain: 0.684683\n",
      "Test set:\n",
      "bn: Loss: 0.0278\tAccuracy: 62.0/90 (69%)\n",
      "drop: Loss: 0.0305\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0282\tAccuracy: 61.0/90 (68%)\n",
      "\n",
      "Train Epoch: 42 [0/801 (0%)]\tLosses bn: 0.451495 drop: 0.535143 plain: 0.473370\n",
      "Train Epoch: 42 [800/801 (6%)]\tLosses bn: 0.681474 drop: 0.695025 plain: 0.682388\n",
      "Test set:\n",
      "bn: Loss: 0.0276\tAccuracy: 62.0/90 (69%)\n",
      "drop: Loss: 0.0303\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0280\tAccuracy: 61.0/90 (68%)\n",
      "\n",
      "Train Epoch: 43 [0/801 (0%)]\tLosses bn: 0.448617 drop: 0.545986 plain: 0.469800\n",
      "Train Epoch: 43 [800/801 (6%)]\tLosses bn: 0.678474 drop: 0.663357 plain: 0.680144\n",
      "Test set:\n",
      "bn: Loss: 0.0275\tAccuracy: 62.0/90 (69%)\n",
      "drop: Loss: 0.0302\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0279\tAccuracy: 61.0/90 (68%)\n",
      "\n",
      "Train Epoch: 44 [0/801 (0%)]\tLosses bn: 0.445813 drop: 0.537225 plain: 0.466296\n",
      "Train Epoch: 44 [800/801 (6%)]\tLosses bn: 0.675481 drop: 0.728371 plain: 0.678006\n",
      "Test set:\n",
      "bn: Loss: 0.0273\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0300\tAccuracy: 60.0/90 (67%)\n",
      "plain: Loss: 0.0277\tAccuracy: 61.0/90 (68%)\n",
      "\n",
      "Train Epoch: 45 [0/801 (0%)]\tLosses bn: 0.443097 drop: 0.511747 plain: 0.462864\n",
      "Train Epoch: 45 [800/801 (6%)]\tLosses bn: 0.672615 drop: 0.667054 plain: 0.675866\n",
      "Test set:\n",
      "bn: Loss: 0.0272\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0298\tAccuracy: 60.0/90 (67%)\n",
      "plain: Loss: 0.0275\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 46 [0/801 (0%)]\tLosses bn: 0.440503 drop: 0.525703 plain: 0.459496\n",
      "Train Epoch: 46 [800/801 (6%)]\tLosses bn: 0.669738 drop: 0.670811 plain: 0.673774\n",
      "Test set:\n",
      "bn: Loss: 0.0270\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0297\tAccuracy: 60.0/90 (67%)\n",
      "plain: Loss: 0.0274\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 47 [0/801 (0%)]\tLosses bn: 0.437931 drop: 0.545089 plain: 0.456216\n",
      "Train Epoch: 47 [800/801 (6%)]\tLosses bn: 0.666882 drop: 0.633501 plain: 0.671719\n",
      "Test set:\n",
      "bn: Loss: 0.0269\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0295\tAccuracy: 60.0/90 (67%)\n",
      "plain: Loss: 0.0272\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 48 [0/801 (0%)]\tLosses bn: 0.435329 drop: 0.536536 plain: 0.453040\n",
      "Train Epoch: 48 [800/801 (6%)]\tLosses bn: 0.664108 drop: 0.662492 plain: 0.669700\n",
      "Test set:\n",
      "bn: Loss: 0.0267\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0294\tAccuracy: 61.0/90 (68%)\n",
      "plain: Loss: 0.0270\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 49 [0/801 (0%)]\tLosses bn: 0.432775 drop: 0.478583 plain: 0.449926\n",
      "Train Epoch: 49 [800/801 (6%)]\tLosses bn: 0.661425 drop: 0.603268 plain: 0.667745\n",
      "Test set:\n",
      "bn: Loss: 0.0266\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0292\tAccuracy: 61.0/90 (68%)\n",
      "plain: Loss: 0.0269\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 50 [0/801 (0%)]\tLosses bn: 0.430280 drop: 0.516186 plain: 0.446890\n",
      "Train Epoch: 50 [800/801 (6%)]\tLosses bn: 0.658849 drop: 0.709517 plain: 0.665859\n",
      "Test set:\n",
      "bn: Loss: 0.0264\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0291\tAccuracy: 61.0/90 (68%)\n",
      "plain: Loss: 0.0267\tAccuracy: 62.0/90 (69%)\n",
      "\n",
      "Train Epoch: 51 [0/801 (0%)]\tLosses bn: 0.427887 drop: 0.506940 plain: 0.443894\n",
      "Train Epoch: 51 [800/801 (6%)]\tLosses bn: 0.656393 drop: 0.667632 plain: 0.663914\n",
      "Test set:\n",
      "bn: Loss: 0.0263\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0290\tAccuracy: 61.0/90 (68%)\n",
      "plain: Loss: 0.0266\tAccuracy: 62.0/90 (69%)\n",
      "\n",
      "Train Epoch: 52 [0/801 (0%)]\tLosses bn: 0.425546 drop: 0.515059 plain: 0.440945\n",
      "Train Epoch: 52 [800/801 (6%)]\tLosses bn: 0.654015 drop: 0.637078 plain: 0.661922\n",
      "Test set:\n",
      "bn: Loss: 0.0262\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0288\tAccuracy: 61.0/90 (68%)\n",
      "plain: Loss: 0.0264\tAccuracy: 62.0/90 (69%)\n",
      "\n",
      "Train Epoch: 53 [0/801 (0%)]\tLosses bn: 0.423237 drop: 0.472489 plain: 0.438076\n",
      "Train Epoch: 53 [800/801 (6%)]\tLosses bn: 0.651616 drop: 0.628980 plain: 0.660007\n",
      "Test set:\n",
      "bn: Loss: 0.0261\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0287\tAccuracy: 61.0/90 (68%)\n",
      "plain: Loss: 0.0263\tAccuracy: 62.0/90 (69%)\n",
      "\n",
      "Train Epoch: 54 [0/801 (0%)]\tLosses bn: 0.421007 drop: 0.504748 plain: 0.435275\n",
      "Train Epoch: 54 [800/801 (6%)]\tLosses bn: 0.649156 drop: 0.648301 plain: 0.658157\n",
      "Test set:\n",
      "bn: Loss: 0.0259\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0285\tAccuracy: 61.0/90 (68%)\n",
      "plain: Loss: 0.0262\tAccuracy: 62.0/90 (69%)\n",
      "\n",
      "Train Epoch: 55 [0/801 (0%)]\tLosses bn: 0.418828 drop: 0.525007 plain: 0.432546\n",
      "Train Epoch: 55 [800/801 (6%)]\tLosses bn: 0.646766 drop: 0.705495 plain: 0.656299\n",
      "Test set:\n",
      "bn: Loss: 0.0258\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0284\tAccuracy: 61.0/90 (68%)\n",
      "plain: Loss: 0.0260\tAccuracy: 62.0/90 (69%)\n",
      "\n",
      "Train Epoch: 56 [0/801 (0%)]\tLosses bn: 0.416704 drop: 0.485956 plain: 0.429892\n",
      "Train Epoch: 56 [800/801 (6%)]\tLosses bn: 0.644446 drop: 0.673471 plain: 0.654443\n",
      "Test set:\n",
      "bn: Loss: 0.0257\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0283\tAccuracy: 61.0/90 (68%)\n",
      "plain: Loss: 0.0259\tAccuracy: 62.0/90 (69%)\n",
      "\n",
      "Train Epoch: 57 [0/801 (0%)]\tLosses bn: 0.414655 drop: 0.453929 plain: 0.427290\n",
      "Train Epoch: 57 [800/801 (6%)]\tLosses bn: 0.642101 drop: 0.612572 plain: 0.652655\n",
      "Test set:\n",
      "bn: Loss: 0.0256\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0281\tAccuracy: 62.0/90 (69%)\n",
      "plain: Loss: 0.0258\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 58 [0/801 (0%)]\tLosses bn: 0.412657 drop: 0.458407 plain: 0.424756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 58 [800/801 (6%)]\tLosses bn: 0.639762 drop: 0.688012 plain: 0.650785\n",
      "Test set:\n",
      "bn: Loss: 0.0254\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0280\tAccuracy: 62.0/90 (69%)\n",
      "plain: Loss: 0.0256\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 59 [0/801 (0%)]\tLosses bn: 0.410697 drop: 0.487073 plain: 0.422286\n",
      "Train Epoch: 59 [800/801 (6%)]\tLosses bn: 0.637453 drop: 0.676386 plain: 0.648950\n",
      "Test set:\n",
      "bn: Loss: 0.0253\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0279\tAccuracy: 62.0/90 (69%)\n",
      "plain: Loss: 0.0255\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 60 [0/801 (0%)]\tLosses bn: 0.408769 drop: 0.481253 plain: 0.419878\n",
      "Train Epoch: 60 [800/801 (6%)]\tLosses bn: 0.635184 drop: 0.626632 plain: 0.647176\n",
      "Test set:\n",
      "bn: Loss: 0.0252\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0278\tAccuracy: 62.0/90 (69%)\n",
      "plain: Loss: 0.0254\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 61 [0/801 (0%)]\tLosses bn: 0.406889 drop: 0.490408 plain: 0.417533\n",
      "Train Epoch: 61 [800/801 (6%)]\tLosses bn: 0.632970 drop: 0.663182 plain: 0.645422\n",
      "Test set:\n",
      "bn: Loss: 0.0251\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0276\tAccuracy: 62.0/90 (69%)\n",
      "plain: Loss: 0.0253\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 62 [0/801 (0%)]\tLosses bn: 0.405017 drop: 0.443185 plain: 0.415273\n",
      "Train Epoch: 62 [800/801 (6%)]\tLosses bn: 0.630816 drop: 0.710121 plain: 0.643675\n",
      "Test set:\n",
      "bn: Loss: 0.0250\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0275\tAccuracy: 62.0/90 (69%)\n",
      "plain: Loss: 0.0252\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 63 [0/801 (0%)]\tLosses bn: 0.403191 drop: 0.462480 plain: 0.413080\n",
      "Train Epoch: 63 [800/801 (6%)]\tLosses bn: 0.628744 drop: 0.672685 plain: 0.641959\n",
      "Test set:\n",
      "bn: Loss: 0.0249\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0274\tAccuracy: 62.0/90 (69%)\n",
      "plain: Loss: 0.0251\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 64 [0/801 (0%)]\tLosses bn: 0.401382 drop: 0.490781 plain: 0.410950\n",
      "Train Epoch: 64 [800/801 (6%)]\tLosses bn: 0.626688 drop: 0.621485 plain: 0.640218\n",
      "Test set:\n",
      "bn: Loss: 0.0248\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0273\tAccuracy: 63.0/90 (70%)\n",
      "plain: Loss: 0.0250\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 65 [0/801 (0%)]\tLosses bn: 0.399587 drop: 0.427511 plain: 0.408869\n",
      "Train Epoch: 65 [800/801 (6%)]\tLosses bn: 0.624671 drop: 0.707122 plain: 0.638497\n",
      "Test set:\n",
      "bn: Loss: 0.0247\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0272\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0248\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 66 [0/801 (0%)]\tLosses bn: 0.397817 drop: 0.457435 plain: 0.406831\n",
      "Train Epoch: 66 [800/801 (6%)]\tLosses bn: 0.622680 drop: 0.625788 plain: 0.636797\n",
      "Test set:\n",
      "bn: Loss: 0.0246\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0270\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0247\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 67 [0/801 (0%)]\tLosses bn: 0.396082 drop: 0.473965 plain: 0.404844\n",
      "Train Epoch: 67 [800/801 (6%)]\tLosses bn: 0.620718 drop: 0.575897 plain: 0.635136\n",
      "Test set:\n",
      "bn: Loss: 0.0245\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0269\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0246\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 68 [0/801 (0%)]\tLosses bn: 0.394386 drop: 0.471401 plain: 0.402892\n",
      "Train Epoch: 68 [800/801 (6%)]\tLosses bn: 0.618792 drop: 0.570537 plain: 0.633531\n",
      "Test set:\n",
      "bn: Loss: 0.0244\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0268\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0245\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 69 [0/801 (0%)]\tLosses bn: 0.392721 drop: 0.439242 plain: 0.400982\n",
      "Train Epoch: 69 [800/801 (6%)]\tLosses bn: 0.616835 drop: 0.616571 plain: 0.631944\n",
      "Test set:\n",
      "bn: Loss: 0.0243\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0267\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0244\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 70 [0/801 (0%)]\tLosses bn: 0.391064 drop: 0.470735 plain: 0.399128\n",
      "Train Epoch: 70 [800/801 (6%)]\tLosses bn: 0.614953 drop: 0.599813 plain: 0.630386\n",
      "Test set:\n",
      "bn: Loss: 0.0242\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0266\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0243\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 71 [0/801 (0%)]\tLosses bn: 0.389405 drop: 0.429680 plain: 0.397327\n",
      "Train Epoch: 71 [800/801 (6%)]\tLosses bn: 0.613103 drop: 0.695336 plain: 0.628888\n",
      "Test set:\n",
      "bn: Loss: 0.0241\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0265\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0242\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 72 [0/801 (0%)]\tLosses bn: 0.387763 drop: 0.485150 plain: 0.395579\n",
      "Train Epoch: 72 [800/801 (6%)]\tLosses bn: 0.611244 drop: 0.550220 plain: 0.627410\n",
      "Test set:\n",
      "bn: Loss: 0.0240\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0264\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0241\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 73 [0/801 (0%)]\tLosses bn: 0.386166 drop: 0.482793 plain: 0.393878\n",
      "Train Epoch: 73 [800/801 (6%)]\tLosses bn: 0.609394 drop: 0.592986 plain: 0.625983\n",
      "Test set:\n",
      "bn: Loss: 0.0239\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0263\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0241\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 74 [0/801 (0%)]\tLosses bn: 0.384611 drop: 0.456908 plain: 0.392221\n",
      "Train Epoch: 74 [800/801 (6%)]\tLosses bn: 0.607579 drop: 0.684003 plain: 0.624593\n",
      "Test set:\n",
      "bn: Loss: 0.0238\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0262\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0240\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 75 [0/801 (0%)]\tLosses bn: 0.383093 drop: 0.445543 plain: 0.390629\n",
      "Train Epoch: 75 [800/801 (6%)]\tLosses bn: 0.605825 drop: 0.617392 plain: 0.623258\n",
      "Test set:\n",
      "bn: Loss: 0.0237\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0261\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0239\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 76 [0/801 (0%)]\tLosses bn: 0.381616 drop: 0.440853 plain: 0.389094\n",
      "Train Epoch: 76 [800/801 (6%)]\tLosses bn: 0.604121 drop: 0.639486 plain: 0.621936\n",
      "Test set:\n",
      "bn: Loss: 0.0236\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0260\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0238\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 77 [0/801 (0%)]\tLosses bn: 0.380111 drop: 0.410520 plain: 0.387595\n",
      "Train Epoch: 77 [800/801 (6%)]\tLosses bn: 0.602485 drop: 0.614795 plain: 0.620683\n",
      "Test set:\n",
      "bn: Loss: 0.0236\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0259\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0237\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 78 [0/801 (0%)]\tLosses bn: 0.378650 drop: 0.450896 plain: 0.386129\n",
      "Train Epoch: 78 [800/801 (6%)]\tLosses bn: 0.600994 drop: 0.656304 plain: 0.619415\n",
      "Test set:\n",
      "bn: Loss: 0.0235\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0258\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0236\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 79 [0/801 (0%)]\tLosses bn: 0.377229 drop: 0.408474 plain: 0.384704\n",
      "Train Epoch: 79 [800/801 (6%)]\tLosses bn: 0.599549 drop: 0.509000 plain: 0.618147\n",
      "Test set:\n",
      "bn: Loss: 0.0234\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0257\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0236\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 80 [0/801 (0%)]\tLosses bn: 0.375843 drop: 0.434455 plain: 0.383330\n",
      "Train Epoch: 80 [800/801 (6%)]\tLosses bn: 0.598144 drop: 0.597209 plain: 0.616862\n",
      "Test set:\n",
      "bn: Loss: 0.0233\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0256\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0235\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 81 [0/801 (0%)]\tLosses bn: 0.374503 drop: 0.477959 plain: 0.382000\n",
      "Train Epoch: 81 [800/801 (6%)]\tLosses bn: 0.596706 drop: 0.535754 plain: 0.615569\n",
      "Test set:\n",
      "bn: Loss: 0.0233\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0255\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0234\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 82 [0/801 (0%)]\tLosses bn: 0.373198 drop: 0.378945 plain: 0.380709\n",
      "Train Epoch: 82 [800/801 (6%)]\tLosses bn: 0.595200 drop: 0.636480 plain: 0.614274\n",
      "Test set:\n",
      "bn: Loss: 0.0232\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0254\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0233\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 83 [0/801 (0%)]\tLosses bn: 0.371917 drop: 0.439476 plain: 0.379455\n",
      "Train Epoch: 83 [800/801 (6%)]\tLosses bn: 0.593703 drop: 0.599591 plain: 0.613002\n",
      "Test set:\n",
      "bn: Loss: 0.0231\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0253\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0232\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 84 [0/801 (0%)]\tLosses bn: 0.370677 drop: 0.455246 plain: 0.378235\n",
      "Train Epoch: 84 [800/801 (6%)]\tLosses bn: 0.592127 drop: 0.691867 plain: 0.611739\n",
      "Test set:\n",
      "bn: Loss: 0.0230\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0252\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0232\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 85 [0/801 (0%)]\tLosses bn: 0.369455 drop: 0.432544 plain: 0.377052\n",
      "Train Epoch: 85 [800/801 (6%)]\tLosses bn: 0.590567 drop: 0.587195 plain: 0.610482\n",
      "Test set:\n",
      "bn: Loss: 0.0230\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0251\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0231\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 86 [0/801 (0%)]\tLosses bn: 0.368266 drop: 0.420538 plain: 0.375892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 86 [800/801 (6%)]\tLosses bn: 0.589079 drop: 0.621321 plain: 0.609226\n",
      "Test set:\n",
      "bn: Loss: 0.0229\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0250\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0230\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 87 [0/801 (0%)]\tLosses bn: 0.367129 drop: 0.433357 plain: 0.374759\n",
      "Train Epoch: 87 [800/801 (6%)]\tLosses bn: 0.587588 drop: 0.539765 plain: 0.607998\n",
      "Test set:\n",
      "bn: Loss: 0.0228\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0249\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0230\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 88 [0/801 (0%)]\tLosses bn: 0.366020 drop: 0.435199 plain: 0.373657\n",
      "Train Epoch: 88 [800/801 (6%)]\tLosses bn: 0.586137 drop: 0.661701 plain: 0.606812\n",
      "Test set:\n",
      "bn: Loss: 0.0228\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0248\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0229\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 89 [0/801 (0%)]\tLosses bn: 0.364941 drop: 0.425853 plain: 0.372583\n",
      "Train Epoch: 89 [800/801 (6%)]\tLosses bn: 0.584704 drop: 0.613929 plain: 0.605615\n",
      "Test set:\n",
      "bn: Loss: 0.0227\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0247\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0228\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 90 [0/801 (0%)]\tLosses bn: 0.363887 drop: 0.532929 plain: 0.371532\n",
      "Train Epoch: 90 [800/801 (6%)]\tLosses bn: 0.583302 drop: 0.565069 plain: 0.604439\n",
      "Test set:\n",
      "bn: Loss: 0.0226\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0247\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0228\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 91 [0/801 (0%)]\tLosses bn: 0.362844 drop: 0.438449 plain: 0.370510\n",
      "Train Epoch: 91 [800/801 (6%)]\tLosses bn: 0.581963 drop: 0.591689 plain: 0.603280\n",
      "Test set:\n",
      "bn: Loss: 0.0226\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0246\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0227\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 92 [0/801 (0%)]\tLosses bn: 0.361808 drop: 0.448276 plain: 0.369515\n",
      "Train Epoch: 92 [800/801 (6%)]\tLosses bn: 0.580500 drop: 0.651597 plain: 0.602137\n",
      "Test set:\n",
      "bn: Loss: 0.0225\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0245\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0226\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 93 [0/801 (0%)]\tLosses bn: 0.360825 drop: 0.460187 plain: 0.368538\n",
      "Train Epoch: 93 [800/801 (6%)]\tLosses bn: 0.578964 drop: 0.652767 plain: 0.601017\n",
      "Test set:\n",
      "bn: Loss: 0.0225\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0244\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0226\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 94 [0/801 (0%)]\tLosses bn: 0.359879 drop: 0.458239 plain: 0.367585\n",
      "Train Epoch: 94 [800/801 (6%)]\tLosses bn: 0.577465 drop: 0.581177 plain: 0.599923\n",
      "Test set:\n",
      "bn: Loss: 0.0224\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0243\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0225\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 95 [0/801 (0%)]\tLosses bn: 0.358950 drop: 0.430125 plain: 0.366656\n",
      "Train Epoch: 95 [800/801 (6%)]\tLosses bn: 0.576031 drop: 0.619757 plain: 0.598843\n",
      "Test set:\n",
      "bn: Loss: 0.0224\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0242\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0225\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 96 [0/801 (0%)]\tLosses bn: 0.358039 drop: 0.405170 plain: 0.365757\n",
      "Train Epoch: 96 [800/801 (6%)]\tLosses bn: 0.574600 drop: 0.601878 plain: 0.597782\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0242\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0224\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 97 [0/801 (0%)]\tLosses bn: 0.357129 drop: 0.474763 plain: 0.364888\n",
      "Train Epoch: 97 [800/801 (6%)]\tLosses bn: 0.573207 drop: 0.607347 plain: 0.596720\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0241\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0224\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 98 [0/801 (0%)]\tLosses bn: 0.356213 drop: 0.446258 plain: 0.364051\n",
      "Train Epoch: 98 [800/801 (6%)]\tLosses bn: 0.571848 drop: 0.588053 plain: 0.595651\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0240\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0223\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 99 [0/801 (0%)]\tLosses bn: 0.355303 drop: 0.434010 plain: 0.363240\n",
      "Train Epoch: 99 [800/801 (6%)]\tLosses bn: 0.570523 drop: 0.701589 plain: 0.594605\n",
      "Test set:\n",
      "bn: Loss: 0.0221\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0239\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0223\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 100 [0/801 (0%)]\tLosses bn: 0.354413 drop: 0.430220 plain: 0.362446\n",
      "Train Epoch: 100 [800/801 (6%)]\tLosses bn: 0.569215 drop: 0.644558 plain: 0.593550\n",
      "Test set:\n",
      "bn: Loss: 0.0221\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0239\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0222\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 101 [0/801 (0%)]\tLosses bn: 0.353538 drop: 0.344942 plain: 0.361678\n",
      "Train Epoch: 101 [800/801 (6%)]\tLosses bn: 0.567928 drop: 0.560275 plain: 0.592512\n",
      "Test set:\n",
      "bn: Loss: 0.0220\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0238\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 102 [0/801 (0%)]\tLosses bn: 0.352677 drop: 0.368213 plain: 0.360924\n",
      "Train Epoch: 102 [800/801 (6%)]\tLosses bn: 0.566710 drop: 0.570502 plain: 0.591491\n",
      "Test set:\n",
      "bn: Loss: 0.0220\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0237\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0221\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 103 [0/801 (0%)]\tLosses bn: 0.351832 drop: 0.397392 plain: 0.360181\n",
      "Train Epoch: 103 [800/801 (6%)]\tLosses bn: 0.565495 drop: 0.560200 plain: 0.590481\n",
      "Test set:\n",
      "bn: Loss: 0.0219\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0237\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0221\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 104 [0/801 (0%)]\tLosses bn: 0.351009 drop: 0.477446 plain: 0.359448\n",
      "Train Epoch: 104 [800/801 (6%)]\tLosses bn: 0.564304 drop: 0.659064 plain: 0.589504\n",
      "Test set:\n",
      "bn: Loss: 0.0219\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0236\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0220\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 105 [0/801 (0%)]\tLosses bn: 0.350193 drop: 0.428647 plain: 0.358726\n",
      "Train Epoch: 105 [800/801 (6%)]\tLosses bn: 0.563143 drop: 0.612025 plain: 0.588540\n",
      "Test set:\n",
      "bn: Loss: 0.0219\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0235\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0220\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 106 [0/801 (0%)]\tLosses bn: 0.349383 drop: 0.416732 plain: 0.358030\n",
      "Train Epoch: 106 [800/801 (6%)]\tLosses bn: 0.562024 drop: 0.605099 plain: 0.587596\n",
      "Test set:\n",
      "bn: Loss: 0.0218\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0235\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0219\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 107 [0/801 (0%)]\tLosses bn: 0.348590 drop: 0.446725 plain: 0.357351\n",
      "Train Epoch: 107 [800/801 (6%)]\tLosses bn: 0.560940 drop: 0.609180 plain: 0.586651\n",
      "Test set:\n",
      "bn: Loss: 0.0218\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0234\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0219\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 108 [0/801 (0%)]\tLosses bn: 0.347805 drop: 0.420976 plain: 0.356693\n",
      "Train Epoch: 108 [800/801 (6%)]\tLosses bn: 0.559886 drop: 0.617360 plain: 0.585711\n",
      "Test set:\n",
      "bn: Loss: 0.0217\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0233\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0218\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 109 [0/801 (0%)]\tLosses bn: 0.347014 drop: 0.403768 plain: 0.356047\n",
      "Train Epoch: 109 [800/801 (6%)]\tLosses bn: 0.558842 drop: 0.614891 plain: 0.584786\n",
      "Test set:\n",
      "bn: Loss: 0.0217\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0233\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0218\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 110 [0/801 (0%)]\tLosses bn: 0.346233 drop: 0.399011 plain: 0.355418\n",
      "Train Epoch: 110 [800/801 (6%)]\tLosses bn: 0.557823 drop: 0.601089 plain: 0.583856\n",
      "Test set:\n",
      "bn: Loss: 0.0216\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0232\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0218\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 111 [0/801 (0%)]\tLosses bn: 0.345467 drop: 0.413696 plain: 0.354807\n",
      "Train Epoch: 111 [800/801 (6%)]\tLosses bn: 0.556803 drop: 0.662127 plain: 0.582905\n",
      "Test set:\n",
      "bn: Loss: 0.0216\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0232\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0217\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 112 [0/801 (0%)]\tLosses bn: 0.344716 drop: 0.410479 plain: 0.354196\n",
      "Train Epoch: 112 [800/801 (6%)]\tLosses bn: 0.555796 drop: 0.626020 plain: 0.581996\n",
      "Test set:\n",
      "bn: Loss: 0.0216\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0231\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0217\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 113 [0/801 (0%)]\tLosses bn: 0.343979 drop: 0.353069 plain: 0.353598\n",
      "Train Epoch: 113 [800/801 (6%)]\tLosses bn: 0.554802 drop: 0.574842 plain: 0.581077\n",
      "Test set:\n",
      "bn: Loss: 0.0215\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0230\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0216\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 114 [0/801 (0%)]\tLosses bn: 0.343244 drop: 0.418413 plain: 0.353004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 114 [800/801 (6%)]\tLosses bn: 0.553783 drop: 0.619754 plain: 0.580172\n",
      "Test set:\n",
      "bn: Loss: 0.0215\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0230\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0216\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 115 [0/801 (0%)]\tLosses bn: 0.342519 drop: 0.412064 plain: 0.352451\n",
      "Train Epoch: 115 [800/801 (6%)]\tLosses bn: 0.552794 drop: 0.523245 plain: 0.579269\n",
      "Test set:\n",
      "bn: Loss: 0.0215\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0229\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0215\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 116 [0/801 (0%)]\tLosses bn: 0.341797 drop: 0.428942 plain: 0.351890\n",
      "Train Epoch: 116 [800/801 (6%)]\tLosses bn: 0.551823 drop: 0.618438 plain: 0.578373\n",
      "Test set:\n",
      "bn: Loss: 0.0214\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0229\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0215\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 117 [0/801 (0%)]\tLosses bn: 0.341078 drop: 0.407198 plain: 0.351348\n",
      "Train Epoch: 117 [800/801 (6%)]\tLosses bn: 0.550878 drop: 0.606792 plain: 0.577511\n",
      "Test set:\n",
      "bn: Loss: 0.0214\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0228\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0215\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 118 [0/801 (0%)]\tLosses bn: 0.340353 drop: 0.410603 plain: 0.350813\n",
      "Train Epoch: 118 [800/801 (6%)]\tLosses bn: 0.549955 drop: 0.552358 plain: 0.576618\n",
      "Test set:\n",
      "bn: Loss: 0.0213\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0228\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0214\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 119 [0/801 (0%)]\tLosses bn: 0.339625 drop: 0.406132 plain: 0.350300\n",
      "Train Epoch: 119 [800/801 (6%)]\tLosses bn: 0.549077 drop: 0.676813 plain: 0.575758\n",
      "Test set:\n",
      "bn: Loss: 0.0213\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0227\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0214\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 120 [0/801 (0%)]\tLosses bn: 0.338913 drop: 0.403357 plain: 0.349802\n",
      "Train Epoch: 120 [800/801 (6%)]\tLosses bn: 0.548228 drop: 0.541030 plain: 0.574891\n",
      "Test set:\n",
      "bn: Loss: 0.0213\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0227\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0214\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 121 [0/801 (0%)]\tLosses bn: 0.338204 drop: 0.461210 plain: 0.349294\n",
      "Train Epoch: 121 [800/801 (6%)]\tLosses bn: 0.547377 drop: 0.528157 plain: 0.574068\n",
      "Test set:\n",
      "bn: Loss: 0.0212\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0226\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0213\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 122 [0/801 (0%)]\tLosses bn: 0.337515 drop: 0.356637 plain: 0.348823\n",
      "Train Epoch: 122 [800/801 (6%)]\tLosses bn: 0.546545 drop: 0.617439 plain: 0.573248\n",
      "Test set:\n",
      "bn: Loss: 0.0212\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0226\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0213\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 123 [0/801 (0%)]\tLosses bn: 0.336821 drop: 0.339787 plain: 0.348332\n",
      "Train Epoch: 123 [800/801 (6%)]\tLosses bn: 0.545713 drop: 0.558780 plain: 0.572457\n",
      "Test set:\n",
      "bn: Loss: 0.0212\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0226\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0213\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 124 [0/801 (0%)]\tLosses bn: 0.336152 drop: 0.441722 plain: 0.347863\n",
      "Train Epoch: 124 [800/801 (6%)]\tLosses bn: 0.544884 drop: 0.626862 plain: 0.571644\n",
      "Test set:\n",
      "bn: Loss: 0.0211\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0225\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0212\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 125 [0/801 (0%)]\tLosses bn: 0.335487 drop: 0.434080 plain: 0.347380\n",
      "Train Epoch: 125 [800/801 (6%)]\tLosses bn: 0.544074 drop: 0.583425 plain: 0.570841\n",
      "Test set:\n",
      "bn: Loss: 0.0211\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0225\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0212\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 126 [0/801 (0%)]\tLosses bn: 0.334828 drop: 0.348966 plain: 0.346933\n",
      "Train Epoch: 126 [800/801 (6%)]\tLosses bn: 0.543274 drop: 0.579701 plain: 0.570031\n",
      "Test set:\n",
      "bn: Loss: 0.0211\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0224\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0212\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 127 [0/801 (0%)]\tLosses bn: 0.334180 drop: 0.417390 plain: 0.346461\n",
      "Train Epoch: 127 [800/801 (6%)]\tLosses bn: 0.542488 drop: 0.542107 plain: 0.569229\n",
      "Test set:\n",
      "bn: Loss: 0.0211\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0224\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0211\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 128 [0/801 (0%)]\tLosses bn: 0.333538 drop: 0.405668 plain: 0.346022\n",
      "Train Epoch: 128 [800/801 (6%)]\tLosses bn: 0.541716 drop: 0.598060 plain: 0.568427\n",
      "Test set:\n",
      "bn: Loss: 0.0210\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0223\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0211\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 129 [0/801 (0%)]\tLosses bn: 0.332903 drop: 0.348487 plain: 0.345580\n",
      "Train Epoch: 129 [800/801 (6%)]\tLosses bn: 0.540900 drop: 0.550250 plain: 0.567649\n",
      "Test set:\n",
      "bn: Loss: 0.0210\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0223\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0211\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 130 [0/801 (0%)]\tLosses bn: 0.332276 drop: 0.361192 plain: 0.345148\n",
      "Train Epoch: 130 [800/801 (6%)]\tLosses bn: 0.540120 drop: 0.558897 plain: 0.566871\n",
      "Test set:\n",
      "bn: Loss: 0.0210\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0223\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0210\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 131 [0/801 (0%)]\tLosses bn: 0.331659 drop: 0.465112 plain: 0.344739\n",
      "Train Epoch: 131 [800/801 (6%)]\tLosses bn: 0.539344 drop: 0.556711 plain: 0.566097\n",
      "Test set:\n",
      "bn: Loss: 0.0209\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0210\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 132 [0/801 (0%)]\tLosses bn: 0.331050 drop: 0.397856 plain: 0.344312\n",
      "Train Epoch: 132 [800/801 (6%)]\tLosses bn: 0.538583 drop: 0.515951 plain: 0.565349\n",
      "Test set:\n",
      "bn: Loss: 0.0209\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0210\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 133 [0/801 (0%)]\tLosses bn: 0.330456 drop: 0.358641 plain: 0.343914\n",
      "Train Epoch: 133 [800/801 (6%)]\tLosses bn: 0.537808 drop: 0.543384 plain: 0.564597\n",
      "Test set:\n",
      "bn: Loss: 0.0209\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0221\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0210\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 134 [0/801 (0%)]\tLosses bn: 0.329877 drop: 0.355371 plain: 0.343496\n",
      "Train Epoch: 134 [800/801 (6%)]\tLosses bn: 0.537068 drop: 0.456762 plain: 0.563871\n",
      "Test set:\n",
      "bn: Loss: 0.0209\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0221\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0209\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 135 [0/801 (0%)]\tLosses bn: 0.329305 drop: 0.371813 plain: 0.343094\n",
      "Train Epoch: 135 [800/801 (6%)]\tLosses bn: 0.536338 drop: 0.567106 plain: 0.563164\n",
      "Test set:\n",
      "bn: Loss: 0.0208\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0220\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0209\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 136 [0/801 (0%)]\tLosses bn: 0.328751 drop: 0.407520 plain: 0.342702\n",
      "Train Epoch: 136 [800/801 (6%)]\tLosses bn: 0.535605 drop: 0.611584 plain: 0.562436\n",
      "Test set:\n",
      "bn: Loss: 0.0208\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0220\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0209\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 137 [0/801 (0%)]\tLosses bn: 0.328203 drop: 0.354164 plain: 0.342321\n",
      "Train Epoch: 137 [800/801 (6%)]\tLosses bn: 0.534895 drop: 0.513078 plain: 0.561703\n",
      "Test set:\n",
      "bn: Loss: 0.0208\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0220\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0209\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 138 [0/801 (0%)]\tLosses bn: 0.327663 drop: 0.358543 plain: 0.341925\n",
      "Train Epoch: 138 [800/801 (6%)]\tLosses bn: 0.534201 drop: 0.525311 plain: 0.560998\n",
      "Test set:\n",
      "bn: Loss: 0.0208\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0219\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0208\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 139 [0/801 (0%)]\tLosses bn: 0.327120 drop: 0.351677 plain: 0.341575\n",
      "Train Epoch: 139 [800/801 (6%)]\tLosses bn: 0.533511 drop: 0.575069 plain: 0.560267\n",
      "Test set:\n",
      "bn: Loss: 0.0207\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0219\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0208\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 140 [0/801 (0%)]\tLosses bn: 0.326581 drop: 0.350926 plain: 0.341211\n",
      "Train Epoch: 140 [800/801 (6%)]\tLosses bn: 0.532848 drop: 0.596139 plain: 0.559565\n",
      "Test set:\n",
      "bn: Loss: 0.0207\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0218\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0208\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 141 [0/801 (0%)]\tLosses bn: 0.326048 drop: 0.424130 plain: 0.340850\n",
      "Train Epoch: 141 [800/801 (6%)]\tLosses bn: 0.532162 drop: 0.544186 plain: 0.558886\n",
      "Test set:\n",
      "bn: Loss: 0.0207\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0218\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0208\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 142 [0/801 (0%)]\tLosses bn: 0.325521 drop: 0.352606 plain: 0.340524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 142 [800/801 (6%)]\tLosses bn: 0.531513 drop: 0.524598 plain: 0.558190\n",
      "Test set:\n",
      "bn: Loss: 0.0207\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0218\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0207\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 143 [0/801 (0%)]\tLosses bn: 0.324998 drop: 0.395426 plain: 0.340191\n",
      "Train Epoch: 143 [800/801 (6%)]\tLosses bn: 0.530884 drop: 0.536953 plain: 0.557529\n",
      "Test set:\n",
      "bn: Loss: 0.0207\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0217\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0207\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 144 [0/801 (0%)]\tLosses bn: 0.324478 drop: 0.347858 plain: 0.339859\n",
      "Train Epoch: 144 [800/801 (6%)]\tLosses bn: 0.530256 drop: 0.543188 plain: 0.556886\n",
      "Test set:\n",
      "bn: Loss: 0.0206\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0217\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0207\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 145 [0/801 (0%)]\tLosses bn: 0.323963 drop: 0.391661 plain: 0.339518\n",
      "Train Epoch: 145 [800/801 (6%)]\tLosses bn: 0.529647 drop: 0.491959 plain: 0.556223\n",
      "Test set:\n",
      "bn: Loss: 0.0206\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0217\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0207\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 146 [0/801 (0%)]\tLosses bn: 0.323462 drop: 0.389030 plain: 0.339148\n",
      "Train Epoch: 146 [800/801 (6%)]\tLosses bn: 0.529046 drop: 0.509971 plain: 0.555573\n",
      "Test set:\n",
      "bn: Loss: 0.0206\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0217\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0206\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 147 [0/801 (0%)]\tLosses bn: 0.322961 drop: 0.355483 plain: 0.338784\n",
      "Train Epoch: 147 [800/801 (6%)]\tLosses bn: 0.528470 drop: 0.538436 plain: 0.554949\n",
      "Test set:\n",
      "bn: Loss: 0.0206\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0216\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0206\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 148 [0/801 (0%)]\tLosses bn: 0.322470 drop: 0.413291 plain: 0.338415\n",
      "Train Epoch: 148 [800/801 (6%)]\tLosses bn: 0.527894 drop: 0.600999 plain: 0.554325\n",
      "Test set:\n",
      "bn: Loss: 0.0206\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0216\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0206\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 149 [0/801 (0%)]\tLosses bn: 0.321985 drop: 0.373121 plain: 0.338070\n",
      "Train Epoch: 149 [800/801 (6%)]\tLosses bn: 0.527328 drop: 0.558709 plain: 0.553714\n",
      "Test set:\n",
      "bn: Loss: 0.0206\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0216\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0206\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 150 [0/801 (0%)]\tLosses bn: 0.321496 drop: 0.339185 plain: 0.337732\n",
      "Train Epoch: 150 [800/801 (6%)]\tLosses bn: 0.526743 drop: 0.587566 plain: 0.553088\n",
      "Test set:\n",
      "bn: Loss: 0.0205\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0215\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0206\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 151 [0/801 (0%)]\tLosses bn: 0.321023 drop: 0.355851 plain: 0.337387\n",
      "Train Epoch: 151 [800/801 (6%)]\tLosses bn: 0.526153 drop: 0.596033 plain: 0.552467\n",
      "Test set:\n",
      "bn: Loss: 0.0205\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0215\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0205\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 152 [0/801 (0%)]\tLosses bn: 0.320570 drop: 0.362821 plain: 0.337049\n",
      "Train Epoch: 152 [800/801 (6%)]\tLosses bn: 0.525574 drop: 0.609829 plain: 0.551851\n",
      "Test set:\n",
      "bn: Loss: 0.0205\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0215\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0205\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 153 [0/801 (0%)]\tLosses bn: 0.320114 drop: 0.340208 plain: 0.336716\n",
      "Train Epoch: 153 [800/801 (6%)]\tLosses bn: 0.524994 drop: 0.561197 plain: 0.551237\n",
      "Test set:\n",
      "bn: Loss: 0.0205\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0215\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0205\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 154 [0/801 (0%)]\tLosses bn: 0.319667 drop: 0.324707 plain: 0.336383\n",
      "Train Epoch: 154 [800/801 (6%)]\tLosses bn: 0.524454 drop: 0.522350 plain: 0.550638\n",
      "Test set:\n",
      "bn: Loss: 0.0205\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0214\tAccuracy: 69.0/90 (77%)\n",
      "plain: Loss: 0.0205\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 155 [0/801 (0%)]\tLosses bn: 0.319217 drop: 0.415659 plain: 0.336052\n",
      "Train Epoch: 155 [800/801 (6%)]\tLosses bn: 0.523901 drop: 0.494010 plain: 0.550046\n",
      "Test set:\n",
      "bn: Loss: 0.0205\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0214\tAccuracy: 69.0/90 (77%)\n",
      "plain: Loss: 0.0205\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 156 [0/801 (0%)]\tLosses bn: 0.318772 drop: 0.411524 plain: 0.335721\n",
      "Train Epoch: 156 [800/801 (6%)]\tLosses bn: 0.523372 drop: 0.567826 plain: 0.549464\n",
      "Test set:\n",
      "bn: Loss: 0.0204\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0214\tAccuracy: 69.0/90 (77%)\n",
      "plain: Loss: 0.0205\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 157 [0/801 (0%)]\tLosses bn: 0.318327 drop: 0.368241 plain: 0.335422\n",
      "Train Epoch: 157 [800/801 (6%)]\tLosses bn: 0.522825 drop: 0.531601 plain: 0.548866\n",
      "Test set:\n",
      "bn: Loss: 0.0204\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0213\tAccuracy: 69.0/90 (77%)\n",
      "plain: Loss: 0.0204\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 158 [0/801 (0%)]\tLosses bn: 0.317895 drop: 0.401441 plain: 0.335096\n",
      "Train Epoch: 158 [800/801 (6%)]\tLosses bn: 0.522293 drop: 0.600657 plain: 0.548282\n",
      "Test set:\n",
      "bn: Loss: 0.0204\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0213\tAccuracy: 69.0/90 (77%)\n",
      "plain: Loss: 0.0204\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 159 [0/801 (0%)]\tLosses bn: 0.317467 drop: 0.378275 plain: 0.334773\n",
      "Train Epoch: 159 [800/801 (6%)]\tLosses bn: 0.521767 drop: 0.528352 plain: 0.547703\n",
      "Test set:\n",
      "bn: Loss: 0.0204\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0213\tAccuracy: 69.0/90 (77%)\n",
      "plain: Loss: 0.0204\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 160 [0/801 (0%)]\tLosses bn: 0.317039 drop: 0.344922 plain: 0.334463\n",
      "Train Epoch: 160 [800/801 (6%)]\tLosses bn: 0.521228 drop: 0.513831 plain: 0.547127\n",
      "Test set:\n",
      "bn: Loss: 0.0204\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0213\tAccuracy: 69.0/90 (77%)\n",
      "plain: Loss: 0.0204\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 161 [0/801 (0%)]\tLosses bn: 0.316625 drop: 0.350042 plain: 0.334141\n",
      "Train Epoch: 161 [800/801 (6%)]\tLosses bn: 0.520705 drop: 0.600031 plain: 0.546574\n",
      "Test set:\n",
      "bn: Loss: 0.0204\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0212\tAccuracy: 69.0/90 (77%)\n",
      "plain: Loss: 0.0204\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 162 [0/801 (0%)]\tLosses bn: 0.316209 drop: 0.338459 plain: 0.333820\n",
      "Train Epoch: 162 [800/801 (6%)]\tLosses bn: 0.520228 drop: 0.587621 plain: 0.546019\n",
      "Test set:\n",
      "bn: Loss: 0.0204\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0212\tAccuracy: 69.0/90 (77%)\n",
      "plain: Loss: 0.0203\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 163 [0/801 (0%)]\tLosses bn: 0.315787 drop: 0.408531 plain: 0.333503\n",
      "Train Epoch: 163 [800/801 (6%)]\tLosses bn: 0.519746 drop: 0.616572 plain: 0.545455\n",
      "Test set:\n",
      "bn: Loss: 0.0203\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0212\tAccuracy: 69.0/90 (77%)\n",
      "plain: Loss: 0.0203\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 164 [0/801 (0%)]\tLosses bn: 0.315378 drop: 0.419048 plain: 0.333186\n",
      "Train Epoch: 164 [800/801 (6%)]\tLosses bn: 0.519287 drop: 0.550841 plain: 0.544899\n",
      "Test set:\n",
      "bn: Loss: 0.0203\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0212\tAccuracy: 69.0/90 (77%)\n",
      "plain: Loss: 0.0203\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 165 [0/801 (0%)]\tLosses bn: 0.314978 drop: 0.399607 plain: 0.332876\n",
      "Train Epoch: 165 [800/801 (6%)]\tLosses bn: 0.518791 drop: 0.550415 plain: 0.544339\n",
      "Test set:\n",
      "bn: Loss: 0.0203\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0212\tAccuracy: 69.0/90 (77%)\n",
      "plain: Loss: 0.0203\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 166 [0/801 (0%)]\tLosses bn: 0.314606 drop: 0.391535 plain: 0.332586\n",
      "Train Epoch: 166 [800/801 (6%)]\tLosses bn: 0.518314 drop: 0.547840 plain: 0.543777\n",
      "Test set:\n",
      "bn: Loss: 0.0203\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0211\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0203\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 167 [0/801 (0%)]\tLosses bn: 0.314236 drop: 0.415541 plain: 0.332288\n",
      "Train Epoch: 167 [800/801 (6%)]\tLosses bn: 0.517836 drop: 0.572538 plain: 0.543225\n",
      "Test set:\n",
      "bn: Loss: 0.0203\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0211\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0203\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 168 [0/801 (0%)]\tLosses bn: 0.313861 drop: 0.375462 plain: 0.331995\n",
      "Train Epoch: 168 [800/801 (6%)]\tLosses bn: 0.517360 drop: 0.553824 plain: 0.542683\n",
      "Test set:\n",
      "bn: Loss: 0.0203\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0211\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0203\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 169 [0/801 (0%)]\tLosses bn: 0.313492 drop: 0.344950 plain: 0.331714\n",
      "Train Epoch: 169 [800/801 (6%)]\tLosses bn: 0.516897 drop: 0.582996 plain: 0.542095\n",
      "Test set:\n",
      "bn: Loss: 0.0203\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0211\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 170 [0/801 (0%)]\tLosses bn: 0.313117 drop: 0.362575 plain: 0.331407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 170 [800/801 (6%)]\tLosses bn: 0.516429 drop: 0.519250 plain: 0.541501\n",
      "Test set:\n",
      "bn: Loss: 0.0203\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0210\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 171 [0/801 (0%)]\tLosses bn: 0.312757 drop: 0.349968 plain: 0.331128\n",
      "Train Epoch: 171 [800/801 (6%)]\tLosses bn: 0.515957 drop: 0.528040 plain: 0.540888\n",
      "Test set:\n",
      "bn: Loss: 0.0203\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0210\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 172 [0/801 (0%)]\tLosses bn: 0.312401 drop: 0.373999 plain: 0.330839\n",
      "Train Epoch: 172 [800/801 (6%)]\tLosses bn: 0.515491 drop: 0.588139 plain: 0.540300\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0210\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 173 [0/801 (0%)]\tLosses bn: 0.312049 drop: 0.322967 plain: 0.330543\n",
      "Train Epoch: 173 [800/801 (6%)]\tLosses bn: 0.515020 drop: 0.587799 plain: 0.539713\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0210\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 174 [0/801 (0%)]\tLosses bn: 0.311697 drop: 0.355182 plain: 0.330262\n",
      "Train Epoch: 174 [800/801 (6%)]\tLosses bn: 0.514582 drop: 0.587310 plain: 0.539130\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0210\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 175 [0/801 (0%)]\tLosses bn: 0.311359 drop: 0.359867 plain: 0.329968\n",
      "Train Epoch: 175 [800/801 (6%)]\tLosses bn: 0.514143 drop: 0.489652 plain: 0.538532\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0209\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 176 [0/801 (0%)]\tLosses bn: 0.311014 drop: 0.346471 plain: 0.329683\n",
      "Train Epoch: 176 [800/801 (6%)]\tLosses bn: 0.513704 drop: 0.575302 plain: 0.537943\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0209\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 177 [0/801 (0%)]\tLosses bn: 0.310681 drop: 0.392807 plain: 0.329393\n",
      "Train Epoch: 177 [800/801 (6%)]\tLosses bn: 0.513282 drop: 0.562976 plain: 0.537368\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0209\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 178 [0/801 (0%)]\tLosses bn: 0.310345 drop: 0.351187 plain: 0.329124\n",
      "Train Epoch: 178 [800/801 (6%)]\tLosses bn: 0.512846 drop: 0.569206 plain: 0.536790\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0209\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 179 [0/801 (0%)]\tLosses bn: 0.310014 drop: 0.352781 plain: 0.328826\n",
      "Train Epoch: 179 [800/801 (6%)]\tLosses bn: 0.512424 drop: 0.674629 plain: 0.536226\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0208\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 180 [0/801 (0%)]\tLosses bn: 0.309681 drop: 0.362150 plain: 0.328546\n",
      "Train Epoch: 180 [800/801 (6%)]\tLosses bn: 0.512015 drop: 0.591382 plain: 0.535668\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0208\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 181 [0/801 (0%)]\tLosses bn: 0.309362 drop: 0.358452 plain: 0.328270\n",
      "Train Epoch: 181 [800/801 (6%)]\tLosses bn: 0.511600 drop: 0.545987 plain: 0.535107\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0208\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 182 [0/801 (0%)]\tLosses bn: 0.309040 drop: 0.353255 plain: 0.328001\n",
      "Train Epoch: 182 [800/801 (6%)]\tLosses bn: 0.511200 drop: 0.516103 plain: 0.534559\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0208\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0201\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 183 [0/801 (0%)]\tLosses bn: 0.308709 drop: 0.409087 plain: 0.327735\n",
      "Train Epoch: 183 [800/801 (6%)]\tLosses bn: 0.510809 drop: 0.626162 plain: 0.534006\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0208\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 184 [0/801 (0%)]\tLosses bn: 0.308387 drop: 0.401360 plain: 0.327458\n",
      "Train Epoch: 184 [800/801 (6%)]\tLosses bn: 0.510433 drop: 0.573928 plain: 0.533477\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0207\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 185 [0/801 (0%)]\tLosses bn: 0.308056 drop: 0.353530 plain: 0.327201\n",
      "Train Epoch: 185 [800/801 (6%)]\tLosses bn: 0.510056 drop: 0.514479 plain: 0.532946\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0207\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 186 [0/801 (0%)]\tLosses bn: 0.307719 drop: 0.395268 plain: 0.326914\n",
      "Train Epoch: 186 [800/801 (6%)]\tLosses bn: 0.509679 drop: 0.601260 plain: 0.532419\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0207\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 187 [0/801 (0%)]\tLosses bn: 0.307401 drop: 0.367638 plain: 0.326657\n",
      "Train Epoch: 187 [800/801 (6%)]\tLosses bn: 0.509300 drop: 0.491878 plain: 0.531893\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0207\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 188 [0/801 (0%)]\tLosses bn: 0.307094 drop: 0.333210 plain: 0.326364\n",
      "Train Epoch: 188 [800/801 (6%)]\tLosses bn: 0.508942 drop: 0.629145 plain: 0.531393\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0207\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 189 [0/801 (0%)]\tLosses bn: 0.306782 drop: 0.421045 plain: 0.326090\n",
      "Train Epoch: 189 [800/801 (6%)]\tLosses bn: 0.508557 drop: 0.581291 plain: 0.530890\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0207\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 190 [0/801 (0%)]\tLosses bn: 0.306474 drop: 0.354181 plain: 0.325809\n",
      "Train Epoch: 190 [800/801 (6%)]\tLosses bn: 0.508190 drop: 0.515824 plain: 0.530424\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0207\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 191 [0/801 (0%)]\tLosses bn: 0.306161 drop: 0.379755 plain: 0.325532\n",
      "Train Epoch: 191 [800/801 (6%)]\tLosses bn: 0.507835 drop: 0.567345 plain: 0.529973\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0207\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 192 [0/801 (0%)]\tLosses bn: 0.305859 drop: 0.331363 plain: 0.325251\n",
      "Train Epoch: 192 [800/801 (6%)]\tLosses bn: 0.507498 drop: 0.566661 plain: 0.529518\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0206\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 193 [0/801 (0%)]\tLosses bn: 0.305555 drop: 0.375050 plain: 0.324984\n",
      "Train Epoch: 193 [800/801 (6%)]\tLosses bn: 0.507158 drop: 0.574616 plain: 0.529054\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0206\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 194 [0/801 (0%)]\tLosses bn: 0.305249 drop: 0.356530 plain: 0.324706\n",
      "Train Epoch: 194 [800/801 (6%)]\tLosses bn: 0.506810 drop: 0.573850 plain: 0.528601\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0206\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 195 [0/801 (0%)]\tLosses bn: 0.304954 drop: 0.378561 plain: 0.324440\n",
      "Train Epoch: 195 [800/801 (6%)]\tLosses bn: 0.506491 drop: 0.639051 plain: 0.528153\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0206\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 196 [0/801 (0%)]\tLosses bn: 0.304655 drop: 0.435124 plain: 0.324156\n",
      "Train Epoch: 196 [800/801 (6%)]\tLosses bn: 0.506160 drop: 0.473306 plain: 0.527706\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0206\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 197 [0/801 (0%)]\tLosses bn: 0.304361 drop: 0.387605 plain: 0.323895\n",
      "Train Epoch: 197 [800/801 (6%)]\tLosses bn: 0.505851 drop: 0.564214 plain: 0.527271\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0206\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 198 [0/801 (0%)]\tLosses bn: 0.304064 drop: 0.360329 plain: 0.323605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 198 [800/801 (6%)]\tLosses bn: 0.505538 drop: 0.548498 plain: 0.526825\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0205\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 199 [0/801 (0%)]\tLosses bn: 0.303778 drop: 0.356661 plain: 0.323346\n",
      "Train Epoch: 199 [800/801 (6%)]\tLosses bn: 0.505199 drop: 0.650412 plain: 0.526401\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0205\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 200 [0/801 (0%)]\tLosses bn: 0.303491 drop: 0.372240 plain: 0.323064\n",
      "Train Epoch: 200 [800/801 (6%)]\tLosses bn: 0.504880 drop: 0.445878 plain: 0.525975\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0205\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0198\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 201 [0/801 (0%)]\tLosses bn: 0.303195 drop: 0.370690 plain: 0.322805\n",
      "Train Epoch: 201 [800/801 (6%)]\tLosses bn: 0.504556 drop: 0.502377 plain: 0.525554\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0205\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0198\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 202 [0/801 (0%)]\tLosses bn: 0.302912 drop: 0.360095 plain: 0.322529\n",
      "Train Epoch: 202 [800/801 (6%)]\tLosses bn: 0.504254 drop: 0.521880 plain: 0.525142\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0205\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0198\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 203 [0/801 (0%)]\tLosses bn: 0.302654 drop: 0.359080 plain: 0.322276\n",
      "Train Epoch: 203 [800/801 (6%)]\tLosses bn: 0.503941 drop: 0.559837 plain: 0.524749\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0205\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0198\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 204 [0/801 (0%)]\tLosses bn: 0.302393 drop: 0.378912 plain: 0.321993\n",
      "Train Epoch: 204 [800/801 (6%)]\tLosses bn: 0.503645 drop: 0.519795 plain: 0.524338\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0205\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0198\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 205 [0/801 (0%)]\tLosses bn: 0.302127 drop: 0.394309 plain: 0.321746\n",
      "Train Epoch: 205 [800/801 (6%)]\tLosses bn: 0.503330 drop: 0.567601 plain: 0.523956\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0205\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0198\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 206 [0/801 (0%)]\tLosses bn: 0.301861 drop: 0.451415 plain: 0.321470\n",
      "Train Epoch: 206 [800/801 (6%)]\tLosses bn: 0.503030 drop: 0.639606 plain: 0.523591\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0205\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0198\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 207 [0/801 (0%)]\tLosses bn: 0.301602 drop: 0.339675 plain: 0.321238\n",
      "Train Epoch: 207 [800/801 (6%)]\tLosses bn: 0.502743 drop: 0.686003 plain: 0.523196\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0204\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0198\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 208 [0/801 (0%)]\tLosses bn: 0.301341 drop: 0.421747 plain: 0.320980\n",
      "Train Epoch: 208 [800/801 (6%)]\tLosses bn: 0.502425 drop: 0.570424 plain: 0.522811\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0204\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0198\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 209 [0/801 (0%)]\tLosses bn: 0.301089 drop: 0.325750 plain: 0.320721\n",
      "Train Epoch: 209 [800/801 (6%)]\tLosses bn: 0.502095 drop: 0.541701 plain: 0.522458\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0204\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0198\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 210 [0/801 (0%)]\tLosses bn: 0.300842 drop: 0.386084 plain: 0.320469\n",
      "Train Epoch: 210 [800/801 (6%)]\tLosses bn: 0.501765 drop: 0.556634 plain: 0.522080\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0204\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0198\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 211 [0/801 (0%)]\tLosses bn: 0.300583 drop: 0.339635 plain: 0.320212\n",
      "Train Epoch: 211 [800/801 (6%)]\tLosses bn: 0.501430 drop: 0.562139 plain: 0.521706\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0204\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 212 [0/801 (0%)]\tLosses bn: 0.300326 drop: 0.343932 plain: 0.319968\n",
      "Train Epoch: 212 [800/801 (6%)]\tLosses bn: 0.501106 drop: 0.557874 plain: 0.521321\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0204\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 213 [0/801 (0%)]\tLosses bn: 0.300074 drop: 0.342857 plain: 0.319715\n",
      "Train Epoch: 213 [800/801 (6%)]\tLosses bn: 0.500795 drop: 0.611118 plain: 0.520956\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0203\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 214 [0/801 (0%)]\tLosses bn: 0.299819 drop: 0.386365 plain: 0.319462\n",
      "Train Epoch: 214 [800/801 (6%)]\tLosses bn: 0.500463 drop: 0.422717 plain: 0.520597\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0203\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 215 [0/801 (0%)]\tLosses bn: 0.299577 drop: 0.398443 plain: 0.319230\n",
      "Train Epoch: 215 [800/801 (6%)]\tLosses bn: 0.500160 drop: 0.537628 plain: 0.520249\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0203\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 216 [0/801 (0%)]\tLosses bn: 0.299331 drop: 0.347901 plain: 0.318978\n",
      "Train Epoch: 216 [800/801 (6%)]\tLosses bn: 0.499848 drop: 0.575814 plain: 0.519890\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0203\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 217 [0/801 (0%)]\tLosses bn: 0.299100 drop: 0.406328 plain: 0.318749\n",
      "Train Epoch: 217 [800/801 (6%)]\tLosses bn: 0.499560 drop: 0.617660 plain: 0.519527\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0203\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 218 [0/801 (0%)]\tLosses bn: 0.298868 drop: 0.317520 plain: 0.318486\n",
      "Train Epoch: 218 [800/801 (6%)]\tLosses bn: 0.499310 drop: 0.503436 plain: 0.519187\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0203\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 219 [0/801 (0%)]\tLosses bn: 0.298653 drop: 0.384421 plain: 0.318192\n",
      "Train Epoch: 219 [800/801 (6%)]\tLosses bn: 0.499052 drop: 0.573504 plain: 0.518842\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0203\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 220 [0/801 (0%)]\tLosses bn: 0.298429 drop: 0.360160 plain: 0.317941\n",
      "Train Epoch: 220 [800/801 (6%)]\tLosses bn: 0.498793 drop: 0.527642 plain: 0.518502\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0203\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 221 [0/801 (0%)]\tLosses bn: 0.298191 drop: 0.342243 plain: 0.317656\n",
      "Train Epoch: 221 [800/801 (6%)]\tLosses bn: 0.498515 drop: 0.544009 plain: 0.518151\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0203\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 222 [0/801 (0%)]\tLosses bn: 0.297953 drop: 0.414790 plain: 0.317385\n",
      "Train Epoch: 222 [800/801 (6%)]\tLosses bn: 0.498233 drop: 0.509812 plain: 0.517798\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 223 [0/801 (0%)]\tLosses bn: 0.297701 drop: 0.359323 plain: 0.317117\n",
      "Train Epoch: 223 [800/801 (6%)]\tLosses bn: 0.497964 drop: 0.584798 plain: 0.517458\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 224 [0/801 (0%)]\tLosses bn: 0.297447 drop: 0.354847 plain: 0.316856\n",
      "Train Epoch: 224 [800/801 (6%)]\tLosses bn: 0.497672 drop: 0.564058 plain: 0.517086\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0202\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 225 [0/801 (0%)]\tLosses bn: 0.297193 drop: 0.336287 plain: 0.316589\n",
      "Train Epoch: 225 [800/801 (6%)]\tLosses bn: 0.497388 drop: 0.517187 plain: 0.516737\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0202\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 226 [0/801 (0%)]\tLosses bn: 0.296937 drop: 0.340832 plain: 0.316311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 226 [800/801 (6%)]\tLosses bn: 0.497111 drop: 0.569493 plain: 0.516382\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0202\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0197\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 227 [0/801 (0%)]\tLosses bn: 0.296688 drop: 0.299990 plain: 0.316064\n",
      "Train Epoch: 227 [800/801 (6%)]\tLosses bn: 0.496825 drop: 0.557415 plain: 0.516027\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "drop: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0196\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 228 [0/801 (0%)]\tLosses bn: 0.296435 drop: 0.325721 plain: 0.315801\n",
      "Train Epoch: 228 [800/801 (6%)]\tLosses bn: 0.496555 drop: 0.556035 plain: 0.515680\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0196\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 229 [0/801 (0%)]\tLosses bn: 0.296176 drop: 0.393658 plain: 0.315535\n",
      "Train Epoch: 229 [800/801 (6%)]\tLosses bn: 0.496260 drop: 0.604514 plain: 0.515323\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0202\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0196\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 230 [0/801 (0%)]\tLosses bn: 0.295924 drop: 0.428434 plain: 0.315309\n",
      "Train Epoch: 230 [800/801 (6%)]\tLosses bn: 0.495983 drop: 0.486061 plain: 0.514991\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0196\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 231 [0/801 (0%)]\tLosses bn: 0.295669 drop: 0.363040 plain: 0.315060\n",
      "Train Epoch: 231 [800/801 (6%)]\tLosses bn: 0.495757 drop: 0.622083 plain: 0.514671\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0196\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 232 [0/801 (0%)]\tLosses bn: 0.295442 drop: 0.382110 plain: 0.314802\n",
      "Train Epoch: 232 [800/801 (6%)]\tLosses bn: 0.495530 drop: 0.613777 plain: 0.514349\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0196\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 233 [0/801 (0%)]\tLosses bn: 0.295207 drop: 0.332522 plain: 0.314544\n",
      "Train Epoch: 233 [800/801 (6%)]\tLosses bn: 0.495298 drop: 0.554321 plain: 0.514030\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0196\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 234 [0/801 (0%)]\tLosses bn: 0.294983 drop: 0.347786 plain: 0.314291\n",
      "Train Epoch: 234 [800/801 (6%)]\tLosses bn: 0.495079 drop: 0.482280 plain: 0.513714\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0196\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 235 [0/801 (0%)]\tLosses bn: 0.294755 drop: 0.399583 plain: 0.314029\n",
      "Train Epoch: 235 [800/801 (6%)]\tLosses bn: 0.494860 drop: 0.562202 plain: 0.513401\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0196\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 236 [0/801 (0%)]\tLosses bn: 0.294525 drop: 0.349977 plain: 0.313767\n",
      "Train Epoch: 236 [800/801 (6%)]\tLosses bn: 0.494640 drop: 0.514424 plain: 0.513069\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0201\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0196\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 237 [0/801 (0%)]\tLosses bn: 0.294297 drop: 0.343432 plain: 0.313513\n",
      "Train Epoch: 237 [800/801 (6%)]\tLosses bn: 0.494396 drop: 0.595534 plain: 0.512748\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0201\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0196\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 238 [0/801 (0%)]\tLosses bn: 0.294086 drop: 0.308732 plain: 0.313265\n",
      "Train Epoch: 238 [800/801 (6%)]\tLosses bn: 0.494145 drop: 0.534340 plain: 0.512444\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0196\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 239 [0/801 (0%)]\tLosses bn: 0.293853 drop: 0.340721 plain: 0.313016\n",
      "Train Epoch: 239 [800/801 (6%)]\tLosses bn: 0.493908 drop: 0.524214 plain: 0.512115\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0196\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 240 [0/801 (0%)]\tLosses bn: 0.293622 drop: 0.340941 plain: 0.312778\n",
      "Train Epoch: 240 [800/801 (6%)]\tLosses bn: 0.493679 drop: 0.537114 plain: 0.511806\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 241 [0/801 (0%)]\tLosses bn: 0.293390 drop: 0.362879 plain: 0.312549\n",
      "Train Epoch: 241 [800/801 (6%)]\tLosses bn: 0.493470 drop: 0.532481 plain: 0.511501\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 242 [0/801 (0%)]\tLosses bn: 0.293170 drop: 0.323070 plain: 0.312315\n",
      "Train Epoch: 242 [800/801 (6%)]\tLosses bn: 0.493231 drop: 0.471191 plain: 0.511230\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 243 [0/801 (0%)]\tLosses bn: 0.292988 drop: 0.358588 plain: 0.312090\n",
      "Train Epoch: 243 [800/801 (6%)]\tLosses bn: 0.492975 drop: 0.554194 plain: 0.510957\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 244 [0/801 (0%)]\tLosses bn: 0.292806 drop: 0.327187 plain: 0.311865\n",
      "Train Epoch: 244 [800/801 (6%)]\tLosses bn: 0.492707 drop: 0.484050 plain: 0.510683\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 245 [0/801 (0%)]\tLosses bn: 0.292627 drop: 0.370562 plain: 0.311628\n",
      "Train Epoch: 245 [800/801 (6%)]\tLosses bn: 0.492420 drop: 0.479042 plain: 0.510402\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 246 [0/801 (0%)]\tLosses bn: 0.292445 drop: 0.342380 plain: 0.311403\n",
      "Train Epoch: 246 [800/801 (6%)]\tLosses bn: 0.492110 drop: 0.673633 plain: 0.510143\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0200\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 247 [0/801 (0%)]\tLosses bn: 0.292259 drop: 0.309707 plain: 0.311173\n",
      "Train Epoch: 247 [800/801 (6%)]\tLosses bn: 0.491815 drop: 0.496456 plain: 0.509886\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 248 [0/801 (0%)]\tLosses bn: 0.292082 drop: 0.348905 plain: 0.310944\n",
      "Train Epoch: 248 [800/801 (6%)]\tLosses bn: 0.491524 drop: 0.559084 plain: 0.509601\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0200\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 249 [0/801 (0%)]\tLosses bn: 0.291903 drop: 0.388355 plain: 0.310720\n",
      "Train Epoch: 249 [800/801 (6%)]\tLosses bn: 0.491226 drop: 0.489243 plain: 0.509346\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 250 [0/801 (0%)]\tLosses bn: 0.291747 drop: 0.331223 plain: 0.310494\n",
      "Train Epoch: 250 [800/801 (6%)]\tLosses bn: 0.490943 drop: 0.613576 plain: 0.509084\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 251 [0/801 (0%)]\tLosses bn: 0.291572 drop: 0.313287 plain: 0.310256\n",
      "Train Epoch: 251 [800/801 (6%)]\tLosses bn: 0.490668 drop: 0.500237 plain: 0.508816\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 252 [0/801 (0%)]\tLosses bn: 0.291380 drop: 0.376826 plain: 0.310048\n",
      "Train Epoch: 252 [800/801 (6%)]\tLosses bn: 0.490389 drop: 0.490845 plain: 0.508572\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 253 [0/801 (0%)]\tLosses bn: 0.291199 drop: 0.369294 plain: 0.309827\n",
      "Train Epoch: 253 [800/801 (6%)]\tLosses bn: 0.490145 drop: 0.622503 plain: 0.508310\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 254 [0/801 (0%)]\tLosses bn: 0.291031 drop: 0.377397 plain: 0.309632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 254 [800/801 (6%)]\tLosses bn: 0.489901 drop: 0.553583 plain: 0.508067\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 255 [0/801 (0%)]\tLosses bn: 0.290862 drop: 0.303735 plain: 0.309399\n",
      "Train Epoch: 255 [800/801 (6%)]\tLosses bn: 0.489664 drop: 0.593853 plain: 0.507829\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 256 [0/801 (0%)]\tLosses bn: 0.290715 drop: 0.297000 plain: 0.309190\n",
      "Train Epoch: 256 [800/801 (6%)]\tLosses bn: 0.489455 drop: 0.598621 plain: 0.507578\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0199\tAccuracy: 68.0/90 (76%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 257 [0/801 (0%)]\tLosses bn: 0.290551 drop: 0.392351 plain: 0.308972\n",
      "Train Epoch: 257 [800/801 (6%)]\tLosses bn: 0.489226 drop: 0.626176 plain: 0.507349\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 258 [0/801 (0%)]\tLosses bn: 0.290382 drop: 0.380247 plain: 0.308755\n",
      "Train Epoch: 258 [800/801 (6%)]\tLosses bn: 0.488967 drop: 0.538406 plain: 0.507109\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0199\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 259 [0/801 (0%)]\tLosses bn: 0.290201 drop: 0.324828 plain: 0.308559\n",
      "Train Epoch: 259 [800/801 (6%)]\tLosses bn: 0.488756 drop: 0.567900 plain: 0.506867\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 260 [0/801 (0%)]\tLosses bn: 0.290013 drop: 0.313062 plain: 0.308348\n",
      "Train Epoch: 260 [800/801 (6%)]\tLosses bn: 0.488522 drop: 0.587844 plain: 0.506621\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 261 [0/801 (0%)]\tLosses bn: 0.289844 drop: 0.312601 plain: 0.308151\n",
      "Train Epoch: 261 [800/801 (6%)]\tLosses bn: 0.488280 drop: 0.530558 plain: 0.506356\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 262 [0/801 (0%)]\tLosses bn: 0.289651 drop: 0.373713 plain: 0.307922\n",
      "Train Epoch: 262 [800/801 (6%)]\tLosses bn: 0.488053 drop: 0.612726 plain: 0.506107\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 263 [0/801 (0%)]\tLosses bn: 0.289491 drop: 0.335917 plain: 0.307704\n",
      "Train Epoch: 263 [800/801 (6%)]\tLosses bn: 0.487849 drop: 0.534493 plain: 0.505853\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 264 [0/801 (0%)]\tLosses bn: 0.289338 drop: 0.372606 plain: 0.307478\n",
      "Train Epoch: 264 [800/801 (6%)]\tLosses bn: 0.487612 drop: 0.410491 plain: 0.505623\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 265 [0/801 (0%)]\tLosses bn: 0.289169 drop: 0.276153 plain: 0.307261\n",
      "Train Epoch: 265 [800/801 (6%)]\tLosses bn: 0.487388 drop: 0.504707 plain: 0.505356\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 266 [0/801 (0%)]\tLosses bn: 0.289032 drop: 0.344789 plain: 0.307054\n",
      "Train Epoch: 266 [800/801 (6%)]\tLosses bn: 0.487152 drop: 0.557649 plain: 0.505116\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 267 [0/801 (0%)]\tLosses bn: 0.288891 drop: 0.325117 plain: 0.306846\n",
      "Train Epoch: 267 [800/801 (6%)]\tLosses bn: 0.486942 drop: 0.570549 plain: 0.504853\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 268 [0/801 (0%)]\tLosses bn: 0.288727 drop: 0.359689 plain: 0.306636\n",
      "Train Epoch: 268 [800/801 (6%)]\tLosses bn: 0.486734 drop: 0.523137 plain: 0.504618\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 269 [0/801 (0%)]\tLosses bn: 0.288565 drop: 0.325725 plain: 0.306423\n",
      "Train Epoch: 269 [800/801 (6%)]\tLosses bn: 0.486517 drop: 0.667399 plain: 0.504361\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 270 [0/801 (0%)]\tLosses bn: 0.288403 drop: 0.303812 plain: 0.306225\n",
      "Train Epoch: 270 [800/801 (6%)]\tLosses bn: 0.486304 drop: 0.520679 plain: 0.504131\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0198\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 271 [0/801 (0%)]\tLosses bn: 0.288229 drop: 0.359205 plain: 0.306010\n",
      "Train Epoch: 271 [800/801 (6%)]\tLosses bn: 0.486123 drop: 0.499060 plain: 0.503883\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 272 [0/801 (0%)]\tLosses bn: 0.288075 drop: 0.356164 plain: 0.305795\n",
      "Train Epoch: 272 [800/801 (6%)]\tLosses bn: 0.485946 drop: 0.574591 plain: 0.503636\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 273 [0/801 (0%)]\tLosses bn: 0.287917 drop: 0.325778 plain: 0.305597\n",
      "Train Epoch: 273 [800/801 (6%)]\tLosses bn: 0.485769 drop: 0.560414 plain: 0.503402\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 274 [0/801 (0%)]\tLosses bn: 0.287769 drop: 0.314656 plain: 0.305380\n",
      "Train Epoch: 274 [800/801 (6%)]\tLosses bn: 0.485565 drop: 0.558323 plain: 0.503159\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 275 [0/801 (0%)]\tLosses bn: 0.287619 drop: 0.337866 plain: 0.305180\n",
      "Train Epoch: 275 [800/801 (6%)]\tLosses bn: 0.485384 drop: 0.531702 plain: 0.502948\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 276 [0/801 (0%)]\tLosses bn: 0.287457 drop: 0.373888 plain: 0.304965\n",
      "Train Epoch: 276 [800/801 (6%)]\tLosses bn: 0.485178 drop: 0.485868 plain: 0.502716\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 277 [0/801 (0%)]\tLosses bn: 0.287292 drop: 0.260190 plain: 0.304767\n",
      "Train Epoch: 277 [800/801 (6%)]\tLosses bn: 0.484964 drop: 0.594048 plain: 0.502499\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 278 [0/801 (0%)]\tLosses bn: 0.287130 drop: 0.354054 plain: 0.304565\n",
      "Train Epoch: 278 [800/801 (6%)]\tLosses bn: 0.484766 drop: 0.533110 plain: 0.502277\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 279 [0/801 (0%)]\tLosses bn: 0.286967 drop: 0.354887 plain: 0.304369\n",
      "Train Epoch: 279 [800/801 (6%)]\tLosses bn: 0.484553 drop: 0.532128 plain: 0.502053\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 280 [0/801 (0%)]\tLosses bn: 0.286809 drop: 0.368719 plain: 0.304164\n",
      "Train Epoch: 280 [800/801 (6%)]\tLosses bn: 0.484372 drop: 0.457802 plain: 0.501842\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 281 [0/801 (0%)]\tLosses bn: 0.286641 drop: 0.322810 plain: 0.303980\n",
      "Train Epoch: 281 [800/801 (6%)]\tLosses bn: 0.484192 drop: 0.528907 plain: 0.501634\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0195\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 282 [0/801 (0%)]\tLosses bn: 0.286494 drop: 0.403360 plain: 0.303772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 282 [800/801 (6%)]\tLosses bn: 0.483998 drop: 0.553841 plain: 0.501419\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 283 [0/801 (0%)]\tLosses bn: 0.286319 drop: 0.329720 plain: 0.303592\n",
      "Train Epoch: 283 [800/801 (6%)]\tLosses bn: 0.483832 drop: 0.480130 plain: 0.501223\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 284 [0/801 (0%)]\tLosses bn: 0.286192 drop: 0.317067 plain: 0.303414\n",
      "Train Epoch: 284 [800/801 (6%)]\tLosses bn: 0.483633 drop: 0.503104 plain: 0.501006\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 285 [0/801 (0%)]\tLosses bn: 0.286030 drop: 0.335315 plain: 0.303229\n",
      "Train Epoch: 285 [800/801 (6%)]\tLosses bn: 0.483435 drop: 0.564781 plain: 0.500807\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 286 [0/801 (0%)]\tLosses bn: 0.285888 drop: 0.311587 plain: 0.303076\n",
      "Train Epoch: 286 [800/801 (6%)]\tLosses bn: 0.483235 drop: 0.504351 plain: 0.500603\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 287 [0/801 (0%)]\tLosses bn: 0.285733 drop: 0.392420 plain: 0.302898\n",
      "Train Epoch: 287 [800/801 (6%)]\tLosses bn: 0.483054 drop: 0.580593 plain: 0.500412\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 288 [0/801 (0%)]\tLosses bn: 0.285584 drop: 0.356184 plain: 0.302737\n",
      "Train Epoch: 288 [800/801 (6%)]\tLosses bn: 0.482852 drop: 0.484061 plain: 0.500210\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 289 [0/801 (0%)]\tLosses bn: 0.285431 drop: 0.333060 plain: 0.302541\n",
      "Train Epoch: 289 [800/801 (6%)]\tLosses bn: 0.482660 drop: 0.461049 plain: 0.499992\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 290 [0/801 (0%)]\tLosses bn: 0.285281 drop: 0.297629 plain: 0.302350\n",
      "Train Epoch: 290 [800/801 (6%)]\tLosses bn: 0.482516 drop: 0.497750 plain: 0.499797\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 291 [0/801 (0%)]\tLosses bn: 0.285150 drop: 0.334642 plain: 0.302162\n",
      "Train Epoch: 291 [800/801 (6%)]\tLosses bn: 0.482377 drop: 0.558533 plain: 0.499575\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 292 [0/801 (0%)]\tLosses bn: 0.285034 drop: 0.351186 plain: 0.301968\n",
      "Train Epoch: 292 [800/801 (6%)]\tLosses bn: 0.482248 drop: 0.468043 plain: 0.499391\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 293 [0/801 (0%)]\tLosses bn: 0.284896 drop: 0.376350 plain: 0.301784\n",
      "Train Epoch: 293 [800/801 (6%)]\tLosses bn: 0.482114 drop: 0.568390 plain: 0.499184\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 294 [0/801 (0%)]\tLosses bn: 0.284773 drop: 0.381835 plain: 0.301582\n",
      "Train Epoch: 294 [800/801 (6%)]\tLosses bn: 0.481956 drop: 0.621247 plain: 0.498992\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 295 [0/801 (0%)]\tLosses bn: 0.284636 drop: 0.339350 plain: 0.301399\n",
      "Train Epoch: 295 [800/801 (6%)]\tLosses bn: 0.481813 drop: 0.525341 plain: 0.498814\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 296 [0/801 (0%)]\tLosses bn: 0.284521 drop: 0.321544 plain: 0.301218\n",
      "Train Epoch: 296 [800/801 (6%)]\tLosses bn: 0.481663 drop: 0.453335 plain: 0.498603\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 297 [0/801 (0%)]\tLosses bn: 0.284395 drop: 0.359867 plain: 0.301030\n",
      "Train Epoch: 297 [800/801 (6%)]\tLosses bn: 0.481514 drop: 0.500584 plain: 0.498414\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 298 [0/801 (0%)]\tLosses bn: 0.284272 drop: 0.348383 plain: 0.300854\n",
      "Train Epoch: 298 [800/801 (6%)]\tLosses bn: 0.481349 drop: 0.476812 plain: 0.498224\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 299 [0/801 (0%)]\tLosses bn: 0.284146 drop: 0.317658 plain: 0.300661\n",
      "Train Epoch: 299 [800/801 (6%)]\tLosses bn: 0.481197 drop: 0.449353 plain: 0.498042\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 300 [0/801 (0%)]\tLosses bn: 0.284015 drop: 0.325087 plain: 0.300488\n",
      "Train Epoch: 300 [800/801 (6%)]\tLosses bn: 0.481016 drop: 0.505848 plain: 0.497822\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 301 [0/801 (0%)]\tLosses bn: 0.283877 drop: 0.337468 plain: 0.300311\n",
      "Train Epoch: 301 [800/801 (6%)]\tLosses bn: 0.480836 drop: 0.555234 plain: 0.497631\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 302 [0/801 (0%)]\tLosses bn: 0.283740 drop: 0.376006 plain: 0.300138\n",
      "Train Epoch: 302 [800/801 (6%)]\tLosses bn: 0.480695 drop: 0.490231 plain: 0.497438\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 303 [0/801 (0%)]\tLosses bn: 0.283611 drop: 0.361365 plain: 0.299962\n",
      "Train Epoch: 303 [800/801 (6%)]\tLosses bn: 0.480524 drop: 0.552252 plain: 0.497204\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 304 [0/801 (0%)]\tLosses bn: 0.283486 drop: 0.381026 plain: 0.299782\n",
      "Train Epoch: 304 [800/801 (6%)]\tLosses bn: 0.480372 drop: 0.419323 plain: 0.496994\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 305 [0/801 (0%)]\tLosses bn: 0.283363 drop: 0.372999 plain: 0.299609\n",
      "Train Epoch: 305 [800/801 (6%)]\tLosses bn: 0.480209 drop: 0.608544 plain: 0.496791\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 306 [0/801 (0%)]\tLosses bn: 0.283253 drop: 0.322689 plain: 0.299431\n",
      "Train Epoch: 306 [800/801 (6%)]\tLosses bn: 0.480099 drop: 0.481041 plain: 0.496600\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 307 [0/801 (0%)]\tLosses bn: 0.283140 drop: 0.308028 plain: 0.299257\n",
      "Train Epoch: 307 [800/801 (6%)]\tLosses bn: 0.479980 drop: 0.487995 plain: 0.496388\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 308 [0/801 (0%)]\tLosses bn: 0.283015 drop: 0.371878 plain: 0.299077\n",
      "Train Epoch: 308 [800/801 (6%)]\tLosses bn: 0.479833 drop: 0.510442 plain: 0.496182\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 309 [0/801 (0%)]\tLosses bn: 0.282875 drop: 0.385156 plain: 0.298910\n",
      "Train Epoch: 309 [800/801 (6%)]\tLosses bn: 0.479682 drop: 0.543634 plain: 0.495972\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 310 [0/801 (0%)]\tLosses bn: 0.282760 drop: 0.368223 plain: 0.298724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 310 [800/801 (6%)]\tLosses bn: 0.479513 drop: 0.525654 plain: 0.495767\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 311 [0/801 (0%)]\tLosses bn: 0.282609 drop: 0.334823 plain: 0.298545\n",
      "Train Epoch: 311 [800/801 (6%)]\tLosses bn: 0.479325 drop: 0.447398 plain: 0.495576\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 312 [0/801 (0%)]\tLosses bn: 0.282476 drop: 0.331544 plain: 0.298361\n",
      "Train Epoch: 312 [800/801 (6%)]\tLosses bn: 0.479158 drop: 0.598873 plain: 0.495376\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 313 [0/801 (0%)]\tLosses bn: 0.282337 drop: 0.259804 plain: 0.298189\n",
      "Train Epoch: 313 [800/801 (6%)]\tLosses bn: 0.478974 drop: 0.553851 plain: 0.495184\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 314 [0/801 (0%)]\tLosses bn: 0.282198 drop: 0.321987 plain: 0.298005\n",
      "Train Epoch: 314 [800/801 (6%)]\tLosses bn: 0.478815 drop: 0.584118 plain: 0.494990\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 315 [0/801 (0%)]\tLosses bn: 0.282065 drop: 0.358471 plain: 0.297836\n",
      "Train Epoch: 315 [800/801 (6%)]\tLosses bn: 0.478631 drop: 0.503019 plain: 0.494798\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 316 [0/801 (0%)]\tLosses bn: 0.281922 drop: 0.332928 plain: 0.297656\n",
      "Train Epoch: 316 [800/801 (6%)]\tLosses bn: 0.478446 drop: 0.497673 plain: 0.494609\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 317 [0/801 (0%)]\tLosses bn: 0.281769 drop: 0.315361 plain: 0.297495\n",
      "Train Epoch: 317 [800/801 (6%)]\tLosses bn: 0.478255 drop: 0.506576 plain: 0.494416\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0196\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 318 [0/801 (0%)]\tLosses bn: 0.281619 drop: 0.346511 plain: 0.297331\n",
      "Train Epoch: 318 [800/801 (6%)]\tLosses bn: 0.478079 drop: 0.567108 plain: 0.494228\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 319 [0/801 (0%)]\tLosses bn: 0.281457 drop: 0.337095 plain: 0.297173\n",
      "Train Epoch: 319 [800/801 (6%)]\tLosses bn: 0.477898 drop: 0.535599 plain: 0.494049\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 320 [0/801 (0%)]\tLosses bn: 0.281302 drop: 0.350540 plain: 0.297004\n",
      "Train Epoch: 320 [800/801 (6%)]\tLosses bn: 0.477732 drop: 0.522271 plain: 0.493875\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 321 [0/801 (0%)]\tLosses bn: 0.281150 drop: 0.276265 plain: 0.296831\n",
      "Train Epoch: 321 [800/801 (6%)]\tLosses bn: 0.477542 drop: 0.439837 plain: 0.493672\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 322 [0/801 (0%)]\tLosses bn: 0.280981 drop: 0.305488 plain: 0.296672\n",
      "Train Epoch: 322 [800/801 (6%)]\tLosses bn: 0.477387 drop: 0.562632 plain: 0.493499\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 323 [0/801 (0%)]\tLosses bn: 0.280841 drop: 0.346810 plain: 0.296518\n",
      "Train Epoch: 323 [800/801 (6%)]\tLosses bn: 0.477203 drop: 0.583851 plain: 0.493308\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 324 [0/801 (0%)]\tLosses bn: 0.280690 drop: 0.367568 plain: 0.296354\n",
      "Train Epoch: 324 [800/801 (6%)]\tLosses bn: 0.477036 drop: 0.492954 plain: 0.493134\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 325 [0/801 (0%)]\tLosses bn: 0.280540 drop: 0.351819 plain: 0.296181\n",
      "Train Epoch: 325 [800/801 (6%)]\tLosses bn: 0.476854 drop: 0.556689 plain: 0.492930\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 326 [0/801 (0%)]\tLosses bn: 0.280383 drop: 0.335422 plain: 0.296032\n",
      "Train Epoch: 326 [800/801 (6%)]\tLosses bn: 0.476697 drop: 0.631667 plain: 0.492746\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 327 [0/801 (0%)]\tLosses bn: 0.280236 drop: 0.322551 plain: 0.295873\n",
      "Train Epoch: 327 [800/801 (6%)]\tLosses bn: 0.476526 drop: 0.591942 plain: 0.492546\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 328 [0/801 (0%)]\tLosses bn: 0.280095 drop: 0.341403 plain: 0.295707\n",
      "Train Epoch: 328 [800/801 (6%)]\tLosses bn: 0.476384 drop: 0.558950 plain: 0.492355\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 329 [0/801 (0%)]\tLosses bn: 0.279948 drop: 0.334596 plain: 0.295534\n",
      "Train Epoch: 329 [800/801 (6%)]\tLosses bn: 0.476216 drop: 0.549253 plain: 0.492161\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 330 [0/801 (0%)]\tLosses bn: 0.279810 drop: 0.322687 plain: 0.295369\n",
      "Train Epoch: 330 [800/801 (6%)]\tLosses bn: 0.476046 drop: 0.543774 plain: 0.491990\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 331 [0/801 (0%)]\tLosses bn: 0.279676 drop: 0.368316 plain: 0.295216\n",
      "Train Epoch: 331 [800/801 (6%)]\tLosses bn: 0.475867 drop: 0.527451 plain: 0.491809\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 332 [0/801 (0%)]\tLosses bn: 0.279520 drop: 0.317952 plain: 0.295038\n",
      "Train Epoch: 332 [800/801 (6%)]\tLosses bn: 0.475715 drop: 0.456384 plain: 0.491621\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 333 [0/801 (0%)]\tLosses bn: 0.279395 drop: 0.344663 plain: 0.294878\n",
      "Train Epoch: 333 [800/801 (6%)]\tLosses bn: 0.475577 drop: 0.512205 plain: 0.491422\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 334 [0/801 (0%)]\tLosses bn: 0.279262 drop: 0.400817 plain: 0.294716\n",
      "Train Epoch: 334 [800/801 (6%)]\tLosses bn: 0.475445 drop: 0.441305 plain: 0.491229\n",
      "Test set:\n",
      "bn: Loss: 0.0197\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 335 [0/801 (0%)]\tLosses bn: 0.279141 drop: 0.381242 plain: 0.294556\n",
      "Train Epoch: 335 [800/801 (6%)]\tLosses bn: 0.475300 drop: 0.472316 plain: 0.491019\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 336 [0/801 (0%)]\tLosses bn: 0.279026 drop: 0.321804 plain: 0.294417\n",
      "Train Epoch: 336 [800/801 (6%)]\tLosses bn: 0.475152 drop: 0.501427 plain: 0.490825\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 337 [0/801 (0%)]\tLosses bn: 0.278898 drop: 0.328513 plain: 0.294281\n",
      "Train Epoch: 337 [800/801 (6%)]\tLosses bn: 0.475008 drop: 0.443013 plain: 0.490624\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 338 [0/801 (0%)]\tLosses bn: 0.278780 drop: 0.316175 plain: 0.294151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 338 [800/801 (6%)]\tLosses bn: 0.474855 drop: 0.640385 plain: 0.490435\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 339 [0/801 (0%)]\tLosses bn: 0.278651 drop: 0.339809 plain: 0.294026\n",
      "Train Epoch: 339 [800/801 (6%)]\tLosses bn: 0.474715 drop: 0.467453 plain: 0.490227\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 340 [0/801 (0%)]\tLosses bn: 0.278544 drop: 0.335023 plain: 0.293912\n",
      "Train Epoch: 340 [800/801 (6%)]\tLosses bn: 0.474520 drop: 0.600585 plain: 0.490011\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 341 [0/801 (0%)]\tLosses bn: 0.278432 drop: 0.325064 plain: 0.293793\n",
      "Train Epoch: 341 [800/801 (6%)]\tLosses bn: 0.474354 drop: 0.460519 plain: 0.489812\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 342 [0/801 (0%)]\tLosses bn: 0.278315 drop: 0.320789 plain: 0.293668\n",
      "Train Epoch: 342 [800/801 (6%)]\tLosses bn: 0.474243 drop: 0.567923 plain: 0.489589\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 68.0/90 (76%)\n",
      "\n",
      "Train Epoch: 343 [0/801 (0%)]\tLosses bn: 0.278195 drop: 0.334374 plain: 0.293529\n",
      "Train Epoch: 343 [800/801 (6%)]\tLosses bn: 0.474105 drop: 0.504457 plain: 0.489395\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 344 [0/801 (0%)]\tLosses bn: 0.278085 drop: 0.303188 plain: 0.293386\n",
      "Train Epoch: 344 [800/801 (6%)]\tLosses bn: 0.473974 drop: 0.476694 plain: 0.489184\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 345 [0/801 (0%)]\tLosses bn: 0.277979 drop: 0.297366 plain: 0.293251\n",
      "Train Epoch: 345 [800/801 (6%)]\tLosses bn: 0.473826 drop: 0.569066 plain: 0.488992\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 346 [0/801 (0%)]\tLosses bn: 0.277867 drop: 0.322762 plain: 0.293129\n",
      "Train Epoch: 346 [800/801 (6%)]\tLosses bn: 0.473670 drop: 0.544721 plain: 0.488779\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 347 [0/801 (0%)]\tLosses bn: 0.277756 drop: 0.339829 plain: 0.292984\n",
      "Train Epoch: 347 [800/801 (6%)]\tLosses bn: 0.473524 drop: 0.531013 plain: 0.488554\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 348 [0/801 (0%)]\tLosses bn: 0.277647 drop: 0.347840 plain: 0.292871\n",
      "Train Epoch: 348 [800/801 (6%)]\tLosses bn: 0.473401 drop: 0.489955 plain: 0.488322\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 349 [0/801 (0%)]\tLosses bn: 0.277531 drop: 0.346698 plain: 0.292743\n",
      "Train Epoch: 349 [800/801 (6%)]\tLosses bn: 0.473265 drop: 0.569757 plain: 0.488104\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 350 [0/801 (0%)]\tLosses bn: 0.277428 drop: 0.327380 plain: 0.292630\n",
      "Train Epoch: 350 [800/801 (6%)]\tLosses bn: 0.473099 drop: 0.507255 plain: 0.487870\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 351 [0/801 (0%)]\tLosses bn: 0.277317 drop: 0.312493 plain: 0.292518\n",
      "Train Epoch: 351 [800/801 (6%)]\tLosses bn: 0.472940 drop: 0.587649 plain: 0.487626\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 352 [0/801 (0%)]\tLosses bn: 0.277187 drop: 0.382014 plain: 0.292402\n",
      "Train Epoch: 352 [800/801 (6%)]\tLosses bn: 0.472802 drop: 0.573349 plain: 0.487394\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 353 [0/801 (0%)]\tLosses bn: 0.277086 drop: 0.377024 plain: 0.292244\n",
      "Train Epoch: 353 [800/801 (6%)]\tLosses bn: 0.472673 drop: 0.476549 plain: 0.487180\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 354 [0/801 (0%)]\tLosses bn: 0.276958 drop: 0.323558 plain: 0.292120\n",
      "Train Epoch: 354 [800/801 (6%)]\tLosses bn: 0.472494 drop: 0.491313 plain: 0.486947\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 355 [0/801 (0%)]\tLosses bn: 0.276847 drop: 0.337286 plain: 0.291993\n",
      "Train Epoch: 355 [800/801 (6%)]\tLosses bn: 0.472361 drop: 0.503625 plain: 0.486741\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 356 [0/801 (0%)]\tLosses bn: 0.276747 drop: 0.349172 plain: 0.291880\n",
      "Train Epoch: 356 [800/801 (6%)]\tLosses bn: 0.472216 drop: 0.558546 plain: 0.486528\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 357 [0/801 (0%)]\tLosses bn: 0.276640 drop: 0.349829 plain: 0.291737\n",
      "Train Epoch: 357 [800/801 (6%)]\tLosses bn: 0.472073 drop: 0.477877 plain: 0.486311\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 358 [0/801 (0%)]\tLosses bn: 0.276544 drop: 0.352978 plain: 0.291619\n",
      "Train Epoch: 358 [800/801 (6%)]\tLosses bn: 0.471955 drop: 0.566741 plain: 0.486120\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 359 [0/801 (0%)]\tLosses bn: 0.276476 drop: 0.329377 plain: 0.291455\n",
      "Train Epoch: 359 [800/801 (6%)]\tLosses bn: 0.471820 drop: 0.441727 plain: 0.485930\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 360 [0/801 (0%)]\tLosses bn: 0.276408 drop: 0.343134 plain: 0.291229\n",
      "Train Epoch: 360 [800/801 (6%)]\tLosses bn: 0.471674 drop: 0.507700 plain: 0.485739\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 361 [0/801 (0%)]\tLosses bn: 0.276347 drop: 0.355470 plain: 0.290989\n",
      "Train Epoch: 361 [800/801 (6%)]\tLosses bn: 0.471609 drop: 0.542096 plain: 0.485537\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 362 [0/801 (0%)]\tLosses bn: 0.276306 drop: 0.341327 plain: 0.290755\n",
      "Train Epoch: 362 [800/801 (6%)]\tLosses bn: 0.471493 drop: 0.476948 plain: 0.485355\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 363 [0/801 (0%)]\tLosses bn: 0.276243 drop: 0.329490 plain: 0.290530\n",
      "Train Epoch: 363 [800/801 (6%)]\tLosses bn: 0.471368 drop: 0.543784 plain: 0.485169\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 364 [0/801 (0%)]\tLosses bn: 0.276193 drop: 0.341115 plain: 0.290297\n",
      "Train Epoch: 364 [800/801 (6%)]\tLosses bn: 0.471209 drop: 0.507236 plain: 0.484957\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 365 [0/801 (0%)]\tLosses bn: 0.276165 drop: 0.354604 plain: 0.290081\n",
      "Train Epoch: 365 [800/801 (6%)]\tLosses bn: 0.471044 drop: 0.540833 plain: 0.484789\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 366 [0/801 (0%)]\tLosses bn: 0.276117 drop: 0.326193 plain: 0.289867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 366 [800/801 (6%)]\tLosses bn: 0.470915 drop: 0.530520 plain: 0.484572\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 367 [0/801 (0%)]\tLosses bn: 0.276070 drop: 0.323348 plain: 0.289650\n",
      "Train Epoch: 367 [800/801 (6%)]\tLosses bn: 0.470742 drop: 0.535706 plain: 0.484398\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 368 [0/801 (0%)]\tLosses bn: 0.276024 drop: 0.261146 plain: 0.289447\n",
      "Train Epoch: 368 [800/801 (6%)]\tLosses bn: 0.470595 drop: 0.577935 plain: 0.484185\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 369 [0/801 (0%)]\tLosses bn: 0.275981 drop: 0.276368 plain: 0.289231\n",
      "Train Epoch: 369 [800/801 (6%)]\tLosses bn: 0.470436 drop: 0.502843 plain: 0.484003\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 370 [0/801 (0%)]\tLosses bn: 0.275940 drop: 0.390392 plain: 0.289010\n",
      "Train Epoch: 370 [800/801 (6%)]\tLosses bn: 0.470277 drop: 0.494492 plain: 0.483819\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 371 [0/801 (0%)]\tLosses bn: 0.275907 drop: 0.327130 plain: 0.288781\n",
      "Train Epoch: 371 [800/801 (6%)]\tLosses bn: 0.470122 drop: 0.554146 plain: 0.483639\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 372 [0/801 (0%)]\tLosses bn: 0.275861 drop: 0.355468 plain: 0.288558\n",
      "Train Epoch: 372 [800/801 (6%)]\tLosses bn: 0.469920 drop: 0.552390 plain: 0.483468\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 373 [0/801 (0%)]\tLosses bn: 0.275803 drop: 0.343881 plain: 0.288338\n",
      "Train Epoch: 373 [800/801 (6%)]\tLosses bn: 0.469758 drop: 0.490173 plain: 0.483295\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 374 [0/801 (0%)]\tLosses bn: 0.275739 drop: 0.285893 plain: 0.288098\n",
      "Train Epoch: 374 [800/801 (6%)]\tLosses bn: 0.469600 drop: 0.529333 plain: 0.483119\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 375 [0/801 (0%)]\tLosses bn: 0.275680 drop: 0.327192 plain: 0.287867\n",
      "Train Epoch: 375 [800/801 (6%)]\tLosses bn: 0.469442 drop: 0.493105 plain: 0.482927\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 376 [0/801 (0%)]\tLosses bn: 0.275606 drop: 0.297929 plain: 0.287631\n",
      "Train Epoch: 376 [800/801 (6%)]\tLosses bn: 0.469279 drop: 0.462390 plain: 0.482756\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 377 [0/801 (0%)]\tLosses bn: 0.275571 drop: 0.285495 plain: 0.287414\n",
      "Train Epoch: 377 [800/801 (6%)]\tLosses bn: 0.469096 drop: 0.487806 plain: 0.482577\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 378 [0/801 (0%)]\tLosses bn: 0.275512 drop: 0.338776 plain: 0.287195\n",
      "Train Epoch: 378 [800/801 (6%)]\tLosses bn: 0.468906 drop: 0.484719 plain: 0.482389\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 379 [0/801 (0%)]\tLosses bn: 0.275492 drop: 0.271328 plain: 0.286977\n",
      "Train Epoch: 379 [800/801 (6%)]\tLosses bn: 0.468707 drop: 0.456451 plain: 0.482225\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 380 [0/801 (0%)]\tLosses bn: 0.275436 drop: 0.285666 plain: 0.286757\n",
      "Train Epoch: 380 [800/801 (6%)]\tLosses bn: 0.468542 drop: 0.550097 plain: 0.482064\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 381 [0/801 (0%)]\tLosses bn: 0.275385 drop: 0.335716 plain: 0.286531\n",
      "Train Epoch: 381 [800/801 (6%)]\tLosses bn: 0.468339 drop: 0.503755 plain: 0.481876\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 382 [0/801 (0%)]\tLosses bn: 0.275337 drop: 0.379091 plain: 0.286321\n",
      "Train Epoch: 382 [800/801 (6%)]\tLosses bn: 0.468191 drop: 0.598516 plain: 0.481723\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 383 [0/801 (0%)]\tLosses bn: 0.275282 drop: 0.353494 plain: 0.286116\n",
      "Train Epoch: 383 [800/801 (6%)]\tLosses bn: 0.468021 drop: 0.613753 plain: 0.481534\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 384 [0/801 (0%)]\tLosses bn: 0.275259 drop: 0.333458 plain: 0.285916\n",
      "Train Epoch: 384 [800/801 (6%)]\tLosses bn: 0.467838 drop: 0.545464 plain: 0.481383\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 385 [0/801 (0%)]\tLosses bn: 0.275189 drop: 0.305273 plain: 0.285722\n",
      "Train Epoch: 385 [800/801 (6%)]\tLosses bn: 0.467650 drop: 0.463270 plain: 0.481175\n",
      "Test set:\n",
      "bn: Loss: 0.0198\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 386 [0/801 (0%)]\tLosses bn: 0.275148 drop: 0.413637 plain: 0.285523\n",
      "Train Epoch: 386 [800/801 (6%)]\tLosses bn: 0.467484 drop: 0.543943 plain: 0.480990\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 387 [0/801 (0%)]\tLosses bn: 0.275098 drop: 0.376098 plain: 0.285312\n",
      "Train Epoch: 387 [800/801 (6%)]\tLosses bn: 0.467273 drop: 0.534188 plain: 0.480800\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 388 [0/801 (0%)]\tLosses bn: 0.275067 drop: 0.340704 plain: 0.285116\n",
      "Train Epoch: 388 [800/801 (6%)]\tLosses bn: 0.467070 drop: 0.495538 plain: 0.480615\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 389 [0/801 (0%)]\tLosses bn: 0.275008 drop: 0.324384 plain: 0.284925\n",
      "Train Epoch: 389 [800/801 (6%)]\tLosses bn: 0.466882 drop: 0.556813 plain: 0.480389\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 390 [0/801 (0%)]\tLosses bn: 0.275009 drop: 0.300661 plain: 0.284735\n",
      "Train Epoch: 390 [800/801 (6%)]\tLosses bn: 0.466697 drop: 0.533532 plain: 0.480203\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 391 [0/801 (0%)]\tLosses bn: 0.274993 drop: 0.274247 plain: 0.284551\n",
      "Train Epoch: 391 [800/801 (6%)]\tLosses bn: 0.466477 drop: 0.595861 plain: 0.480024\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 392 [0/801 (0%)]\tLosses bn: 0.274972 drop: 0.326078 plain: 0.284359\n",
      "Train Epoch: 392 [800/801 (6%)]\tLosses bn: 0.466281 drop: 0.467989 plain: 0.479799\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 393 [0/801 (0%)]\tLosses bn: 0.274918 drop: 0.302427 plain: 0.284169\n",
      "Train Epoch: 393 [800/801 (6%)]\tLosses bn: 0.466129 drop: 0.457856 plain: 0.479630\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 394 [0/801 (0%)]\tLosses bn: 0.274901 drop: 0.345304 plain: 0.284004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 394 [800/801 (6%)]\tLosses bn: 0.465917 drop: 0.566244 plain: 0.479419\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 395 [0/801 (0%)]\tLosses bn: 0.274845 drop: 0.289346 plain: 0.283843\n",
      "Train Epoch: 395 [800/801 (6%)]\tLosses bn: 0.465767 drop: 0.514426 plain: 0.479225\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 396 [0/801 (0%)]\tLosses bn: 0.274816 drop: 0.330499 plain: 0.283667\n",
      "Train Epoch: 396 [800/801 (6%)]\tLosses bn: 0.465577 drop: 0.546904 plain: 0.479026\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 397 [0/801 (0%)]\tLosses bn: 0.274766 drop: 0.342568 plain: 0.283509\n",
      "Train Epoch: 397 [800/801 (6%)]\tLosses bn: 0.465380 drop: 0.469319 plain: 0.478835\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 398 [0/801 (0%)]\tLosses bn: 0.274744 drop: 0.302484 plain: 0.283342\n",
      "Train Epoch: 398 [800/801 (6%)]\tLosses bn: 0.465216 drop: 0.481749 plain: 0.478648\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 399 [0/801 (0%)]\tLosses bn: 0.274678 drop: 0.345385 plain: 0.283161\n",
      "Train Epoch: 399 [800/801 (6%)]\tLosses bn: 0.465040 drop: 0.502939 plain: 0.478453\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 400 [0/801 (0%)]\tLosses bn: 0.274644 drop: 0.373055 plain: 0.282998\n",
      "Train Epoch: 400 [800/801 (6%)]\tLosses bn: 0.464844 drop: 0.480099 plain: 0.478291\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 401 [0/801 (0%)]\tLosses bn: 0.274599 drop: 0.400093 plain: 0.282824\n",
      "Train Epoch: 401 [800/801 (6%)]\tLosses bn: 0.464721 drop: 0.480895 plain: 0.478151\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 402 [0/801 (0%)]\tLosses bn: 0.274573 drop: 0.322115 plain: 0.282652\n",
      "Train Epoch: 402 [800/801 (6%)]\tLosses bn: 0.464548 drop: 0.537843 plain: 0.477979\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 403 [0/801 (0%)]\tLosses bn: 0.274534 drop: 0.339694 plain: 0.282483\n",
      "Train Epoch: 403 [800/801 (6%)]\tLosses bn: 0.464400 drop: 0.531043 plain: 0.477809\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 404 [0/801 (0%)]\tLosses bn: 0.274505 drop: 0.347500 plain: 0.282326\n",
      "Train Epoch: 404 [800/801 (6%)]\tLosses bn: 0.464265 drop: 0.447167 plain: 0.477645\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 405 [0/801 (0%)]\tLosses bn: 0.274467 drop: 0.420871 plain: 0.282173\n",
      "Train Epoch: 405 [800/801 (6%)]\tLosses bn: 0.464116 drop: 0.655118 plain: 0.477488\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 406 [0/801 (0%)]\tLosses bn: 0.274449 drop: 0.278216 plain: 0.282013\n",
      "Train Epoch: 406 [800/801 (6%)]\tLosses bn: 0.463949 drop: 0.529370 plain: 0.477309\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 407 [0/801 (0%)]\tLosses bn: 0.274431 drop: 0.331290 plain: 0.281845\n",
      "Train Epoch: 407 [800/801 (6%)]\tLosses bn: 0.463794 drop: 0.534520 plain: 0.477160\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 408 [0/801 (0%)]\tLosses bn: 0.274389 drop: 0.326689 plain: 0.281676\n",
      "Train Epoch: 408 [800/801 (6%)]\tLosses bn: 0.463613 drop: 0.508556 plain: 0.476995\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0195\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 409 [0/801 (0%)]\tLosses bn: 0.274347 drop: 0.353186 plain: 0.281517\n",
      "Train Epoch: 409 [800/801 (6%)]\tLosses bn: 0.463449 drop: 0.481969 plain: 0.476809\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 410 [0/801 (0%)]\tLosses bn: 0.274335 drop: 0.375818 plain: 0.281355\n",
      "Train Epoch: 410 [800/801 (6%)]\tLosses bn: 0.463280 drop: 0.515833 plain: 0.476677\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 411 [0/801 (0%)]\tLosses bn: 0.274295 drop: 0.377719 plain: 0.281200\n",
      "Train Epoch: 411 [800/801 (6%)]\tLosses bn: 0.463108 drop: 0.484751 plain: 0.476495\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 412 [0/801 (0%)]\tLosses bn: 0.274271 drop: 0.348477 plain: 0.281033\n",
      "Train Epoch: 412 [800/801 (6%)]\tLosses bn: 0.462949 drop: 0.515514 plain: 0.476331\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 413 [0/801 (0%)]\tLosses bn: 0.274245 drop: 0.350969 plain: 0.280884\n",
      "Train Epoch: 413 [800/801 (6%)]\tLosses bn: 0.462774 drop: 0.471274 plain: 0.476179\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 414 [0/801 (0%)]\tLosses bn: 0.274206 drop: 0.395670 plain: 0.280712\n",
      "Train Epoch: 414 [800/801 (6%)]\tLosses bn: 0.462580 drop: 0.522140 plain: 0.476039\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 415 [0/801 (0%)]\tLosses bn: 0.274162 drop: 0.334313 plain: 0.280558\n",
      "Train Epoch: 415 [800/801 (6%)]\tLosses bn: 0.462420 drop: 0.502710 plain: 0.475874\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 416 [0/801 (0%)]\tLosses bn: 0.274124 drop: 0.454143 plain: 0.280409\n",
      "Train Epoch: 416 [800/801 (6%)]\tLosses bn: 0.462248 drop: 0.484865 plain: 0.475701\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 417 [0/801 (0%)]\tLosses bn: 0.274085 drop: 0.327966 plain: 0.280261\n",
      "Train Epoch: 417 [800/801 (6%)]\tLosses bn: 0.462086 drop: 0.521727 plain: 0.475566\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 418 [0/801 (0%)]\tLosses bn: 0.274058 drop: 0.340894 plain: 0.280098\n",
      "Train Epoch: 418 [800/801 (6%)]\tLosses bn: 0.461908 drop: 0.606744 plain: 0.475396\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 419 [0/801 (0%)]\tLosses bn: 0.274014 drop: 0.340760 plain: 0.279960\n",
      "Train Epoch: 419 [800/801 (6%)]\tLosses bn: 0.461749 drop: 0.455137 plain: 0.475241\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 420 [0/801 (0%)]\tLosses bn: 0.274013 drop: 0.299845 plain: 0.279806\n",
      "Train Epoch: 420 [800/801 (6%)]\tLosses bn: 0.461564 drop: 0.516405 plain: 0.475131\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 421 [0/801 (0%)]\tLosses bn: 0.273958 drop: 0.290603 plain: 0.279663\n",
      "Train Epoch: 421 [800/801 (6%)]\tLosses bn: 0.461422 drop: 0.525266 plain: 0.474986\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0194\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 422 [0/801 (0%)]\tLosses bn: 0.273923 drop: 0.317452 plain: 0.279514\n",
      "Train Epoch: 422 [800/801 (6%)]\tLosses bn: 0.461261 drop: 0.517324 plain: 0.474841\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 423 [0/801 (0%)]\tLosses bn: 0.273871 drop: 0.351327 plain: 0.279359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 423 [800/801 (6%)]\tLosses bn: 0.461123 drop: 0.522985 plain: 0.474689\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 424 [0/801 (0%)]\tLosses bn: 0.273838 drop: 0.365858 plain: 0.279219\n",
      "Train Epoch: 424 [800/801 (6%)]\tLosses bn: 0.460989 drop: 0.514212 plain: 0.474544\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 425 [0/801 (0%)]\tLosses bn: 0.273827 drop: 0.350654 plain: 0.279067\n",
      "Train Epoch: 425 [800/801 (6%)]\tLosses bn: 0.460851 drop: 0.444201 plain: 0.474374\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 426 [0/801 (0%)]\tLosses bn: 0.273806 drop: 0.323855 plain: 0.278932\n",
      "Train Epoch: 426 [800/801 (6%)]\tLosses bn: 0.460739 drop: 0.501728 plain: 0.474205\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 427 [0/801 (0%)]\tLosses bn: 0.273782 drop: 0.349560 plain: 0.278796\n",
      "Train Epoch: 427 [800/801 (6%)]\tLosses bn: 0.460567 drop: 0.534516 plain: 0.474038\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 428 [0/801 (0%)]\tLosses bn: 0.273752 drop: 0.380350 plain: 0.278664\n",
      "Train Epoch: 428 [800/801 (6%)]\tLosses bn: 0.460444 drop: 0.505196 plain: 0.473884\n",
      "Test set:\n",
      "bn: Loss: 0.0199\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 429 [0/801 (0%)]\tLosses bn: 0.273723 drop: 0.316885 plain: 0.278508\n",
      "Train Epoch: 429 [800/801 (6%)]\tLosses bn: 0.460287 drop: 0.502438 plain: 0.473729\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 66.0/90 (73%)\n",
      "\n",
      "Train Epoch: 430 [0/801 (0%)]\tLosses bn: 0.273689 drop: 0.349217 plain: 0.278370\n",
      "Train Epoch: 430 [800/801 (6%)]\tLosses bn: 0.460143 drop: 0.555276 plain: 0.473579\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 431 [0/801 (0%)]\tLosses bn: 0.273660 drop: 0.317294 plain: 0.278221\n",
      "Train Epoch: 431 [800/801 (6%)]\tLosses bn: 0.460021 drop: 0.501766 plain: 0.473411\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 432 [0/801 (0%)]\tLosses bn: 0.273630 drop: 0.319093 plain: 0.278081\n",
      "Train Epoch: 432 [800/801 (6%)]\tLosses bn: 0.459880 drop: 0.493325 plain: 0.473247\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 433 [0/801 (0%)]\tLosses bn: 0.273593 drop: 0.295722 plain: 0.277940\n",
      "Train Epoch: 433 [800/801 (6%)]\tLosses bn: 0.459804 drop: 0.559498 plain: 0.473099\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 434 [0/801 (0%)]\tLosses bn: 0.273551 drop: 0.341287 plain: 0.277794\n",
      "Train Epoch: 434 [800/801 (6%)]\tLosses bn: 0.459649 drop: 0.559519 plain: 0.472946\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 435 [0/801 (0%)]\tLosses bn: 0.273475 drop: 0.307144 plain: 0.277657\n",
      "Train Epoch: 435 [800/801 (6%)]\tLosses bn: 0.459511 drop: 0.501689 plain: 0.472791\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 436 [0/801 (0%)]\tLosses bn: 0.273412 drop: 0.357951 plain: 0.277520\n",
      "Train Epoch: 436 [800/801 (6%)]\tLosses bn: 0.459379 drop: 0.521922 plain: 0.472621\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 437 [0/801 (0%)]\tLosses bn: 0.273339 drop: 0.272009 plain: 0.277374\n",
      "Train Epoch: 437 [800/801 (6%)]\tLosses bn: 0.459248 drop: 0.512412 plain: 0.472488\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 438 [0/801 (0%)]\tLosses bn: 0.273278 drop: 0.365049 plain: 0.277223\n",
      "Train Epoch: 438 [800/801 (6%)]\tLosses bn: 0.459090 drop: 0.471192 plain: 0.472326\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 439 [0/801 (0%)]\tLosses bn: 0.273205 drop: 0.309855 plain: 0.277081\n",
      "Train Epoch: 439 [800/801 (6%)]\tLosses bn: 0.458952 drop: 0.515041 plain: 0.472165\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 440 [0/801 (0%)]\tLosses bn: 0.273152 drop: 0.318838 plain: 0.276933\n",
      "Train Epoch: 440 [800/801 (6%)]\tLosses bn: 0.458823 drop: 0.588772 plain: 0.472012\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 441 [0/801 (0%)]\tLosses bn: 0.273077 drop: 0.364259 plain: 0.276791\n",
      "Train Epoch: 441 [800/801 (6%)]\tLosses bn: 0.458668 drop: 0.589775 plain: 0.471854\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 442 [0/801 (0%)]\tLosses bn: 0.273032 drop: 0.268180 plain: 0.276640\n",
      "Train Epoch: 442 [800/801 (6%)]\tLosses bn: 0.458525 drop: 0.569503 plain: 0.471659\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 443 [0/801 (0%)]\tLosses bn: 0.272958 drop: 0.308706 plain: 0.276512\n",
      "Train Epoch: 443 [800/801 (6%)]\tLosses bn: 0.458388 drop: 0.524999 plain: 0.471495\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 444 [0/801 (0%)]\tLosses bn: 0.272895 drop: 0.361558 plain: 0.276378\n",
      "Train Epoch: 444 [800/801 (6%)]\tLosses bn: 0.458240 drop: 0.526825 plain: 0.471341\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 445 [0/801 (0%)]\tLosses bn: 0.272834 drop: 0.308805 plain: 0.276241\n",
      "Train Epoch: 445 [800/801 (6%)]\tLosses bn: 0.458123 drop: 0.464691 plain: 0.471179\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 446 [0/801 (0%)]\tLosses bn: 0.272771 drop: 0.396757 plain: 0.276108\n",
      "Train Epoch: 446 [800/801 (6%)]\tLosses bn: 0.457955 drop: 0.476503 plain: 0.470986\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 447 [0/801 (0%)]\tLosses bn: 0.272694 drop: 0.302082 plain: 0.275970\n",
      "Train Epoch: 447 [800/801 (6%)]\tLosses bn: 0.457838 drop: 0.453384 plain: 0.470801\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 448 [0/801 (0%)]\tLosses bn: 0.272641 drop: 0.316700 plain: 0.275838\n",
      "Train Epoch: 448 [800/801 (6%)]\tLosses bn: 0.457711 drop: 0.464525 plain: 0.470650\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 449 [0/801 (0%)]\tLosses bn: 0.272574 drop: 0.347212 plain: 0.275680\n",
      "Train Epoch: 449 [800/801 (6%)]\tLosses bn: 0.457588 drop: 0.500353 plain: 0.470453\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 450 [0/801 (0%)]\tLosses bn: 0.272522 drop: 0.313026 plain: 0.275538\n",
      "Train Epoch: 450 [800/801 (6%)]\tLosses bn: 0.457460 drop: 0.498912 plain: 0.470298\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 451 [0/801 (0%)]\tLosses bn: 0.272454 drop: 0.303101 plain: 0.275385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 451 [800/801 (6%)]\tLosses bn: 0.457310 drop: 0.518038 plain: 0.470097\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0195\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 452 [0/801 (0%)]\tLosses bn: 0.272393 drop: 0.390574 plain: 0.275244\n",
      "Train Epoch: 452 [800/801 (6%)]\tLosses bn: 0.457230 drop: 0.500650 plain: 0.469911\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 453 [0/801 (0%)]\tLosses bn: 0.272314 drop: 0.349056 plain: 0.275108\n",
      "Train Epoch: 453 [800/801 (6%)]\tLosses bn: 0.457067 drop: 0.532686 plain: 0.469725\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 454 [0/801 (0%)]\tLosses bn: 0.272254 drop: 0.377367 plain: 0.274963\n",
      "Train Epoch: 454 [800/801 (6%)]\tLosses bn: 0.456987 drop: 0.471871 plain: 0.469551\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 455 [0/801 (0%)]\tLosses bn: 0.272199 drop: 0.382211 plain: 0.274817\n",
      "Train Epoch: 455 [800/801 (6%)]\tLosses bn: 0.456856 drop: 0.561158 plain: 0.469375\n",
      "Test set:\n",
      "bn: Loss: 0.0200\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 456 [0/801 (0%)]\tLosses bn: 0.272141 drop: 0.444259 plain: 0.274683\n",
      "Train Epoch: 456 [800/801 (6%)]\tLosses bn: 0.456739 drop: 0.523707 plain: 0.469180\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 457 [0/801 (0%)]\tLosses bn: 0.272084 drop: 0.367036 plain: 0.274530\n",
      "Train Epoch: 457 [800/801 (6%)]\tLosses bn: 0.456632 drop: 0.515953 plain: 0.468976\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 458 [0/801 (0%)]\tLosses bn: 0.272033 drop: 0.303816 plain: 0.274406\n",
      "Train Epoch: 458 [800/801 (6%)]\tLosses bn: 0.456536 drop: 0.558071 plain: 0.468820\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 459 [0/801 (0%)]\tLosses bn: 0.271989 drop: 0.307088 plain: 0.274254\n",
      "Train Epoch: 459 [800/801 (6%)]\tLosses bn: 0.456436 drop: 0.500929 plain: 0.468625\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 460 [0/801 (0%)]\tLosses bn: 0.271939 drop: 0.343487 plain: 0.274104\n",
      "Train Epoch: 460 [800/801 (6%)]\tLosses bn: 0.456301 drop: 0.504953 plain: 0.468441\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 461 [0/801 (0%)]\tLosses bn: 0.271867 drop: 0.338612 plain: 0.273968\n",
      "Train Epoch: 461 [800/801 (6%)]\tLosses bn: 0.456197 drop: 0.496341 plain: 0.468248\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 462 [0/801 (0%)]\tLosses bn: 0.271806 drop: 0.379736 plain: 0.273818\n",
      "Train Epoch: 462 [800/801 (6%)]\tLosses bn: 0.456100 drop: 0.526158 plain: 0.468079\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 463 [0/801 (0%)]\tLosses bn: 0.271738 drop: 0.323658 plain: 0.273685\n",
      "Train Epoch: 463 [800/801 (6%)]\tLosses bn: 0.455963 drop: 0.511541 plain: 0.467919\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 464 [0/801 (0%)]\tLosses bn: 0.271688 drop: 0.339714 plain: 0.273570\n",
      "Train Epoch: 464 [800/801 (6%)]\tLosses bn: 0.455861 drop: 0.561120 plain: 0.467771\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 465 [0/801 (0%)]\tLosses bn: 0.271629 drop: 0.347121 plain: 0.273437\n",
      "Train Epoch: 465 [800/801 (6%)]\tLosses bn: 0.455750 drop: 0.467273 plain: 0.467582\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 466 [0/801 (0%)]\tLosses bn: 0.271580 drop: 0.322267 plain: 0.273314\n",
      "Train Epoch: 466 [800/801 (6%)]\tLosses bn: 0.455706 drop: 0.512823 plain: 0.467436\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 467 [0/801 (0%)]\tLosses bn: 0.271515 drop: 0.361113 plain: 0.273194\n",
      "Train Epoch: 467 [800/801 (6%)]\tLosses bn: 0.455591 drop: 0.472041 plain: 0.467288\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 468 [0/801 (0%)]\tLosses bn: 0.271444 drop: 0.317914 plain: 0.273063\n",
      "Train Epoch: 468 [800/801 (6%)]\tLosses bn: 0.455533 drop: 0.553288 plain: 0.467155\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 469 [0/801 (0%)]\tLosses bn: 0.271393 drop: 0.331606 plain: 0.272946\n",
      "Train Epoch: 469 [800/801 (6%)]\tLosses bn: 0.455422 drop: 0.489955 plain: 0.466992\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 470 [0/801 (0%)]\tLosses bn: 0.271317 drop: 0.386371 plain: 0.272814\n",
      "Train Epoch: 470 [800/801 (6%)]\tLosses bn: 0.455366 drop: 0.559520 plain: 0.466871\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 471 [0/801 (0%)]\tLosses bn: 0.271255 drop: 0.357657 plain: 0.272696\n",
      "Train Epoch: 471 [800/801 (6%)]\tLosses bn: 0.455293 drop: 0.495805 plain: 0.466745\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 472 [0/801 (0%)]\tLosses bn: 0.271201 drop: 0.328963 plain: 0.272560\n",
      "Train Epoch: 472 [800/801 (6%)]\tLosses bn: 0.455184 drop: 0.653219 plain: 0.466614\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 473 [0/801 (0%)]\tLosses bn: 0.271119 drop: 0.354234 plain: 0.272451\n",
      "Train Epoch: 473 [800/801 (6%)]\tLosses bn: 0.455098 drop: 0.539223 plain: 0.466484\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 474 [0/801 (0%)]\tLosses bn: 0.271060 drop: 0.327600 plain: 0.272318\n",
      "Train Epoch: 474 [800/801 (6%)]\tLosses bn: 0.455042 drop: 0.477641 plain: 0.466377\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 475 [0/801 (0%)]\tLosses bn: 0.271002 drop: 0.326478 plain: 0.272191\n",
      "Train Epoch: 475 [800/801 (6%)]\tLosses bn: 0.454962 drop: 0.526393 plain: 0.466230\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 476 [0/801 (0%)]\tLosses bn: 0.270918 drop: 0.322886 plain: 0.272072\n",
      "Train Epoch: 476 [800/801 (6%)]\tLosses bn: 0.454864 drop: 0.493196 plain: 0.466087\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 477 [0/801 (0%)]\tLosses bn: 0.270852 drop: 0.282721 plain: 0.271951\n",
      "Train Epoch: 477 [800/801 (6%)]\tLosses bn: 0.454773 drop: 0.558808 plain: 0.465954\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 478 [0/801 (0%)]\tLosses bn: 0.270799 drop: 0.289708 plain: 0.271836\n",
      "Train Epoch: 478 [800/801 (6%)]\tLosses bn: 0.454668 drop: 0.506357 plain: 0.465824\n",
      "Test set:\n",
      "bn: Loss: 0.0201\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0196\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 479 [0/801 (0%)]\tLosses bn: 0.270706 drop: 0.354621 plain: 0.271716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 479 [800/801 (6%)]\tLosses bn: 0.454617 drop: 0.539331 plain: 0.465668\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 480 [0/801 (0%)]\tLosses bn: 0.270658 drop: 0.281414 plain: 0.271600\n",
      "Train Epoch: 480 [800/801 (6%)]\tLosses bn: 0.454505 drop: 0.447919 plain: 0.465543\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 481 [0/801 (0%)]\tLosses bn: 0.270576 drop: 0.329618 plain: 0.271476\n",
      "Train Epoch: 481 [800/801 (6%)]\tLosses bn: 0.454389 drop: 0.534803 plain: 0.465420\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 482 [0/801 (0%)]\tLosses bn: 0.270518 drop: 0.378093 plain: 0.271357\n",
      "Train Epoch: 482 [800/801 (6%)]\tLosses bn: 0.454258 drop: 0.513142 plain: 0.465293\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 483 [0/801 (0%)]\tLosses bn: 0.270466 drop: 0.297445 plain: 0.271234\n",
      "Train Epoch: 483 [800/801 (6%)]\tLosses bn: 0.454163 drop: 0.477273 plain: 0.465153\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 484 [0/801 (0%)]\tLosses bn: 0.270398 drop: 0.314511 plain: 0.271120\n",
      "Train Epoch: 484 [800/801 (6%)]\tLosses bn: 0.454051 drop: 0.440714 plain: 0.465006\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 485 [0/801 (0%)]\tLosses bn: 0.270351 drop: 0.403440 plain: 0.271016\n",
      "Train Epoch: 485 [800/801 (6%)]\tLosses bn: 0.453928 drop: 0.523166 plain: 0.464891\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 486 [0/801 (0%)]\tLosses bn: 0.270278 drop: 0.332744 plain: 0.270883\n",
      "Train Epoch: 486 [800/801 (6%)]\tLosses bn: 0.453840 drop: 0.497136 plain: 0.464759\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 487 [0/801 (0%)]\tLosses bn: 0.270250 drop: 0.340074 plain: 0.270771\n",
      "Train Epoch: 487 [800/801 (6%)]\tLosses bn: 0.453723 drop: 0.483527 plain: 0.464614\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 488 [0/801 (0%)]\tLosses bn: 0.270191 drop: 0.334508 plain: 0.270646\n",
      "Train Epoch: 488 [800/801 (6%)]\tLosses bn: 0.453581 drop: 0.520466 plain: 0.464482\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 489 [0/801 (0%)]\tLosses bn: 0.270129 drop: 0.386040 plain: 0.270543\n",
      "Train Epoch: 489 [800/801 (6%)]\tLosses bn: 0.453484 drop: 0.524600 plain: 0.464398\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 490 [0/801 (0%)]\tLosses bn: 0.270086 drop: 0.354800 plain: 0.270436\n",
      "Train Epoch: 490 [800/801 (6%)]\tLosses bn: 0.453359 drop: 0.490388 plain: 0.464233\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 491 [0/801 (0%)]\tLosses bn: 0.270036 drop: 0.313734 plain: 0.270320\n",
      "Train Epoch: 491 [800/801 (6%)]\tLosses bn: 0.453248 drop: 0.529169 plain: 0.464094\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 492 [0/801 (0%)]\tLosses bn: 0.269974 drop: 0.337370 plain: 0.270194\n",
      "Train Epoch: 492 [800/801 (6%)]\tLosses bn: 0.453167 drop: 0.489405 plain: 0.463972\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 493 [0/801 (0%)]\tLosses bn: 0.269950 drop: 0.316003 plain: 0.270083\n",
      "Train Epoch: 493 [800/801 (6%)]\tLosses bn: 0.453074 drop: 0.493497 plain: 0.463817\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 494 [0/801 (0%)]\tLosses bn: 0.269903 drop: 0.380888 plain: 0.269965\n",
      "Train Epoch: 494 [800/801 (6%)]\tLosses bn: 0.452964 drop: 0.510687 plain: 0.463678\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 495 [0/801 (0%)]\tLosses bn: 0.269853 drop: 0.354630 plain: 0.269843\n",
      "Train Epoch: 495 [800/801 (6%)]\tLosses bn: 0.452882 drop: 0.542488 plain: 0.463569\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 496 [0/801 (0%)]\tLosses bn: 0.269833 drop: 0.299446 plain: 0.269727\n",
      "Train Epoch: 496 [800/801 (6%)]\tLosses bn: 0.452781 drop: 0.506438 plain: 0.463411\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 497 [0/801 (0%)]\tLosses bn: 0.269795 drop: 0.331050 plain: 0.269617\n",
      "Train Epoch: 497 [800/801 (6%)]\tLosses bn: 0.452662 drop: 0.488754 plain: 0.463280\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 498 [0/801 (0%)]\tLosses bn: 0.269760 drop: 0.372001 plain: 0.269480\n",
      "Train Epoch: 498 [800/801 (6%)]\tLosses bn: 0.452550 drop: 0.468208 plain: 0.463124\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 499 [0/801 (0%)]\tLosses bn: 0.269718 drop: 0.371656 plain: 0.269364\n",
      "Train Epoch: 499 [800/801 (6%)]\tLosses bn: 0.452456 drop: 0.468668 plain: 0.462988\n",
      "Test set:\n",
      "bn: Loss: 0.0202\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n",
      "Train Epoch: 500 [0/801 (0%)]\tLosses bn: 0.269687 drop: 0.373253 plain: 0.269234\n",
      "Train Epoch: 500 [800/801 (6%)]\tLosses bn: 0.452314 drop: 0.481816 plain: 0.462829\n",
      "Test set:\n",
      "bn: Loss: 0.0203\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0194\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0197\tAccuracy: 67.0/90 (74%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 501):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    train(epoch, models, train_log)\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    test(models, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a164b2240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(test_log, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1037c1a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(test_log, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0       3    male  34.5      0      0   7.8292        Q\n",
       "1       3  female  47.0      1      0   7.0000        S\n",
       "2       2    male  62.0      0      0   9.6875        Q\n",
       "3       3    male  27.0      0      0   8.6625        S\n",
       "4       3  female  22.0      1      1  12.2875        S"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./test.csv')\n",
    "passenger_ids = df['PassengerId'].to_numpy()\n",
    "df = df.drop('PassengerId', 1)\n",
    "df = df.drop('Name', 1)\n",
    "df = df.drop('Ticket', 1)\n",
    "df = df.drop('Cabin', 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>SibSp_0</th>\n",
       "      <th>SibSp_1</th>\n",
       "      <th>SibSp_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Parch_1</th>\n",
       "      <th>Parch_2</th>\n",
       "      <th>Parch_3</th>\n",
       "      <th>Parch_4</th>\n",
       "      <th>Parch_5</th>\n",
       "      <th>Parch_6</th>\n",
       "      <th>Parch_9</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.5</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.0</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age     Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  Sex_male  SibSp_0  \\\n",
       "0  34.5   7.8292         0         0         1           0         1        1   \n",
       "1  47.0   7.0000         0         0         1           1         0        0   \n",
       "2  62.0   9.6875         0         1         0           0         1        1   \n",
       "3  27.0   8.6625         0         0         1           0         1        1   \n",
       "4  22.0  12.2875         0         0         1           1         0        0   \n",
       "\n",
       "   SibSp_1  SibSp_2  ...  Parch_1  Parch_2  Parch_3  Parch_4  Parch_5  \\\n",
       "0        0        0  ...        0        0        0        0        0   \n",
       "1        1        0  ...        0        0        0        0        0   \n",
       "2        0        0  ...        0        0        0        0        0   \n",
       "3        0        0  ...        0        0        0        0        0   \n",
       "4        1        0  ...        1        0        0        0        0   \n",
       "\n",
       "   Parch_6  Parch_9  Embarked_C  Embarked_Q  Embarked_S  \n",
       "0        0        0           0           1           0  \n",
       "1        0        0           0           0           1  \n",
       "2        0        0           0           1           0  \n",
       "3        0        0           0           0           1  \n",
       "4        0        0           0           0           1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_features = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
    "for enc in encode_features:\n",
    "    one_hot = pd.get_dummies(df[enc], prefix=enc)\n",
    "    df = df.drop(enc,axis = 1)\n",
    "    df = df.join(one_hot)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>SibSp_0</th>\n",
       "      <th>SibSp_1</th>\n",
       "      <th>SibSp_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Parch_1</th>\n",
       "      <th>Parch_2</th>\n",
       "      <th>Parch_3</th>\n",
       "      <th>Parch_4</th>\n",
       "      <th>Parch_5</th>\n",
       "      <th>Parch_6</th>\n",
       "      <th>Parch_9</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>332.000000</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30.272590</td>\n",
       "      <td>35.627188</td>\n",
       "      <td>0.255981</td>\n",
       "      <td>0.222488</td>\n",
       "      <td>0.521531</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.677033</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.033493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124402</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.007177</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>0.244019</td>\n",
       "      <td>0.110048</td>\n",
       "      <td>0.645933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.181209</td>\n",
       "      <td>55.907576</td>\n",
       "      <td>0.436934</td>\n",
       "      <td>0.416416</td>\n",
       "      <td>0.500135</td>\n",
       "      <td>0.481622</td>\n",
       "      <td>0.481622</td>\n",
       "      <td>0.468170</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>0.180135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.269980</td>\n",
       "      <td>0.084514</td>\n",
       "      <td>0.069088</td>\n",
       "      <td>0.048912</td>\n",
       "      <td>0.048912</td>\n",
       "      <td>0.069088</td>\n",
       "      <td>0.430019</td>\n",
       "      <td>0.313324</td>\n",
       "      <td>0.478803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.895800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>76.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age        Fare    Pclass_1    Pclass_2    Pclass_3  Sex_female  \\\n",
       "count  332.000000  417.000000  418.000000  418.000000  418.000000  418.000000   \n",
       "mean    30.272590   35.627188    0.255981    0.222488    0.521531    0.363636   \n",
       "std     14.181209   55.907576    0.436934    0.416416    0.500135    0.481622   \n",
       "min      0.170000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%     21.000000    7.895800    0.000000    0.000000    0.000000    0.000000   \n",
       "50%     27.000000   14.454200    0.000000    0.000000    1.000000    0.000000   \n",
       "75%     39.000000   31.500000    1.000000    0.000000    1.000000    1.000000   \n",
       "max     76.000000  512.329200    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         Sex_male     SibSp_0     SibSp_1     SibSp_2  ...     Parch_1  \\\n",
       "count  418.000000  418.000000  418.000000  418.000000  ...  418.000000   \n",
       "mean     0.636364    0.677033    0.263158    0.033493  ...    0.124402   \n",
       "std      0.481622    0.468170    0.440875    0.180135  ...    0.330435   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "50%      1.000000    1.000000    0.000000    0.000000  ...    0.000000   \n",
       "75%      1.000000    1.000000    1.000000    0.000000  ...    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
       "\n",
       "          Parch_2     Parch_3     Parch_4     Parch_5     Parch_6     Parch_9  \\\n",
       "count  418.000000  418.000000  418.000000  418.000000  418.000000  418.000000   \n",
       "mean     0.078947    0.007177    0.004785    0.002392    0.002392    0.004785   \n",
       "std      0.269980    0.084514    0.069088    0.048912    0.048912    0.069088   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "       Embarked_C  Embarked_Q  Embarked_S  \n",
       "count  418.000000  418.000000  418.000000  \n",
       "mean     0.244019    0.110048    0.645933  \n",
       "std      0.430019    0.313324    0.478803  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000  \n",
       "50%      0.000000    0.000000    1.000000  \n",
       "75%      0.000000    0.000000    1.000000  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].fillna(0)\n",
    "df['Fare'] = df['Fare'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_real: [34.5     7.8292] \n",
      "X_cat: [1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0] \n",
      "y: 0 \n"
     ]
    }
   ],
   "source": [
    "X_testing_real_zeros = df[real_features].to_numpy()\n",
    "X_testing_cat_zeros = df[cat_features].to_numpy()\n",
    "\n",
    "X_testing_real_zeros_scaled = scaler.transform(X_testing_real_zeros)\n",
    "\n",
    "print (\"X_real: {} \".format(X_testing_real_zeros[0]))\n",
    "print (\"X_cat: {} \".format(X_testing_cat_zeros[0]))\n",
    "print (\"y: {} \".format(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 25)\n"
     ]
    }
   ],
   "source": [
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0]\n",
      "[ 892  893  894  895  896  897  898  899  900  901  902  903  904  905\n",
      "  906  907  908  909  910  911  912  913  914  915  916  917  918  919\n",
      "  920  921  922  923  924  925  926  927  928  929  930  931  932  933\n",
      "  934  935  936  937  938  939  940  941  942  943  944  945  946  947\n",
      "  948  949  950  951  952  953  954  955  956  957  958  959  960  961\n",
      "  962  963  964  965  966  967  968  969  970  971  972  973  974  975\n",
      "  976  977  978  979  980  981  982  983  984  985  986  987  988  989\n",
      "  990  991  992  993  994  995  996  997  998  999 1000 1001 1002 1003\n",
      " 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017\n",
      " 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031\n",
      " 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045\n",
      " 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059\n",
      " 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073\n",
      " 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087\n",
      " 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101\n",
      " 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115\n",
      " 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129\n",
      " 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143\n",
      " 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157\n",
      " 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171\n",
      " 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185\n",
      " 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199\n",
      " 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213\n",
      " 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227\n",
      " 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241\n",
      " 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255\n",
      " 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269\n",
      " 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283\n",
      " 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297\n",
      " 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309]\n"
     ]
    }
   ],
   "source": [
    "X_testing = np.hstack((X_testing_real_zeros_scaled, X_testing_cat_zeros))\n",
    "X_testing = X_testing[:,selected_features]\n",
    "\n",
    "X_testing = torch.Tensor(X_testing)\n",
    "\n",
    "pred = models['plain'](X_testing)\n",
    "predicted = output_vals(pred)\n",
    "print (predicted)\n",
    "print (passenger_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission_nn.txt\", \"w\") as fout:\n",
    "    fout.write(\"PassengerId,Survived\\n\")\n",
    "    for val in zip(passenger_ids, predicted):\n",
    "        fout.write(\"{},{}\\n\".format(str(int(val[0])), str(int(val[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
