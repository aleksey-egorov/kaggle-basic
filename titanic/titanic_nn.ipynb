{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksey/anaconda3/envs/learning/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import PIL\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0         0       3    male  22.0      1      0   7.2500        S\n",
       "1         1       1  female  38.0      1      0  71.2833        C\n",
       "2         1       3  female  26.0      0      0   7.9250        S\n",
       "3         1       1  female  35.0      1      0  53.1000        S\n",
       "4         0       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./train.csv')\n",
    "df = df.drop('PassengerId', 1)\n",
    "df = df.drop('Name', 1)\n",
    "df = df.drop('Ticket', 1)\n",
    "df = df.drop('Cabin', 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>SibSp_0</th>\n",
       "      <th>SibSp_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Parch_0</th>\n",
       "      <th>Parch_1</th>\n",
       "      <th>Parch_2</th>\n",
       "      <th>Parch_3</th>\n",
       "      <th>Parch_4</th>\n",
       "      <th>Parch_5</th>\n",
       "      <th>Parch_6</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived   Age     Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  \\\n",
       "0         0  22.0   7.2500         0         0         1           0   \n",
       "1         1  38.0  71.2833         1         0         0           1   \n",
       "2         1  26.0   7.9250         0         0         1           1   \n",
       "3         1  35.0  53.1000         1         0         0           1   \n",
       "4         0  35.0   8.0500         0         0         1           0   \n",
       "\n",
       "   Sex_male  SibSp_0  SibSp_1  ...  Parch_0  Parch_1  Parch_2  Parch_3  \\\n",
       "0         1        0        1  ...        1        0        0        0   \n",
       "1         0        0        1  ...        1        0        0        0   \n",
       "2         0        1        0  ...        1        0        0        0   \n",
       "3         0        0        1  ...        1        0        0        0   \n",
       "4         1        1        0  ...        1        0        0        0   \n",
       "\n",
       "   Parch_4  Parch_5  Parch_6  Embarked_C  Embarked_Q  Embarked_S  \n",
       "0        0        0        0           0           0           1  \n",
       "1        0        0        0           1           0           0  \n",
       "2        0        0        0           0           0           1  \n",
       "3        0        0        0           0           0           1  \n",
       "4        0        0        0           0           0           1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_features = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
    "for enc in encode_features:\n",
    "    one_hot = pd.get_dummies(df[enc], prefix=enc)\n",
    "    df = df.drop(enc,axis = 1)\n",
    "    df = df.join(one_hot)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>SibSp_0</th>\n",
       "      <th>SibSp_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Parch_0</th>\n",
       "      <th>Parch_1</th>\n",
       "      <th>Parch_2</th>\n",
       "      <th>Parch_3</th>\n",
       "      <th>Parch_4</th>\n",
       "      <th>Parch_5</th>\n",
       "      <th>Parch_6</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.206510</td>\n",
       "      <td>0.551066</td>\n",
       "      <td>0.352413</td>\n",
       "      <td>0.647587</td>\n",
       "      <td>0.682379</td>\n",
       "      <td>0.234568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760943</td>\n",
       "      <td>0.132435</td>\n",
       "      <td>0.089787</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.005612</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.188552</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.722783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>0.428790</td>\n",
       "      <td>0.405028</td>\n",
       "      <td>0.497665</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.465813</td>\n",
       "      <td>0.423966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426747</td>\n",
       "      <td>0.339154</td>\n",
       "      <td>0.286037</td>\n",
       "      <td>0.074743</td>\n",
       "      <td>0.066890</td>\n",
       "      <td>0.074743</td>\n",
       "      <td>0.033501</td>\n",
       "      <td>0.391372</td>\n",
       "      <td>0.281141</td>\n",
       "      <td>0.447876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Survived         Age        Fare    Pclass_1    Pclass_2    Pclass_3  \\\n",
       "count  891.000000  714.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean     0.383838   29.699118   32.204208    0.242424    0.206510    0.551066   \n",
       "std      0.486592   14.526497   49.693429    0.428790    0.405028    0.497665   \n",
       "min      0.000000    0.420000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000   20.125000    7.910400    0.000000    0.000000    0.000000   \n",
       "50%      0.000000   28.000000   14.454200    0.000000    0.000000    1.000000   \n",
       "75%      1.000000   38.000000   31.000000    0.000000    0.000000    1.000000   \n",
       "max      1.000000   80.000000  512.329200    1.000000    1.000000    1.000000   \n",
       "\n",
       "       Sex_female    Sex_male     SibSp_0     SibSp_1  ...     Parch_0  \\\n",
       "count  891.000000  891.000000  891.000000  891.000000  ...  891.000000   \n",
       "mean     0.352413    0.647587    0.682379    0.234568  ...    0.760943   \n",
       "std      0.477990    0.477990    0.465813    0.423966  ...    0.426747   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000  ...    1.000000   \n",
       "50%      0.000000    1.000000    1.000000    0.000000  ...    1.000000   \n",
       "75%      1.000000    1.000000    1.000000    0.000000  ...    1.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
       "\n",
       "          Parch_1     Parch_2     Parch_3     Parch_4     Parch_5     Parch_6  \\\n",
       "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean     0.132435    0.089787    0.005612    0.004489    0.005612    0.001122   \n",
       "std      0.339154    0.286037    0.074743    0.066890    0.074743    0.033501   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "       Embarked_C  Embarked_Q  Embarked_S  \n",
       "count  891.000000  891.000000  891.000000  \n",
       "mean     0.188552    0.086420    0.722783  \n",
       "std      0.391372    0.281141    0.447876  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000  \n",
       "50%      0.000000    0.000000    1.000000  \n",
       "75%      0.000000    0.000000    1.000000  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Embarked_C', 'Parch_4', 'Pclass_3', 'Parch_2', 'Parch_0', 'SibSp_8', 'Parch_1', 'Pclass_1', 'SibSp_1', 'Parch_3', 'Parch_6', 'SibSp_3', 'Embarked_Q', 'SibSp_0', 'SibSp_4', 'Parch_5', 'SibSp_5', 'Sex_female', 'Pclass_2', 'Embarked_S', 'SibSp_2', 'Sex_male']\n"
     ]
    }
   ],
   "source": [
    "real_features = ['Age', 'Fare']\n",
    "cat_features = list(set(df.columns.values.tolist()) - set(real_features) )\n",
    "cat_features.remove('Survived')\n",
    "print (cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 25)\n"
     ]
    }
   ],
   "source": [
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_real: [22.    7.25] \n",
      "X_cat: [0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1] \n",
      "y: 0 \n"
     ]
    }
   ],
   "source": [
    "y = df['Survived'].to_numpy()\n",
    "df = df.drop('Survived', 1)\n",
    "\n",
    "X_real = df[real_features].to_numpy()\n",
    "X_cat = df[cat_features].to_numpy()\n",
    "\n",
    "print (\"X_real: {} \".format(X_real[0]))\n",
    "print (\"X_cat: {} \".format(X_cat[0]))\n",
    "print (\"y: {} \".format(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_real_scaled = scaler.fit_transform(X_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801, 2) (90, 2)\n",
      "(801, 22) (90, 22)\n"
     ]
    }
   ],
   "source": [
    "(X_train_real, X_test_real, X_train_cat, X_test_cat, \n",
    " y_train, y_test)= train_test_split(X_real_scaled, X_cat, y, test_size=0.1, random_state=0)\n",
    "print (X_train_real.shape, X_test_real.shape)\n",
    "print (X_train_cat.shape, X_test_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = (0, 1, 2, 5, 7, 9, 10, 12, 14, 16, 17, 18, 19, 21, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([801, 15]) torch.Size([90, 15])\n"
     ]
    }
   ],
   "source": [
    "X_train = np.hstack((X_train_real, X_train_cat))\n",
    "X_test = np.hstack((X_test_real, X_test_cat))\n",
    "\n",
    "X_train = X_train[:,selected_features]\n",
    "X_test = X_test[:,selected_features]\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.Tensor(y_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "print (X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, sizes, dropout=False, activation_fn=nn.ReLU):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout())\n",
    "            layers.append(activation_fn())\n",
    "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
    "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "            layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_vals(output, target):\n",
    "    vals = []\n",
    "    for out in output:\n",
    "        out = out.item()\n",
    "        if out >= 0.5:\n",
    "            vals.append(1)\n",
    "        else:\n",
    "            vals.append(0)   \n",
    "    vals = torch.Tensor(vals)\n",
    "    correct = (vals == target).sum().item()         \n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, batchnorm=False, dropout=False, lr=1e-4, l2=0.):\n",
    "        super(Net, self).__init__()\n",
    "               \n",
    "        self.fc = FullyConnected([15, 64, 1], dropout=dropout)\n",
    "        \n",
    "        self._loss = None\n",
    "        self.optim = optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "        self.criterion = nn.BCELoss()\n",
    "           \n",
    "    def forward(self, x):        \n",
    "        out = self.fc(x)\n",
    "        return out.squeeze(1)\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):          \n",
    "        self._loss = self.criterion(output, target)\n",
    "        self._correct = correct_vals(output, target)   \n",
    "        \n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, models, log=None):\n",
    "    train_size = len(X_train)\n",
    "    for batch_idx, (data, target) in enumerate(loader(X_train, y_train)):\n",
    "        for model in models.values():                             \n",
    "            model.optim.zero_grad()            \n",
    "            output = model(data)\n",
    "            loss = model.loss(output, target)\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "            \n",
    "        if batch_idx % 200 == 0:\n",
    "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / train_size)\n",
    "            losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "            print(line + losses)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / train_size)\n",
    "        losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "        if log is not None:\n",
    "            for k in models:\n",
    "                log[k].append((models[k]._loss, models[k]._correct))\n",
    "        print(line + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, log=None):\n",
    "    test_size = len(X_test)\n",
    "    avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
    "    acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, test_size, p)\n",
    "    line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
    "\n",
    "    test_loss = {k: 0. for k in models}\n",
    "    correct = {k: 0. for k in models}\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader(X_test, y_test):\n",
    "            output = {k: m(data) for k, m in models.items()}           \n",
    "            for k, m in models.items():     \n",
    "                #print (output[k].shape, target.shape)\n",
    "                test_loss[k] += m.loss(output[k], target, size_average=False).item() # sum up batch loss                \n",
    "                correct[k] += correct_vals(output[k], target)\n",
    "    \n",
    "    for k in models:\n",
    "        test_loss[k] /= test_size\n",
    "    correct_pct = {k: c / test_size for k, c in correct.items()}\n",
    "    lines = '\\n'.join([line(k, test_loss[k], correct[k], 100*correct_pct[k]) for k in models]) + '\\n'\n",
    "    report = 'Test set:\\n' + lines\n",
    "    if log is not None:\n",
    "        for k in models:\n",
    "            log[k].append((test_loss[k], correct_pct[k]))\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(log, tpe='loss'):\n",
    "    keys = log.keys()\n",
    "    logs = {k:[z for z in zip(*log[k])] for k in keys}\n",
    "    epochs = {k:range(len(log[k])) for k in keys}\n",
    " \n",
    "    \n",
    "    if tpe == 'loss':\n",
    "        handlers, = zip(*[plt.plot(epochs[k], logs[k][0], label=k) for k in keys])\n",
    "        plt.title('errors')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('error')\n",
    "        plt.legend(handles=handlers)\n",
    "        plt.show()\n",
    "    elif tpe == 'accuracy':\n",
    "        handlers, = zip(*[plt.plot(epochs[k], logs[k][1], label=k) for k in log.keys()])\n",
    "        plt.title('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.legend(handles=handlers)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(x, y, batch_size=16):    \n",
    "    for i in range(0, x.shape[0] - 1, batch_size):\n",
    "        data = x[i:i+batch_size]\n",
    "        if data.shape[0] == batch_size:            \n",
    "            targets = y[i:i+batch_size]\n",
    "\n",
    "            yield data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc): FullyConnected(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=15, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (criterion): BCELoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "models = {'bn': Net(True), 'drop': Net(False, True), 'plain': Net()}\n",
    "train_log = {k: [] for k in models}\n",
    "test_log = {k: [] for k in models}\n",
    "\n",
    "print (models['bn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/801 (0%)]\tLosses bn: 0.710103 drop: 0.718825 plain: 0.662790\n",
      "Train Epoch: 1 [800/801 (6%)]\tLosses bn: 0.702312 drop: 0.701909 plain: 0.752292\n",
      "Test set:\n",
      "bn: Loss: 0.0390\tAccuracy: 31.0/90 (34%)\n",
      "drop: Loss: 0.0389\tAccuracy: 39.0/90 (43%)\n",
      "plain: Loss: 0.0377\tAccuracy: 48.0/90 (53%)\n",
      "\n",
      "Train Epoch: 2 [0/801 (0%)]\tLosses bn: 0.699558 drop: 0.716243 plain: 0.654754\n",
      "Train Epoch: 2 [800/801 (6%)]\tLosses bn: 0.702253 drop: 0.715166 plain: 0.748710\n",
      "Test set:\n",
      "bn: Loss: 0.0386\tAccuracy: 46.0/90 (51%)\n",
      "drop: Loss: 0.0386\tAccuracy: 44.0/90 (49%)\n",
      "plain: Loss: 0.0373\tAccuracy: 49.0/90 (54%)\n",
      "\n",
      "Train Epoch: 3 [0/801 (0%)]\tLosses bn: 0.690652 drop: 0.681611 plain: 0.647797\n",
      "Train Epoch: 3 [800/801 (6%)]\tLosses bn: 0.702034 drop: 0.740579 plain: 0.745101\n",
      "Test set:\n",
      "bn: Loss: 0.0383\tAccuracy: 50.0/90 (56%)\n",
      "drop: Loss: 0.0383\tAccuracy: 46.0/90 (51%)\n",
      "plain: Loss: 0.0370\tAccuracy: 51.0/90 (57%)\n",
      "\n",
      "Train Epoch: 4 [0/801 (0%)]\tLosses bn: 0.682282 drop: 0.697847 plain: 0.640982\n",
      "Train Epoch: 4 [800/801 (6%)]\tLosses bn: 0.701602 drop: 0.732372 plain: 0.741322\n",
      "Test set:\n",
      "bn: Loss: 0.0379\tAccuracy: 51.0/90 (57%)\n",
      "drop: Loss: 0.0381\tAccuracy: 47.0/90 (52%)\n",
      "plain: Loss: 0.0366\tAccuracy: 51.0/90 (57%)\n",
      "\n",
      "Train Epoch: 5 [0/801 (0%)]\tLosses bn: 0.674411 drop: 0.698074 plain: 0.634251\n",
      "Train Epoch: 5 [800/801 (6%)]\tLosses bn: 0.700915 drop: 0.668598 plain: 0.737371\n",
      "Test set:\n",
      "bn: Loss: 0.0376\tAccuracy: 49.0/90 (54%)\n",
      "drop: Loss: 0.0378\tAccuracy: 47.0/90 (52%)\n",
      "plain: Loss: 0.0363\tAccuracy: 52.0/90 (58%)\n",
      "\n",
      "Train Epoch: 6 [0/801 (0%)]\tLosses bn: 0.666905 drop: 0.679823 plain: 0.627651\n",
      "Train Epoch: 6 [800/801 (6%)]\tLosses bn: 0.699921 drop: 0.684338 plain: 0.733123\n",
      "Test set:\n",
      "bn: Loss: 0.0372\tAccuracy: 50.0/90 (56%)\n",
      "drop: Loss: 0.0376\tAccuracy: 48.0/90 (53%)\n",
      "plain: Loss: 0.0359\tAccuracy: 52.0/90 (58%)\n",
      "\n",
      "Train Epoch: 7 [0/801 (0%)]\tLosses bn: 0.659685 drop: 0.671678 plain: 0.621128\n",
      "Train Epoch: 7 [800/801 (6%)]\tLosses bn: 0.698696 drop: 0.747859 plain: 0.728724\n",
      "Test set:\n",
      "bn: Loss: 0.0369\tAccuracy: 51.0/90 (57%)\n",
      "drop: Loss: 0.0373\tAccuracy: 48.0/90 (53%)\n",
      "plain: Loss: 0.0356\tAccuracy: 53.0/90 (59%)\n",
      "\n",
      "Train Epoch: 8 [0/801 (0%)]\tLosses bn: 0.652597 drop: 0.650998 plain: 0.614692\n",
      "Train Epoch: 8 [800/801 (6%)]\tLosses bn: 0.697173 drop: 0.728449 plain: 0.724138\n",
      "Test set:\n",
      "bn: Loss: 0.0366\tAccuracy: 51.0/90 (57%)\n",
      "drop: Loss: 0.0370\tAccuracy: 47.0/90 (52%)\n",
      "plain: Loss: 0.0352\tAccuracy: 54.0/90 (60%)\n",
      "\n",
      "Train Epoch: 9 [0/801 (0%)]\tLosses bn: 0.645488 drop: 0.644874 plain: 0.608274\n",
      "Train Epoch: 9 [800/801 (6%)]\tLosses bn: 0.695107 drop: 0.723158 plain: 0.719343\n",
      "Test set:\n",
      "bn: Loss: 0.0363\tAccuracy: 51.0/90 (57%)\n",
      "drop: Loss: 0.0368\tAccuracy: 49.0/90 (54%)\n",
      "plain: Loss: 0.0349\tAccuracy: 54.0/90 (60%)\n",
      "\n",
      "Train Epoch: 10 [0/801 (0%)]\tLosses bn: 0.638438 drop: 0.640264 plain: 0.601864\n",
      "Train Epoch: 10 [800/801 (6%)]\tLosses bn: 0.692657 drop: 0.695903 plain: 0.714231\n",
      "Test set:\n",
      "bn: Loss: 0.0359\tAccuracy: 54.0/90 (60%)\n",
      "drop: Loss: 0.0365\tAccuracy: 51.0/90 (57%)\n",
      "plain: Loss: 0.0346\tAccuracy: 55.0/90 (61%)\n",
      "\n",
      "Train Epoch: 11 [0/801 (0%)]\tLosses bn: 0.631412 drop: 0.607316 plain: 0.595481\n",
      "Train Epoch: 11 [800/801 (6%)]\tLosses bn: 0.689854 drop: 0.714117 plain: 0.708931\n",
      "Test set:\n",
      "bn: Loss: 0.0356\tAccuracy: 54.0/90 (60%)\n",
      "drop: Loss: 0.0363\tAccuracy: 52.0/90 (58%)\n",
      "plain: Loss: 0.0342\tAccuracy: 55.0/90 (61%)\n",
      "\n",
      "Train Epoch: 12 [0/801 (0%)]\tLosses bn: 0.624459 drop: 0.669839 plain: 0.589131\n",
      "Train Epoch: 12 [800/801 (6%)]\tLosses bn: 0.686568 drop: 0.699274 plain: 0.703377\n",
      "Test set:\n",
      "bn: Loss: 0.0352\tAccuracy: 54.0/90 (60%)\n",
      "drop: Loss: 0.0360\tAccuracy: 53.0/90 (59%)\n",
      "plain: Loss: 0.0339\tAccuracy: 54.0/90 (60%)\n",
      "\n",
      "Train Epoch: 13 [0/801 (0%)]\tLosses bn: 0.617538 drop: 0.648151 plain: 0.582796\n",
      "Train Epoch: 13 [800/801 (6%)]\tLosses bn: 0.682883 drop: 0.700014 plain: 0.697664\n",
      "Test set:\n",
      "bn: Loss: 0.0349\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0357\tAccuracy: 53.0/90 (59%)\n",
      "plain: Loss: 0.0335\tAccuracy: 54.0/90 (60%)\n",
      "\n",
      "Train Epoch: 14 [0/801 (0%)]\tLosses bn: 0.610670 drop: 0.656320 plain: 0.576505\n",
      "Train Epoch: 14 [800/801 (6%)]\tLosses bn: 0.678712 drop: 0.687006 plain: 0.691906\n",
      "Test set:\n",
      "bn: Loss: 0.0345\tAccuracy: 55.0/90 (61%)\n",
      "drop: Loss: 0.0355\tAccuracy: 55.0/90 (61%)\n",
      "plain: Loss: 0.0332\tAccuracy: 54.0/90 (60%)\n",
      "\n",
      "Train Epoch: 15 [0/801 (0%)]\tLosses bn: 0.603857 drop: 0.624807 plain: 0.570288\n",
      "Train Epoch: 15 [800/801 (6%)]\tLosses bn: 0.674086 drop: 0.720813 plain: 0.686011\n",
      "Test set:\n",
      "bn: Loss: 0.0342\tAccuracy: 56.0/90 (62%)\n",
      "drop: Loss: 0.0352\tAccuracy: 55.0/90 (61%)\n",
      "plain: Loss: 0.0329\tAccuracy: 54.0/90 (60%)\n",
      "\n",
      "Train Epoch: 16 [0/801 (0%)]\tLosses bn: 0.597057 drop: 0.623044 plain: 0.564206\n",
      "Train Epoch: 16 [800/801 (6%)]\tLosses bn: 0.669121 drop: 0.717950 plain: 0.679965\n",
      "Test set:\n",
      "bn: Loss: 0.0338\tAccuracy: 56.0/90 (62%)\n",
      "drop: Loss: 0.0350\tAccuracy: 56.0/90 (62%)\n",
      "plain: Loss: 0.0325\tAccuracy: 54.0/90 (60%)\n",
      "\n",
      "Train Epoch: 17 [0/801 (0%)]\tLosses bn: 0.590325 drop: 0.615289 plain: 0.558204\n",
      "Train Epoch: 17 [800/801 (6%)]\tLosses bn: 0.663868 drop: 0.727338 plain: 0.673761\n",
      "Test set:\n",
      "bn: Loss: 0.0334\tAccuracy: 57.0/90 (63%)\n",
      "drop: Loss: 0.0347\tAccuracy: 56.0/90 (62%)\n",
      "plain: Loss: 0.0322\tAccuracy: 54.0/90 (60%)\n",
      "\n",
      "Train Epoch: 18 [0/801 (0%)]\tLosses bn: 0.583672 drop: 0.622428 plain: 0.552279\n",
      "Train Epoch: 18 [800/801 (6%)]\tLosses bn: 0.658399 drop: 0.685403 plain: 0.667540\n",
      "Test set:\n",
      "bn: Loss: 0.0331\tAccuracy: 57.0/90 (63%)\n",
      "drop: Loss: 0.0344\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0319\tAccuracy: 54.0/90 (60%)\n",
      "\n",
      "Train Epoch: 19 [0/801 (0%)]\tLosses bn: 0.577051 drop: 0.583641 plain: 0.546453\n",
      "Train Epoch: 19 [800/801 (6%)]\tLosses bn: 0.652762 drop: 0.695046 plain: 0.661337\n",
      "Test set:\n",
      "bn: Loss: 0.0327\tAccuracy: 57.0/90 (63%)\n",
      "drop: Loss: 0.0341\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0316\tAccuracy: 55.0/90 (61%)\n",
      "\n",
      "Train Epoch: 20 [0/801 (0%)]\tLosses bn: 0.570494 drop: 0.646340 plain: 0.540766\n",
      "Train Epoch: 20 [800/801 (6%)]\tLosses bn: 0.646924 drop: 0.670728 plain: 0.655166\n",
      "Test set:\n",
      "bn: Loss: 0.0324\tAccuracy: 58.0/90 (64%)\n",
      "drop: Loss: 0.0339\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0313\tAccuracy: 55.0/90 (61%)\n",
      "\n",
      "Train Epoch: 21 [0/801 (0%)]\tLosses bn: 0.564033 drop: 0.567178 plain: 0.535253\n",
      "Train Epoch: 21 [800/801 (6%)]\tLosses bn: 0.640982 drop: 0.589038 plain: 0.649063\n",
      "Test set:\n",
      "bn: Loss: 0.0320\tAccuracy: 59.0/90 (66%)\n",
      "drop: Loss: 0.0336\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0310\tAccuracy: 56.0/90 (62%)\n",
      "\n",
      "Train Epoch: 22 [0/801 (0%)]\tLosses bn: 0.557689 drop: 0.597796 plain: 0.529851\n",
      "Train Epoch: 22 [800/801 (6%)]\tLosses bn: 0.634921 drop: 0.669224 plain: 0.642988\n",
      "Test set:\n",
      "bn: Loss: 0.0317\tAccuracy: 59.0/90 (66%)\n",
      "drop: Loss: 0.0333\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0307\tAccuracy: 56.0/90 (62%)\n",
      "\n",
      "Train Epoch: 23 [0/801 (0%)]\tLosses bn: 0.551467 drop: 0.618268 plain: 0.524523\n",
      "Train Epoch: 23 [800/801 (6%)]\tLosses bn: 0.628822 drop: 0.667090 plain: 0.636943\n",
      "Test set:\n",
      "bn: Loss: 0.0314\tAccuracy: 59.0/90 (66%)\n",
      "drop: Loss: 0.0331\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0304\tAccuracy: 56.0/90 (62%)\n",
      "\n",
      "Train Epoch: 24 [0/801 (0%)]\tLosses bn: 0.545431 drop: 0.608508 plain: 0.519367\n",
      "Train Epoch: 24 [800/801 (6%)]\tLosses bn: 0.622736 drop: 0.671649 plain: 0.630949\n",
      "Test set:\n",
      "bn: Loss: 0.0310\tAccuracy: 59.0/90 (66%)\n",
      "drop: Loss: 0.0328\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0301\tAccuracy: 56.0/90 (62%)\n",
      "\n",
      "Train Epoch: 25 [0/801 (0%)]\tLosses bn: 0.539615 drop: 0.551406 plain: 0.514340\n",
      "Train Epoch: 25 [800/801 (6%)]\tLosses bn: 0.616684 drop: 0.701714 plain: 0.625117\n",
      "Test set:\n",
      "bn: Loss: 0.0307\tAccuracy: 60.0/90 (67%)\n",
      "drop: Loss: 0.0326\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0298\tAccuracy: 57.0/90 (63%)\n",
      "\n",
      "Train Epoch: 26 [0/801 (0%)]\tLosses bn: 0.533986 drop: 0.546636 plain: 0.509494\n",
      "Train Epoch: 26 [800/801 (6%)]\tLosses bn: 0.610760 drop: 0.709659 plain: 0.619440\n",
      "Test set:\n",
      "bn: Loss: 0.0304\tAccuracy: 60.0/90 (67%)\n",
      "drop: Loss: 0.0323\tAccuracy: 57.0/90 (63%)\n",
      "plain: Loss: 0.0296\tAccuracy: 57.0/90 (63%)\n",
      "\n",
      "Train Epoch: 27 [0/801 (0%)]\tLosses bn: 0.528583 drop: 0.543716 plain: 0.504821\n",
      "Train Epoch: 27 [800/801 (6%)]\tLosses bn: 0.604924 drop: 0.666690 plain: 0.613923\n",
      "Test set:\n",
      "bn: Loss: 0.0301\tAccuracy: 60.0/90 (67%)\n",
      "drop: Loss: 0.0321\tAccuracy: 58.0/90 (64%)\n",
      "plain: Loss: 0.0293\tAccuracy: 59.0/90 (66%)\n",
      "\n",
      "Train Epoch: 28 [0/801 (0%)]\tLosses bn: 0.523347 drop: 0.616042 plain: 0.500310\n",
      "Train Epoch: 28 [800/801 (6%)]\tLosses bn: 0.599137 drop: 0.704914 plain: 0.608537\n",
      "Test set:\n",
      "bn: Loss: 0.0298\tAccuracy: 60.0/90 (67%)\n",
      "drop: Loss: 0.0318\tAccuracy: 58.0/90 (64%)\n",
      "plain: Loss: 0.0291\tAccuracy: 61.0/90 (68%)\n",
      "\n",
      "Train Epoch: 29 [0/801 (0%)]\tLosses bn: 0.518292 drop: 0.561353 plain: 0.495976\n",
      "Train Epoch: 29 [800/801 (6%)]\tLosses bn: 0.593451 drop: 0.678765 plain: 0.603238\n",
      "Test set:\n",
      "bn: Loss: 0.0295\tAccuracy: 59.0/90 (66%)\n",
      "drop: Loss: 0.0316\tAccuracy: 58.0/90 (64%)\n",
      "plain: Loss: 0.0288\tAccuracy: 62.0/90 (69%)\n",
      "\n",
      "Train Epoch: 30 [0/801 (0%)]\tLosses bn: 0.513422 drop: 0.497855 plain: 0.491826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30 [800/801 (6%)]\tLosses bn: 0.587904 drop: 0.669003 plain: 0.598128\n",
      "Test set:\n",
      "bn: Loss: 0.0293\tAccuracy: 61.0/90 (68%)\n",
      "drop: Loss: 0.0314\tAccuracy: 58.0/90 (64%)\n",
      "plain: Loss: 0.0286\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 31 [0/801 (0%)]\tLosses bn: 0.508668 drop: 0.544973 plain: 0.487865\n",
      "Train Epoch: 31 [800/801 (6%)]\tLosses bn: 0.582498 drop: 0.688604 plain: 0.593171\n",
      "Test set:\n",
      "bn: Loss: 0.0290\tAccuracy: 62.0/90 (69%)\n",
      "drop: Loss: 0.0311\tAccuracy: 58.0/90 (64%)\n",
      "plain: Loss: 0.0284\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 32 [0/801 (0%)]\tLosses bn: 0.504055 drop: 0.495612 plain: 0.484065\n",
      "Train Epoch: 32 [800/801 (6%)]\tLosses bn: 0.577250 drop: 0.643697 plain: 0.588339\n",
      "Test set:\n",
      "bn: Loss: 0.0287\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0309\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0281\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 33 [0/801 (0%)]\tLosses bn: 0.499617 drop: 0.508087 plain: 0.480408\n",
      "Train Epoch: 33 [800/801 (6%)]\tLosses bn: 0.572143 drop: 0.607284 plain: 0.583655\n",
      "Test set:\n",
      "bn: Loss: 0.0285\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0306\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0279\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 34 [0/801 (0%)]\tLosses bn: 0.495308 drop: 0.542210 plain: 0.476916\n",
      "Train Epoch: 34 [800/801 (6%)]\tLosses bn: 0.567186 drop: 0.588717 plain: 0.579074\n",
      "Test set:\n",
      "bn: Loss: 0.0283\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0304\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0277\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 35 [0/801 (0%)]\tLosses bn: 0.491137 drop: 0.531839 plain: 0.473583\n",
      "Train Epoch: 35 [800/801 (6%)]\tLosses bn: 0.562387 drop: 0.657726 plain: 0.574619\n",
      "Test set:\n",
      "bn: Loss: 0.0280\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0302\tAccuracy: 60.0/90 (67%)\n",
      "plain: Loss: 0.0275\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 36 [0/801 (0%)]\tLosses bn: 0.487146 drop: 0.576410 plain: 0.470360\n",
      "Train Epoch: 36 [800/801 (6%)]\tLosses bn: 0.557796 drop: 0.624866 plain: 0.570283\n",
      "Test set:\n",
      "bn: Loss: 0.0278\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0299\tAccuracy: 60.0/90 (67%)\n",
      "plain: Loss: 0.0273\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 37 [0/801 (0%)]\tLosses bn: 0.483343 drop: 0.511881 plain: 0.467267\n",
      "Train Epoch: 37 [800/801 (6%)]\tLosses bn: 0.553370 drop: 0.587052 plain: 0.566119\n",
      "Test set:\n",
      "bn: Loss: 0.0276\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0297\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0271\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 38 [0/801 (0%)]\tLosses bn: 0.479747 drop: 0.521745 plain: 0.464336\n",
      "Train Epoch: 38 [800/801 (6%)]\tLosses bn: 0.549127 drop: 0.718121 plain: 0.562122\n",
      "Test set:\n",
      "bn: Loss: 0.0274\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0295\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0269\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 39 [0/801 (0%)]\tLosses bn: 0.476335 drop: 0.498079 plain: 0.461542\n",
      "Train Epoch: 39 [800/801 (6%)]\tLosses bn: 0.545034 drop: 0.600850 plain: 0.558234\n",
      "Test set:\n",
      "bn: Loss: 0.0272\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0293\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0268\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 40 [0/801 (0%)]\tLosses bn: 0.473083 drop: 0.561319 plain: 0.458879\n",
      "Train Epoch: 40 [800/801 (6%)]\tLosses bn: 0.541113 drop: 0.657160 plain: 0.554481\n",
      "Test set:\n",
      "bn: Loss: 0.0270\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0291\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0266\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 41 [0/801 (0%)]\tLosses bn: 0.469983 drop: 0.511853 plain: 0.456335\n",
      "Train Epoch: 41 [800/801 (6%)]\tLosses bn: 0.537364 drop: 0.590900 plain: 0.550825\n",
      "Test set:\n",
      "bn: Loss: 0.0268\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0289\tAccuracy: 59.0/90 (66%)\n",
      "plain: Loss: 0.0264\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 42 [0/801 (0%)]\tLosses bn: 0.467007 drop: 0.496157 plain: 0.453921\n",
      "Train Epoch: 42 [800/801 (6%)]\tLosses bn: 0.533740 drop: 0.570898 plain: 0.547336\n",
      "Test set:\n",
      "bn: Loss: 0.0266\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0287\tAccuracy: 60.0/90 (67%)\n",
      "plain: Loss: 0.0263\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 43 [0/801 (0%)]\tLosses bn: 0.464148 drop: 0.510953 plain: 0.451603\n",
      "Train Epoch: 43 [800/801 (6%)]\tLosses bn: 0.530275 drop: 0.586879 plain: 0.543992\n",
      "Test set:\n",
      "bn: Loss: 0.0265\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0286\tAccuracy: 61.0/90 (68%)\n",
      "plain: Loss: 0.0261\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 44 [0/801 (0%)]\tLosses bn: 0.461442 drop: 0.496836 plain: 0.449384\n",
      "Train Epoch: 44 [800/801 (6%)]\tLosses bn: 0.526964 drop: 0.602947 plain: 0.540787\n",
      "Test set:\n",
      "bn: Loss: 0.0263\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0284\tAccuracy: 62.0/90 (69%)\n",
      "plain: Loss: 0.0260\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 45 [0/801 (0%)]\tLosses bn: 0.458877 drop: 0.490741 plain: 0.447291\n",
      "Train Epoch: 45 [800/801 (6%)]\tLosses bn: 0.523790 drop: 0.595272 plain: 0.537695\n",
      "Test set:\n",
      "bn: Loss: 0.0261\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0282\tAccuracy: 62.0/90 (69%)\n",
      "plain: Loss: 0.0258\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 46 [0/801 (0%)]\tLosses bn: 0.456434 drop: 0.521880 plain: 0.445292\n",
      "Train Epoch: 46 [800/801 (6%)]\tLosses bn: 0.520762 drop: 0.594286 plain: 0.534715\n",
      "Test set:\n",
      "bn: Loss: 0.0260\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0280\tAccuracy: 62.0/90 (69%)\n",
      "plain: Loss: 0.0257\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 47 [0/801 (0%)]\tLosses bn: 0.454114 drop: 0.499151 plain: 0.443400\n",
      "Train Epoch: 47 [800/801 (6%)]\tLosses bn: 0.517829 drop: 0.574693 plain: 0.531792\n",
      "Test set:\n",
      "bn: Loss: 0.0258\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0278\tAccuracy: 63.0/90 (70%)\n",
      "plain: Loss: 0.0256\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 48 [0/801 (0%)]\tLosses bn: 0.451915 drop: 0.491556 plain: 0.441619\n",
      "Train Epoch: 48 [800/801 (6%)]\tLosses bn: 0.515013 drop: 0.584065 plain: 0.528955\n",
      "Test set:\n",
      "bn: Loss: 0.0257\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0277\tAccuracy: 63.0/90 (70%)\n",
      "plain: Loss: 0.0254\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 49 [0/801 (0%)]\tLosses bn: 0.449820 drop: 0.534565 plain: 0.439941\n",
      "Train Epoch: 49 [800/801 (6%)]\tLosses bn: 0.512314 drop: 0.556661 plain: 0.526223\n",
      "Test set:\n",
      "bn: Loss: 0.0256\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0275\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0253\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 50 [0/801 (0%)]\tLosses bn: 0.447823 drop: 0.496314 plain: 0.438376\n",
      "Train Epoch: 50 [800/801 (6%)]\tLosses bn: 0.509729 drop: 0.576235 plain: 0.523622\n",
      "Test set:\n",
      "bn: Loss: 0.0254\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0274\tAccuracy: 63.0/90 (70%)\n",
      "plain: Loss: 0.0252\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 51 [0/801 (0%)]\tLosses bn: 0.445911 drop: 0.495138 plain: 0.436894\n",
      "Train Epoch: 51 [800/801 (6%)]\tLosses bn: 0.507229 drop: 0.511044 plain: 0.521160\n",
      "Test set:\n",
      "bn: Loss: 0.0253\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0272\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0251\tAccuracy: 62.0/90 (69%)\n",
      "\n",
      "Train Epoch: 52 [0/801 (0%)]\tLosses bn: 0.444098 drop: 0.473289 plain: 0.435495\n",
      "Train Epoch: 52 [800/801 (6%)]\tLosses bn: 0.504818 drop: 0.622240 plain: 0.518824\n",
      "Test set:\n",
      "bn: Loss: 0.0252\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0271\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0250\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 53 [0/801 (0%)]\tLosses bn: 0.442366 drop: 0.525733 plain: 0.434176\n",
      "Train Epoch: 53 [800/801 (6%)]\tLosses bn: 0.502493 drop: 0.573565 plain: 0.516555\n",
      "Test set:\n",
      "bn: Loss: 0.0251\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0269\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0249\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 54 [0/801 (0%)]\tLosses bn: 0.440729 drop: 0.518695 plain: 0.432938\n",
      "Train Epoch: 54 [800/801 (6%)]\tLosses bn: 0.500275 drop: 0.557502 plain: 0.514385\n",
      "Test set:\n",
      "bn: Loss: 0.0250\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0268\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0248\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 55 [0/801 (0%)]\tLosses bn: 0.439168 drop: 0.494886 plain: 0.431772\n",
      "Train Epoch: 55 [800/801 (6%)]\tLosses bn: 0.498187 drop: 0.547603 plain: 0.512325\n",
      "Test set:\n",
      "bn: Loss: 0.0249\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0267\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0247\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 56 [0/801 (0%)]\tLosses bn: 0.437678 drop: 0.445553 plain: 0.430674\n",
      "Train Epoch: 56 [800/801 (6%)]\tLosses bn: 0.496196 drop: 0.512822 plain: 0.510361\n",
      "Test set:\n",
      "bn: Loss: 0.0248\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0265\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0246\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 57 [0/801 (0%)]\tLosses bn: 0.436266 drop: 0.477264 plain: 0.429641\n",
      "Train Epoch: 57 [800/801 (6%)]\tLosses bn: 0.494257 drop: 0.543339 plain: 0.508499\n",
      "Test set:\n",
      "bn: Loss: 0.0247\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0264\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0245\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 58 [0/801 (0%)]\tLosses bn: 0.434913 drop: 0.482834 plain: 0.428657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 58 [800/801 (6%)]\tLosses bn: 0.492375 drop: 0.547626 plain: 0.506712\n",
      "Test set:\n",
      "bn: Loss: 0.0246\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0263\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0244\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 59 [0/801 (0%)]\tLosses bn: 0.433640 drop: 0.522137 plain: 0.427720\n",
      "Train Epoch: 59 [800/801 (6%)]\tLosses bn: 0.490588 drop: 0.635830 plain: 0.504976\n",
      "Test set:\n",
      "bn: Loss: 0.0245\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0262\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0243\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 60 [0/801 (0%)]\tLosses bn: 0.432417 drop: 0.467448 plain: 0.426854\n",
      "Train Epoch: 60 [800/801 (6%)]\tLosses bn: 0.488837 drop: 0.579136 plain: 0.503312\n",
      "Test set:\n",
      "bn: Loss: 0.0244\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0261\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0242\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 61 [0/801 (0%)]\tLosses bn: 0.431279 drop: 0.494331 plain: 0.426030\n",
      "Train Epoch: 61 [800/801 (6%)]\tLosses bn: 0.487162 drop: 0.515230 plain: 0.501693\n",
      "Test set:\n",
      "bn: Loss: 0.0243\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0260\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0241\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 62 [0/801 (0%)]\tLosses bn: 0.430191 drop: 0.489999 plain: 0.425258\n",
      "Train Epoch: 62 [800/801 (6%)]\tLosses bn: 0.485571 drop: 0.491444 plain: 0.500146\n",
      "Test set:\n",
      "bn: Loss: 0.0243\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0259\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0241\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 63 [0/801 (0%)]\tLosses bn: 0.429163 drop: 0.494880 plain: 0.424528\n",
      "Train Epoch: 63 [800/801 (6%)]\tLosses bn: 0.484064 drop: 0.582164 plain: 0.498657\n",
      "Test set:\n",
      "bn: Loss: 0.0242\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0258\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0240\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 64 [0/801 (0%)]\tLosses bn: 0.428174 drop: 0.479508 plain: 0.423829\n",
      "Train Epoch: 64 [800/801 (6%)]\tLosses bn: 0.482678 drop: 0.530770 plain: 0.497186\n",
      "Test set:\n",
      "bn: Loss: 0.0241\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0257\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0239\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 65 [0/801 (0%)]\tLosses bn: 0.427222 drop: 0.446751 plain: 0.423158\n",
      "Train Epoch: 65 [800/801 (6%)]\tLosses bn: 0.481332 drop: 0.495687 plain: 0.495765\n",
      "Test set:\n",
      "bn: Loss: 0.0241\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0256\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0239\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 66 [0/801 (0%)]\tLosses bn: 0.426328 drop: 0.428872 plain: 0.422517\n",
      "Train Epoch: 66 [800/801 (6%)]\tLosses bn: 0.480033 drop: 0.530180 plain: 0.494371\n",
      "Test set:\n",
      "bn: Loss: 0.0240\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0255\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0238\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 67 [0/801 (0%)]\tLosses bn: 0.425471 drop: 0.486177 plain: 0.421899\n",
      "Train Epoch: 67 [800/801 (6%)]\tLosses bn: 0.478806 drop: 0.564690 plain: 0.493031\n",
      "Test set:\n",
      "bn: Loss: 0.0239\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0254\tAccuracy: 63.0/90 (70%)\n",
      "plain: Loss: 0.0237\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 68 [0/801 (0%)]\tLosses bn: 0.424653 drop: 0.479506 plain: 0.421311\n",
      "Train Epoch: 68 [800/801 (6%)]\tLosses bn: 0.477676 drop: 0.511621 plain: 0.491735\n",
      "Test set:\n",
      "bn: Loss: 0.0239\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0253\tAccuracy: 63.0/90 (70%)\n",
      "plain: Loss: 0.0237\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 69 [0/801 (0%)]\tLosses bn: 0.423882 drop: 0.451206 plain: 0.420749\n",
      "Train Epoch: 69 [800/801 (6%)]\tLosses bn: 0.476591 drop: 0.553464 plain: 0.490501\n",
      "Test set:\n",
      "bn: Loss: 0.0238\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0252\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0236\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 70 [0/801 (0%)]\tLosses bn: 0.423150 drop: 0.453707 plain: 0.420221\n",
      "Train Epoch: 70 [800/801 (6%)]\tLosses bn: 0.475559 drop: 0.541842 plain: 0.489325\n",
      "Test set:\n",
      "bn: Loss: 0.0238\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0251\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0236\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 71 [0/801 (0%)]\tLosses bn: 0.422445 drop: 0.437376 plain: 0.419713\n",
      "Train Epoch: 71 [800/801 (6%)]\tLosses bn: 0.474585 drop: 0.539131 plain: 0.488184\n",
      "Test set:\n",
      "bn: Loss: 0.0237\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0250\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0235\tAccuracy: 63.0/90 (70%)\n",
      "\n",
      "Train Epoch: 72 [0/801 (0%)]\tLosses bn: 0.421778 drop: 0.515553 plain: 0.419233\n",
      "Train Epoch: 72 [800/801 (6%)]\tLosses bn: 0.473645 drop: 0.595226 plain: 0.487056\n",
      "Test set:\n",
      "bn: Loss: 0.0237\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0249\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0234\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 73 [0/801 (0%)]\tLosses bn: 0.421138 drop: 0.479011 plain: 0.418772\n",
      "Train Epoch: 73 [800/801 (6%)]\tLosses bn: 0.472747 drop: 0.508836 plain: 0.485955\n",
      "Test set:\n",
      "bn: Loss: 0.0236\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0249\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0234\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 74 [0/801 (0%)]\tLosses bn: 0.420515 drop: 0.431239 plain: 0.418331\n",
      "Train Epoch: 74 [800/801 (6%)]\tLosses bn: 0.471888 drop: 0.519763 plain: 0.484880\n",
      "Test set:\n",
      "bn: Loss: 0.0236\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0248\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0233\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 75 [0/801 (0%)]\tLosses bn: 0.419928 drop: 0.480838 plain: 0.417893\n",
      "Train Epoch: 75 [800/801 (6%)]\tLosses bn: 0.471070 drop: 0.522852 plain: 0.483857\n",
      "Test set:\n",
      "bn: Loss: 0.0235\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0247\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0233\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 76 [0/801 (0%)]\tLosses bn: 0.419353 drop: 0.480960 plain: 0.417472\n",
      "Train Epoch: 76 [800/801 (6%)]\tLosses bn: 0.470284 drop: 0.459978 plain: 0.482877\n",
      "Test set:\n",
      "bn: Loss: 0.0235\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0247\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0232\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 77 [0/801 (0%)]\tLosses bn: 0.418805 drop: 0.446501 plain: 0.417064\n",
      "Train Epoch: 77 [800/801 (6%)]\tLosses bn: 0.469538 drop: 0.470132 plain: 0.481939\n",
      "Test set:\n",
      "bn: Loss: 0.0234\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0246\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0232\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 78 [0/801 (0%)]\tLosses bn: 0.418264 drop: 0.446216 plain: 0.416674\n",
      "Train Epoch: 78 [800/801 (6%)]\tLosses bn: 0.468842 drop: 0.492878 plain: 0.481057\n",
      "Test set:\n",
      "bn: Loss: 0.0234\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0245\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0232\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 79 [0/801 (0%)]\tLosses bn: 0.417751 drop: 0.439639 plain: 0.416297\n",
      "Train Epoch: 79 [800/801 (6%)]\tLosses bn: 0.468143 drop: 0.510472 plain: 0.480207\n",
      "Test set:\n",
      "bn: Loss: 0.0233\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0245\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0231\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 80 [0/801 (0%)]\tLosses bn: 0.417232 drop: 0.471256 plain: 0.415924\n",
      "Train Epoch: 80 [800/801 (6%)]\tLosses bn: 0.467451 drop: 0.521059 plain: 0.479385\n",
      "Test set:\n",
      "bn: Loss: 0.0233\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0244\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0231\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 81 [0/801 (0%)]\tLosses bn: 0.416716 drop: 0.421899 plain: 0.415561\n",
      "Train Epoch: 81 [800/801 (6%)]\tLosses bn: 0.466796 drop: 0.504500 plain: 0.478598\n",
      "Test set:\n",
      "bn: Loss: 0.0233\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0244\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0230\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 82 [0/801 (0%)]\tLosses bn: 0.416218 drop: 0.469030 plain: 0.415214\n",
      "Train Epoch: 82 [800/801 (6%)]\tLosses bn: 0.466190 drop: 0.537594 plain: 0.477840\n",
      "Test set:\n",
      "bn: Loss: 0.0232\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0243\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0230\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 83 [0/801 (0%)]\tLosses bn: 0.415733 drop: 0.463390 plain: 0.414872\n",
      "Train Epoch: 83 [800/801 (6%)]\tLosses bn: 0.465608 drop: 0.534618 plain: 0.477104\n",
      "Test set:\n",
      "bn: Loss: 0.0232\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0242\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0230\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 84 [0/801 (0%)]\tLosses bn: 0.415260 drop: 0.440635 plain: 0.414537\n",
      "Train Epoch: 84 [800/801 (6%)]\tLosses bn: 0.465020 drop: 0.478997 plain: 0.476394\n",
      "Test set:\n",
      "bn: Loss: 0.0232\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0242\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0229\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 85 [0/801 (0%)]\tLosses bn: 0.414812 drop: 0.472861 plain: 0.414213\n",
      "Train Epoch: 85 [800/801 (6%)]\tLosses bn: 0.464433 drop: 0.557693 plain: 0.475719\n",
      "Test set:\n",
      "bn: Loss: 0.0231\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0241\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0229\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 86 [0/801 (0%)]\tLosses bn: 0.414384 drop: 0.429944 plain: 0.413896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 86 [800/801 (6%)]\tLosses bn: 0.463856 drop: 0.521696 plain: 0.475056\n",
      "Test set:\n",
      "bn: Loss: 0.0231\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0241\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0229\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 87 [0/801 (0%)]\tLosses bn: 0.413951 drop: 0.464760 plain: 0.413583\n",
      "Train Epoch: 87 [800/801 (6%)]\tLosses bn: 0.463260 drop: 0.528657 plain: 0.474411\n",
      "Test set:\n",
      "bn: Loss: 0.0231\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0241\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 88 [0/801 (0%)]\tLosses bn: 0.413526 drop: 0.422775 plain: 0.413275\n",
      "Train Epoch: 88 [800/801 (6%)]\tLosses bn: 0.462703 drop: 0.524347 plain: 0.473778\n",
      "Test set:\n",
      "bn: Loss: 0.0230\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0240\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 89 [0/801 (0%)]\tLosses bn: 0.413091 drop: 0.443479 plain: 0.412971\n",
      "Train Epoch: 89 [800/801 (6%)]\tLosses bn: 0.462164 drop: 0.510289 plain: 0.473171\n",
      "Test set:\n",
      "bn: Loss: 0.0230\tAccuracy: 63.0/90 (70%)\n",
      "drop: Loss: 0.0240\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 90 [0/801 (0%)]\tLosses bn: 0.412678 drop: 0.461304 plain: 0.412668\n",
      "Train Epoch: 90 [800/801 (6%)]\tLosses bn: 0.461655 drop: 0.490210 plain: 0.472599\n",
      "Test set:\n",
      "bn: Loss: 0.0230\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0239\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 91 [0/801 (0%)]\tLosses bn: 0.412254 drop: 0.525747 plain: 0.412371\n",
      "Train Epoch: 91 [800/801 (6%)]\tLosses bn: 0.461168 drop: 0.529736 plain: 0.472032\n",
      "Test set:\n",
      "bn: Loss: 0.0229\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0239\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 92 [0/801 (0%)]\tLosses bn: 0.411847 drop: 0.429222 plain: 0.412093\n",
      "Train Epoch: 92 [800/801 (6%)]\tLosses bn: 0.460668 drop: 0.535334 plain: 0.471479\n",
      "Test set:\n",
      "bn: Loss: 0.0229\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0238\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 93 [0/801 (0%)]\tLosses bn: 0.411453 drop: 0.460224 plain: 0.411829\n",
      "Train Epoch: 93 [800/801 (6%)]\tLosses bn: 0.460193 drop: 0.458882 plain: 0.470924\n",
      "Test set:\n",
      "bn: Loss: 0.0229\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0238\tAccuracy: 63.0/90 (70%)\n",
      "plain: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 94 [0/801 (0%)]\tLosses bn: 0.411060 drop: 0.479734 plain: 0.411571\n",
      "Train Epoch: 94 [800/801 (6%)]\tLosses bn: 0.459749 drop: 0.517561 plain: 0.470378\n",
      "Test set:\n",
      "bn: Loss: 0.0229\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0237\tAccuracy: 63.0/90 (70%)\n",
      "plain: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 95 [0/801 (0%)]\tLosses bn: 0.410674 drop: 0.415166 plain: 0.411311\n",
      "Train Epoch: 95 [800/801 (6%)]\tLosses bn: 0.459293 drop: 0.479720 plain: 0.469865\n",
      "Test set:\n",
      "bn: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0237\tAccuracy: 63.0/90 (70%)\n",
      "plain: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 96 [0/801 (0%)]\tLosses bn: 0.410275 drop: 0.471825 plain: 0.411048\n",
      "Train Epoch: 96 [800/801 (6%)]\tLosses bn: 0.458871 drop: 0.485664 plain: 0.469384\n",
      "Test set:\n",
      "bn: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0237\tAccuracy: 63.0/90 (70%)\n",
      "plain: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 97 [0/801 (0%)]\tLosses bn: 0.409889 drop: 0.435956 plain: 0.410788\n",
      "Train Epoch: 97 [800/801 (6%)]\tLosses bn: 0.458455 drop: 0.522529 plain: 0.468930\n",
      "Test set:\n",
      "bn: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0236\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 98 [0/801 (0%)]\tLosses bn: 0.409507 drop: 0.420929 plain: 0.410527\n",
      "Train Epoch: 98 [800/801 (6%)]\tLosses bn: 0.458078 drop: 0.463942 plain: 0.468462\n",
      "Test set:\n",
      "bn: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0236\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 99 [0/801 (0%)]\tLosses bn: 0.409134 drop: 0.480534 plain: 0.410274\n",
      "Train Epoch: 99 [800/801 (6%)]\tLosses bn: 0.457707 drop: 0.537523 plain: 0.468008\n",
      "Test set:\n",
      "bn: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0236\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 100 [0/801 (0%)]\tLosses bn: 0.408750 drop: 0.461725 plain: 0.410017\n",
      "Train Epoch: 100 [800/801 (6%)]\tLosses bn: 0.457359 drop: 0.483008 plain: 0.467562\n",
      "Test set:\n",
      "bn: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0235\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 101 [0/801 (0%)]\tLosses bn: 0.408372 drop: 0.416128 plain: 0.409762\n",
      "Train Epoch: 101 [800/801 (6%)]\tLosses bn: 0.457012 drop: 0.460624 plain: 0.467116\n",
      "Test set:\n",
      "bn: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0235\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 102 [0/801 (0%)]\tLosses bn: 0.407991 drop: 0.439238 plain: 0.409509\n",
      "Train Epoch: 102 [800/801 (6%)]\tLosses bn: 0.456682 drop: 0.428543 plain: 0.466682\n",
      "Test set:\n",
      "bn: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0235\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 103 [0/801 (0%)]\tLosses bn: 0.407614 drop: 0.387632 plain: 0.409252\n",
      "Train Epoch: 103 [800/801 (6%)]\tLosses bn: 0.456368 drop: 0.564342 plain: 0.466268\n",
      "Test set:\n",
      "bn: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0234\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 104 [0/801 (0%)]\tLosses bn: 0.407235 drop: 0.426997 plain: 0.408992\n",
      "Train Epoch: 104 [800/801 (6%)]\tLosses bn: 0.456042 drop: 0.444345 plain: 0.465865\n",
      "Test set:\n",
      "bn: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0234\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 105 [0/801 (0%)]\tLosses bn: 0.406857 drop: 0.469401 plain: 0.408734\n",
      "Train Epoch: 105 [800/801 (6%)]\tLosses bn: 0.455732 drop: 0.486400 plain: 0.465480\n",
      "Test set:\n",
      "bn: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0234\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 106 [0/801 (0%)]\tLosses bn: 0.406483 drop: 0.452543 plain: 0.408482\n",
      "Train Epoch: 106 [800/801 (6%)]\tLosses bn: 0.455421 drop: 0.517466 plain: 0.465105\n",
      "Test set:\n",
      "bn: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0233\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 107 [0/801 (0%)]\tLosses bn: 0.406104 drop: 0.480474 plain: 0.408222\n",
      "Train Epoch: 107 [800/801 (6%)]\tLosses bn: 0.455136 drop: 0.487022 plain: 0.464756\n",
      "Test set:\n",
      "bn: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0233\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 108 [0/801 (0%)]\tLosses bn: 0.405735 drop: 0.419906 plain: 0.407959\n",
      "Train Epoch: 108 [800/801 (6%)]\tLosses bn: 0.454869 drop: 0.508237 plain: 0.464402\n",
      "Test set:\n",
      "bn: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0233\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 109 [0/801 (0%)]\tLosses bn: 0.405369 drop: 0.381971 plain: 0.407696\n",
      "Train Epoch: 109 [800/801 (6%)]\tLosses bn: 0.454641 drop: 0.563603 plain: 0.464063\n",
      "Test set:\n",
      "bn: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0232\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 110 [0/801 (0%)]\tLosses bn: 0.405001 drop: 0.459408 plain: 0.407427\n",
      "Train Epoch: 110 [800/801 (6%)]\tLosses bn: 0.454404 drop: 0.494038 plain: 0.463735\n",
      "Test set:\n",
      "bn: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0232\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 111 [0/801 (0%)]\tLosses bn: 0.404649 drop: 0.397237 plain: 0.407150\n",
      "Train Epoch: 111 [800/801 (6%)]\tLosses bn: 0.454183 drop: 0.531568 plain: 0.463429\n",
      "Test set:\n",
      "bn: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0232\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 112 [0/801 (0%)]\tLosses bn: 0.404290 drop: 0.403554 plain: 0.406867\n",
      "Train Epoch: 112 [800/801 (6%)]\tLosses bn: 0.453969 drop: 0.492194 plain: 0.463146\n",
      "Test set:\n",
      "bn: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0232\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 113 [0/801 (0%)]\tLosses bn: 0.403934 drop: 0.393150 plain: 0.406583\n",
      "Train Epoch: 113 [800/801 (6%)]\tLosses bn: 0.453765 drop: 0.554391 plain: 0.462863\n",
      "Test set:\n",
      "bn: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0231\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 114 [0/801 (0%)]\tLosses bn: 0.403566 drop: 0.455738 plain: 0.406306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 114 [800/801 (6%)]\tLosses bn: 0.453566 drop: 0.439762 plain: 0.462597\n",
      "Test set:\n",
      "bn: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0231\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 115 [0/801 (0%)]\tLosses bn: 0.403203 drop: 0.464342 plain: 0.406022\n",
      "Train Epoch: 115 [800/801 (6%)]\tLosses bn: 0.453351 drop: 0.464544 plain: 0.462328\n",
      "Test set:\n",
      "bn: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0231\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 116 [0/801 (0%)]\tLosses bn: 0.402845 drop: 0.373020 plain: 0.405747\n",
      "Train Epoch: 116 [800/801 (6%)]\tLosses bn: 0.453167 drop: 0.610368 plain: 0.462057\n",
      "Test set:\n",
      "bn: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0231\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 117 [0/801 (0%)]\tLosses bn: 0.402476 drop: 0.389454 plain: 0.405473\n",
      "Train Epoch: 117 [800/801 (6%)]\tLosses bn: 0.452983 drop: 0.476819 plain: 0.461812\n",
      "Test set:\n",
      "bn: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0230\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 118 [0/801 (0%)]\tLosses bn: 0.402125 drop: 0.395658 plain: 0.405194\n",
      "Train Epoch: 118 [800/801 (6%)]\tLosses bn: 0.452792 drop: 0.443447 plain: 0.461575\n",
      "Test set:\n",
      "bn: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0230\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 119 [0/801 (0%)]\tLosses bn: 0.401761 drop: 0.396995 plain: 0.404908\n",
      "Train Epoch: 119 [800/801 (6%)]\tLosses bn: 0.452609 drop: 0.534170 plain: 0.461334\n",
      "Test set:\n",
      "bn: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0230\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0221\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 120 [0/801 (0%)]\tLosses bn: 0.401415 drop: 0.416038 plain: 0.404622\n",
      "Train Epoch: 120 [800/801 (6%)]\tLosses bn: 0.452464 drop: 0.494411 plain: 0.461116\n",
      "Test set:\n",
      "bn: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0230\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0221\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 121 [0/801 (0%)]\tLosses bn: 0.401066 drop: 0.324953 plain: 0.404338\n",
      "Train Epoch: 121 [800/801 (6%)]\tLosses bn: 0.452316 drop: 0.506810 plain: 0.460897\n",
      "Test set:\n",
      "bn: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0230\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0221\tAccuracy: 64.0/90 (71%)\n",
      "\n",
      "Train Epoch: 122 [0/801 (0%)]\tLosses bn: 0.400727 drop: 0.423235 plain: 0.404052\n",
      "Train Epoch: 122 [800/801 (6%)]\tLosses bn: 0.452161 drop: 0.528895 plain: 0.460685\n",
      "Test set:\n",
      "bn: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0229\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0221\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 123 [0/801 (0%)]\tLosses bn: 0.400388 drop: 0.425216 plain: 0.403765\n",
      "Train Epoch: 123 [800/801 (6%)]\tLosses bn: 0.452034 drop: 0.439675 plain: 0.460452\n",
      "Test set:\n",
      "bn: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0229\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0221\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 124 [0/801 (0%)]\tLosses bn: 0.400040 drop: 0.477271 plain: 0.403484\n",
      "Train Epoch: 124 [800/801 (6%)]\tLosses bn: 0.451896 drop: 0.564451 plain: 0.460235\n",
      "Test set:\n",
      "bn: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0229\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0221\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 125 [0/801 (0%)]\tLosses bn: 0.399706 drop: 0.418309 plain: 0.403198\n",
      "Train Epoch: 125 [800/801 (6%)]\tLosses bn: 0.451772 drop: 0.534714 plain: 0.460047\n",
      "Test set:\n",
      "bn: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0229\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0221\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 126 [0/801 (0%)]\tLosses bn: 0.399352 drop: 0.416768 plain: 0.402907\n",
      "Train Epoch: 126 [800/801 (6%)]\tLosses bn: 0.451652 drop: 0.492654 plain: 0.459856\n",
      "Test set:\n",
      "bn: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0229\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0221\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 127 [0/801 (0%)]\tLosses bn: 0.399008 drop: 0.442054 plain: 0.402634\n",
      "Train Epoch: 127 [800/801 (6%)]\tLosses bn: 0.451531 drop: 0.463182 plain: 0.459681\n",
      "Test set:\n",
      "bn: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0221\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 128 [0/801 (0%)]\tLosses bn: 0.398656 drop: 0.356511 plain: 0.402353\n",
      "Train Epoch: 128 [800/801 (6%)]\tLosses bn: 0.451394 drop: 0.541447 plain: 0.459518\n",
      "Test set:\n",
      "bn: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0220\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 129 [0/801 (0%)]\tLosses bn: 0.398300 drop: 0.461657 plain: 0.402065\n",
      "Train Epoch: 129 [800/801 (6%)]\tLosses bn: 0.451298 drop: 0.437714 plain: 0.459371\n",
      "Test set:\n",
      "bn: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0220\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 130 [0/801 (0%)]\tLosses bn: 0.397955 drop: 0.414439 plain: 0.401780\n",
      "Train Epoch: 130 [800/801 (6%)]\tLosses bn: 0.451193 drop: 0.512391 plain: 0.459218\n",
      "Test set:\n",
      "bn: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0220\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 131 [0/801 (0%)]\tLosses bn: 0.397609 drop: 0.415142 plain: 0.401494\n",
      "Train Epoch: 131 [800/801 (6%)]\tLosses bn: 0.451068 drop: 0.514357 plain: 0.459065\n",
      "Test set:\n",
      "bn: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0220\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 132 [0/801 (0%)]\tLosses bn: 0.397275 drop: 0.419264 plain: 0.401220\n",
      "Train Epoch: 132 [800/801 (6%)]\tLosses bn: 0.450958 drop: 0.508919 plain: 0.458923\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0220\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 133 [0/801 (0%)]\tLosses bn: 0.396926 drop: 0.383513 plain: 0.400956\n",
      "Train Epoch: 133 [800/801 (6%)]\tLosses bn: 0.450848 drop: 0.512498 plain: 0.458781\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0220\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 134 [0/801 (0%)]\tLosses bn: 0.396586 drop: 0.493511 plain: 0.400691\n",
      "Train Epoch: 134 [800/801 (6%)]\tLosses bn: 0.450748 drop: 0.484956 plain: 0.458636\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0228\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0220\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 135 [0/801 (0%)]\tLosses bn: 0.396243 drop: 0.446349 plain: 0.400428\n",
      "Train Epoch: 135 [800/801 (6%)]\tLosses bn: 0.450670 drop: 0.491783 plain: 0.458514\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0220\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 136 [0/801 (0%)]\tLosses bn: 0.395892 drop: 0.451770 plain: 0.400159\n",
      "Train Epoch: 136 [800/801 (6%)]\tLosses bn: 0.450573 drop: 0.481282 plain: 0.458370\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0220\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 137 [0/801 (0%)]\tLosses bn: 0.395549 drop: 0.434858 plain: 0.399899\n",
      "Train Epoch: 137 [800/801 (6%)]\tLosses bn: 0.450466 drop: 0.522574 plain: 0.458246\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0220\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 138 [0/801 (0%)]\tLosses bn: 0.395215 drop: 0.454692 plain: 0.399628\n",
      "Train Epoch: 138 [800/801 (6%)]\tLosses bn: 0.450382 drop: 0.510141 plain: 0.458113\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0220\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 139 [0/801 (0%)]\tLosses bn: 0.394858 drop: 0.361208 plain: 0.399367\n",
      "Train Epoch: 139 [800/801 (6%)]\tLosses bn: 0.450266 drop: 0.467503 plain: 0.458001\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0227\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 140 [0/801 (0%)]\tLosses bn: 0.394504 drop: 0.377892 plain: 0.399107\n",
      "Train Epoch: 140 [800/801 (6%)]\tLosses bn: 0.450162 drop: 0.514445 plain: 0.457865\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 141 [0/801 (0%)]\tLosses bn: 0.394136 drop: 0.355295 plain: 0.398858\n",
      "Train Epoch: 141 [800/801 (6%)]\tLosses bn: 0.450065 drop: 0.446282 plain: 0.457744\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 142 [0/801 (0%)]\tLosses bn: 0.393774 drop: 0.390045 plain: 0.398607\n",
      "Train Epoch: 142 [800/801 (6%)]\tLosses bn: 0.449993 drop: 0.536006 plain: 0.457602\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 143 [0/801 (0%)]\tLosses bn: 0.393403 drop: 0.386286 plain: 0.398358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 143 [800/801 (6%)]\tLosses bn: 0.449958 drop: 0.501172 plain: 0.457497\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 144 [0/801 (0%)]\tLosses bn: 0.393023 drop: 0.435956 plain: 0.398112\n",
      "Train Epoch: 144 [800/801 (6%)]\tLosses bn: 0.449907 drop: 0.499148 plain: 0.457379\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 145 [0/801 (0%)]\tLosses bn: 0.392648 drop: 0.486108 plain: 0.397855\n",
      "Train Epoch: 145 [800/801 (6%)]\tLosses bn: 0.449891 drop: 0.482350 plain: 0.457265\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 146 [0/801 (0%)]\tLosses bn: 0.392278 drop: 0.384759 plain: 0.397611\n",
      "Train Epoch: 146 [800/801 (6%)]\tLosses bn: 0.449854 drop: 0.462118 plain: 0.457174\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 147 [0/801 (0%)]\tLosses bn: 0.391897 drop: 0.457112 plain: 0.397356\n",
      "Train Epoch: 147 [800/801 (6%)]\tLosses bn: 0.449823 drop: 0.510372 plain: 0.457058\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 148 [0/801 (0%)]\tLosses bn: 0.391532 drop: 0.456822 plain: 0.397114\n",
      "Train Epoch: 148 [800/801 (6%)]\tLosses bn: 0.449777 drop: 0.527894 plain: 0.456958\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 149 [0/801 (0%)]\tLosses bn: 0.391171 drop: 0.420547 plain: 0.396872\n",
      "Train Epoch: 149 [800/801 (6%)]\tLosses bn: 0.449765 drop: 0.436567 plain: 0.456851\n",
      "Test set:\n",
      "bn: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 150 [0/801 (0%)]\tLosses bn: 0.390805 drop: 0.466947 plain: 0.396626\n",
      "Train Epoch: 150 [800/801 (6%)]\tLosses bn: 0.449756 drop: 0.513685 plain: 0.456752\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0226\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 151 [0/801 (0%)]\tLosses bn: 0.390443 drop: 0.429231 plain: 0.396383\n",
      "Train Epoch: 151 [800/801 (6%)]\tLosses bn: 0.449737 drop: 0.413011 plain: 0.456664\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 152 [0/801 (0%)]\tLosses bn: 0.390089 drop: 0.419053 plain: 0.396134\n",
      "Train Epoch: 152 [800/801 (6%)]\tLosses bn: 0.449752 drop: 0.593167 plain: 0.456583\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 153 [0/801 (0%)]\tLosses bn: 0.389719 drop: 0.425138 plain: 0.395899\n",
      "Train Epoch: 153 [800/801 (6%)]\tLosses bn: 0.449728 drop: 0.481994 plain: 0.456487\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 154 [0/801 (0%)]\tLosses bn: 0.389365 drop: 0.370489 plain: 0.395659\n",
      "Train Epoch: 154 [800/801 (6%)]\tLosses bn: 0.449725 drop: 0.524840 plain: 0.456366\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0219\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 155 [0/801 (0%)]\tLosses bn: 0.389006 drop: 0.419967 plain: 0.395428\n",
      "Train Epoch: 155 [800/801 (6%)]\tLosses bn: 0.449715 drop: 0.451456 plain: 0.456290\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 156 [0/801 (0%)]\tLosses bn: 0.388641 drop: 0.468336 plain: 0.395184\n",
      "Train Epoch: 156 [800/801 (6%)]\tLosses bn: 0.449681 drop: 0.506171 plain: 0.456198\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 157 [0/801 (0%)]\tLosses bn: 0.388282 drop: 0.456181 plain: 0.394935\n",
      "Train Epoch: 157 [800/801 (6%)]\tLosses bn: 0.449661 drop: 0.501001 plain: 0.456087\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0225\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 158 [0/801 (0%)]\tLosses bn: 0.387925 drop: 0.393950 plain: 0.394693\n",
      "Train Epoch: 158 [800/801 (6%)]\tLosses bn: 0.449629 drop: 0.425152 plain: 0.456007\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 159 [0/801 (0%)]\tLosses bn: 0.387574 drop: 0.480860 plain: 0.394443\n",
      "Train Epoch: 159 [800/801 (6%)]\tLosses bn: 0.449626 drop: 0.441467 plain: 0.455913\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 160 [0/801 (0%)]\tLosses bn: 0.387206 drop: 0.391449 plain: 0.394202\n",
      "Train Epoch: 160 [800/801 (6%)]\tLosses bn: 0.449602 drop: 0.450457 plain: 0.455852\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 161 [0/801 (0%)]\tLosses bn: 0.386849 drop: 0.424080 plain: 0.393953\n",
      "Train Epoch: 161 [800/801 (6%)]\tLosses bn: 0.449534 drop: 0.482796 plain: 0.455779\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 162 [0/801 (0%)]\tLosses bn: 0.386500 drop: 0.364015 plain: 0.393711\n",
      "Train Epoch: 162 [800/801 (6%)]\tLosses bn: 0.449511 drop: 0.492311 plain: 0.455705\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 163 [0/801 (0%)]\tLosses bn: 0.386135 drop: 0.428824 plain: 0.393463\n",
      "Train Epoch: 163 [800/801 (6%)]\tLosses bn: 0.449456 drop: 0.489839 plain: 0.455622\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 164 [0/801 (0%)]\tLosses bn: 0.385777 drop: 0.431374 plain: 0.393232\n",
      "Train Epoch: 164 [800/801 (6%)]\tLosses bn: 0.449451 drop: 0.455909 plain: 0.455550\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 165 [0/801 (0%)]\tLosses bn: 0.385418 drop: 0.446045 plain: 0.392984\n",
      "Train Epoch: 165 [800/801 (6%)]\tLosses bn: 0.449430 drop: 0.577948 plain: 0.455480\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 166 [0/801 (0%)]\tLosses bn: 0.385056 drop: 0.454390 plain: 0.392750\n",
      "Train Epoch: 166 [800/801 (6%)]\tLosses bn: 0.449391 drop: 0.428686 plain: 0.455430\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 167 [0/801 (0%)]\tLosses bn: 0.384694 drop: 0.368536 plain: 0.392508\n",
      "Train Epoch: 167 [800/801 (6%)]\tLosses bn: 0.449412 drop: 0.503753 plain: 0.455367\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 168 [0/801 (0%)]\tLosses bn: 0.384323 drop: 0.419362 plain: 0.392277\n",
      "Train Epoch: 168 [800/801 (6%)]\tLosses bn: 0.449404 drop: 0.434365 plain: 0.455312\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 169 [0/801 (0%)]\tLosses bn: 0.383957 drop: 0.453175 plain: 0.392028\n",
      "Train Epoch: 169 [800/801 (6%)]\tLosses bn: 0.449415 drop: 0.449893 plain: 0.455241\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 170 [0/801 (0%)]\tLosses bn: 0.383588 drop: 0.491284 plain: 0.391792\n",
      "Train Epoch: 170 [800/801 (6%)]\tLosses bn: 0.449402 drop: 0.462253 plain: 0.455170\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 171 [0/801 (0%)]\tLosses bn: 0.383224 drop: 0.449976 plain: 0.391558\n",
      "Train Epoch: 171 [800/801 (6%)]\tLosses bn: 0.449404 drop: 0.584205 plain: 0.455138\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 172 [0/801 (0%)]\tLosses bn: 0.382855 drop: 0.445217 plain: 0.391321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 172 [800/801 (6%)]\tLosses bn: 0.449434 drop: 0.505212 plain: 0.455094\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 173 [0/801 (0%)]\tLosses bn: 0.382496 drop: 0.366991 plain: 0.391082\n",
      "Train Epoch: 173 [800/801 (6%)]\tLosses bn: 0.449435 drop: 0.484547 plain: 0.455044\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0224\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 174 [0/801 (0%)]\tLosses bn: 0.382133 drop: 0.406129 plain: 0.390847\n",
      "Train Epoch: 174 [800/801 (6%)]\tLosses bn: 0.449446 drop: 0.542481 plain: 0.455032\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 175 [0/801 (0%)]\tLosses bn: 0.381773 drop: 0.389832 plain: 0.390604\n",
      "Train Epoch: 175 [800/801 (6%)]\tLosses bn: 0.449467 drop: 0.467635 plain: 0.454984\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 65.0/90 (72%)\n",
      "drop: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 176 [0/801 (0%)]\tLosses bn: 0.381400 drop: 0.386843 plain: 0.390373\n",
      "Train Epoch: 176 [800/801 (6%)]\tLosses bn: 0.449485 drop: 0.500123 plain: 0.454954\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 177 [0/801 (0%)]\tLosses bn: 0.381037 drop: 0.406912 plain: 0.390148\n",
      "Train Epoch: 177 [800/801 (6%)]\tLosses bn: 0.449515 drop: 0.460519 plain: 0.454933\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 178 [0/801 (0%)]\tLosses bn: 0.380657 drop: 0.408217 plain: 0.389909\n",
      "Train Epoch: 178 [800/801 (6%)]\tLosses bn: 0.449535 drop: 0.539895 plain: 0.454904\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 179 [0/801 (0%)]\tLosses bn: 0.380284 drop: 0.420238 plain: 0.389674\n",
      "Train Epoch: 179 [800/801 (6%)]\tLosses bn: 0.449565 drop: 0.517028 plain: 0.454875\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 180 [0/801 (0%)]\tLosses bn: 0.379904 drop: 0.458055 plain: 0.389443\n",
      "Train Epoch: 180 [800/801 (6%)]\tLosses bn: 0.449579 drop: 0.422441 plain: 0.454839\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0218\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 181 [0/801 (0%)]\tLosses bn: 0.379521 drop: 0.379526 plain: 0.389207\n",
      "Train Epoch: 181 [800/801 (6%)]\tLosses bn: 0.449591 drop: 0.467254 plain: 0.454808\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 182 [0/801 (0%)]\tLosses bn: 0.379139 drop: 0.392946 plain: 0.388976\n",
      "Train Epoch: 182 [800/801 (6%)]\tLosses bn: 0.449602 drop: 0.484146 plain: 0.454782\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 183 [0/801 (0%)]\tLosses bn: 0.378756 drop: 0.402365 plain: 0.388741\n",
      "Train Epoch: 183 [800/801 (6%)]\tLosses bn: 0.449610 drop: 0.454145 plain: 0.454777\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0223\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 184 [0/801 (0%)]\tLosses bn: 0.378383 drop: 0.361993 plain: 0.388501\n",
      "Train Epoch: 184 [800/801 (6%)]\tLosses bn: 0.449623 drop: 0.465088 plain: 0.454752\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 185 [0/801 (0%)]\tLosses bn: 0.378001 drop: 0.409003 plain: 0.388275\n",
      "Train Epoch: 185 [800/801 (6%)]\tLosses bn: 0.449595 drop: 0.500391 plain: 0.454723\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 186 [0/801 (0%)]\tLosses bn: 0.377629 drop: 0.355063 plain: 0.388036\n",
      "Train Epoch: 186 [800/801 (6%)]\tLosses bn: 0.449607 drop: 0.491992 plain: 0.454723\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 187 [0/801 (0%)]\tLosses bn: 0.377244 drop: 0.382899 plain: 0.387797\n",
      "Train Epoch: 187 [800/801 (6%)]\tLosses bn: 0.449612 drop: 0.475593 plain: 0.454672\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 188 [0/801 (0%)]\tLosses bn: 0.376861 drop: 0.377448 plain: 0.387568\n",
      "Train Epoch: 188 [800/801 (6%)]\tLosses bn: 0.449622 drop: 0.462584 plain: 0.454685\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 189 [0/801 (0%)]\tLosses bn: 0.376472 drop: 0.434899 plain: 0.387321\n",
      "Train Epoch: 189 [800/801 (6%)]\tLosses bn: 0.449623 drop: 0.451615 plain: 0.454651\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 190 [0/801 (0%)]\tLosses bn: 0.376092 drop: 0.349413 plain: 0.387090\n",
      "Train Epoch: 190 [800/801 (6%)]\tLosses bn: 0.449632 drop: 0.484531 plain: 0.454643\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 191 [0/801 (0%)]\tLosses bn: 0.375711 drop: 0.398832 plain: 0.386853\n",
      "Train Epoch: 191 [800/801 (6%)]\tLosses bn: 0.449631 drop: 0.501474 plain: 0.454608\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 192 [0/801 (0%)]\tLosses bn: 0.375328 drop: 0.398487 plain: 0.386624\n",
      "Train Epoch: 192 [800/801 (6%)]\tLosses bn: 0.449605 drop: 0.560334 plain: 0.454582\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 193 [0/801 (0%)]\tLosses bn: 0.374976 drop: 0.395623 plain: 0.386382\n",
      "Train Epoch: 193 [800/801 (6%)]\tLosses bn: 0.449634 drop: 0.536956 plain: 0.454575\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 194 [0/801 (0%)]\tLosses bn: 0.374603 drop: 0.354015 plain: 0.386142\n",
      "Train Epoch: 194 [800/801 (6%)]\tLosses bn: 0.449614 drop: 0.501015 plain: 0.454534\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 195 [0/801 (0%)]\tLosses bn: 0.374250 drop: 0.421845 plain: 0.385912\n",
      "Train Epoch: 195 [800/801 (6%)]\tLosses bn: 0.449596 drop: 0.482846 plain: 0.454524\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 196 [0/801 (0%)]\tLosses bn: 0.373895 drop: 0.397477 plain: 0.385684\n",
      "Train Epoch: 196 [800/801 (6%)]\tLosses bn: 0.449604 drop: 0.433465 plain: 0.454504\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 197 [0/801 (0%)]\tLosses bn: 0.373547 drop: 0.424466 plain: 0.385450\n",
      "Train Epoch: 197 [800/801 (6%)]\tLosses bn: 0.449595 drop: 0.499340 plain: 0.454481\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 198 [0/801 (0%)]\tLosses bn: 0.373195 drop: 0.388616 plain: 0.385214\n",
      "Train Epoch: 198 [800/801 (6%)]\tLosses bn: 0.449564 drop: 0.485216 plain: 0.454469\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 64.0/90 (71%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 199 [0/801 (0%)]\tLosses bn: 0.372858 drop: 0.372807 plain: 0.384983\n",
      "Train Epoch: 199 [800/801 (6%)]\tLosses bn: 0.449550 drop: 0.481125 plain: 0.454450\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n",
      "Train Epoch: 200 [0/801 (0%)]\tLosses bn: 0.372522 drop: 0.408753 plain: 0.384759\n",
      "Train Epoch: 200 [800/801 (6%)]\tLosses bn: 0.449523 drop: 0.394347 plain: 0.454424\n",
      "Test set:\n",
      "bn: Loss: 0.0222\tAccuracy: 66.0/90 (73%)\n",
      "drop: Loss: 0.0222\tAccuracy: 65.0/90 (72%)\n",
      "plain: Loss: 0.0217\tAccuracy: 65.0/90 (72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    train(epoch, models, train_log)\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    test(models, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dbb8828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(test_log, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e8ad780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(test_log, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
